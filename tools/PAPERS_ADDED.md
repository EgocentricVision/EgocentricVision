- [On the Utility of 3D Hand Poses for Action Recognition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1025_ECCV_2024_paper.php) - Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao, ECCV 2024. [[project page]](https://s-shamil.github.io/HandFormer/)

- [Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3682_ECCV_2024_paper.php) - Zicong Fan, Takehiko Ohkawa, Linlin Yang, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao, ECCV 2024.

- [Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection?](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8953_ECCV_2024_paper.php) - Rosario Leonardi, Antonino Furnari, Francesco Ragusa, Giovanni Maria Farinella, ECCV 2024. [[project page]](https://fpv-iplab.github.io/HOI-Synth/)

- [3D Hand Pose Estimation in Everyday Egocentric Images](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10034_ECCV_2024_paper.php) - Aditya Prakash, Ruisen Tu, Matthew Chang, Saurabh Gupta, ECCV 2024. [[project page]](https://ap229997.github.io/projects/hands/)

- [HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields](https://arxiv.org/abs/2402.17062v1) - Haozhe Qi, Chen Zhao, Mathieu Salzmann, Alexander Mathis, CVPR 2024. [[code]](https://github.com/amathislab/HOISDF)

- [Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation](https://openaccess.thecvf.com//content/CVPR2024/html/Liu_Single-to-Dual-View_Adaptation_for_Egocentric_3D_Hand_Pose_Estimation_CVPR_2024_paper) - Ruicong Liu, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato, CVPR 2024. [[code]](https://github.com/ut-vision/S2DHand)

- [AFF-ttention! Affordances and Attention models for Short-Term Object Interaction Anticipation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3361_ECCV_2024_paper.php) - Lorenzo Mur-Labadia, Ruben Martinez-Cantin, Jose J Guerrero, Giovanni Maria Farinella, Antonino Furnari, ECCV 2024.

- [Semantically Guided Representation Learning For Action Anticipation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4140_ECCV_2024_paper.php) - Anxhelo Diko, Danilo Avola, Bardh Prenkaj, Federico Fontana, Luigi Cinque, ECCV 2024. [[code]](https://github.com/ADiko1997/S-GEAR)

- [Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation](https://openaccess.thecvf.com//content/CVPR2024/html/Pasca_Summarize_the_Past_to_Predict_the_Future_Natural_Language_Descriptions_CVPR_2024_paper) - Razvan-George Pasca, Alexey Gavryushin, Muhammad Hamza, Yen-Ling Kuo, Kaichun Mo, Luc Van Gool, Otmar Hilliges, Xi Wang, CVPR 2024.

- [Can't Make an Omelette Without Breaking Some Eggs: Plausible Action Anticipation Using Large Video-Language Models](https://openaccess.thecvf.com//content/CVPR2024/html/Mittal_Cant_Make_an_Omelette_Without_Breaking_Some_Eggs_Plausible_Action_CVPR_2024_paper) - Himangi Mittal, Nakul Agarwal, Shao-Yuan Lo, Kwonjoon Lee, CVPR 2024.

- [Bidirectional Progressive Transformer for Interaction Intention Anticipation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7631_ECCV_2024_paper.php) - Zichen Zhang, Hongchen Luo, Wei Zhai, Yu Kang, Yang Cao, ECCV 2024.

- [SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos](https://arxiv.org/abs/2404.05206) - Changan Chen, Kumar Ashutosh, Rohit Girdhar, David Harwath, Kristen Grauman, CVPR 2024. [[project page]](https://vision.cs.utexas.edu/projects/soundingactions/)

- [The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective](https://openaccess.thecvf.com//content/CVPR2024/html/Jia_The_Audio-Visual_Conversational_Graph_From_an_Egocentric-Exocentric_Perspective_CVPR_2024_paper) - Wenqi Jia, Miao Liu, Hao Jiang, Ishwarya Ananthabhotla, James M. Rehg, Vamsi Krishna Ithapu, Ruohan Gao, CVPR 2024. [[project page]](https://vjwq.github.io/AV-CONV/)

- [Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Majumder_Learning_Spatial_Features_from_Audio-Visual_Correspondence_in_Egocentric_Videos_CVPR_2024_paper) - Sagnik Majumder, Ziad Al-Halah, Kristen Grauman, CVPR 2024. [[project page]](https://vision.cs.utexas.edu/projects/ego_av_corr/)

- [SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Chen_SoundingActions_Learning_How_Actions_Sound_from_Narrated_Egocentric_Videos_CVPR_2024_paper) - Changan Chen, Kumar Ashutosh, Rohit Girdhar, David Harwath, Kristen Grauman, CVPR 2024.

- [EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams](https://openaccess.thecvf.com//content/CVPR2024/html/Millerdurai_EventEgo3D_3D_Human_Motion_Capture_from_Egocentric_Event_Streams_CVPR_2024_paper) - Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik, CVPR 2024. [[project page]](https://4dqv.mpi-inf.mpg.de/EventEgo3D/)

- [EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams](https://openaccess.thecvf.com//content/CVPR2024/html/Millerdurai_EventEgo3D_3D_Human_Motion_Capture_from_Egocentric_Event_Streams_CVPR_2024_paper) - Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik, CVPR 2024. [[project page]](https://4dqv.mpi-inf.mpg.de/EventEgo3D/)

- [Masked Video and Body-worn IMU Autoencoder for Egocentric Action Recognition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2781_ECCV_2024_paper.php) - Mingfang Zhang, Yifei Huang*, Ruicong Liu, Yoichi Sato, ECCV 2024.

- [ActionSwitch: Class-agnostic Detection of Simultaneous Actions in Streaming Videos](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1621_ECCV_2024_paper.php) - Hyolim Kang, Jeongseok Hyun, Joungbin An, Youngjae Yu, Seon Joo Kim, ECCV 2024.

- [HAT: History-Augmented Anchor Transformer for Online Temporal Action Localization](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3153_ECCV_2024_paper.php) - Sakib Reza, Yuexi Zhang, Mohsen Moghaddam, Octavia Camps, ECCV 2024. [[code]](https://github.com/sakibreza/ECCV24-HAT/)

- [UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6283_ECCV_2024_paper.php) - Yingsen Zeng, Yujie Zhong, Chengjian Feng, Lin Ma, ECCV 2024. [[code]](https://github.com/yingsen1/UniMD)

- [DyFADet: Dynamic Feature Aggregation for Temporal Action Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6288_ECCV_2024_paper.php) - Le Yang, Ziwei Zheng, Yizeng Han, Hao Cheng, Shiji Song, Gao Huang, Fan Li, ECCV 2024. [[code]](https://github.com/yangle15/DyFADet-pytorch)

- [Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9116_ECCV_2024_paper.php) - Camillo Quattrocchi, Antonino Furnari, Daniele Di Mauro, Mario Valerio Giuffrida, Giovanni Maria Farinella, ECCV 2024. [[code]](https://github.com/fpv-iplab/synchronization-is-all-you-need)

- [FACT: Frame-Action Cross-Attention Temporal](https://openaccess.thecvf.com//content/CVPR2024/html/Lu_FACT_Frame-Action_Cross-Attention_Temporal_Modeling_for_Efficient_Action_Segmentation_CVPR_2024_paper) - Zijia Lu, Ehsan Elhamifar, CVPR 2024. [[code]](https://github.com/ZijiaLewisLu/CVPR2024-FACT)

- [https://openaccess.thecvf.com//content/CVPR2024/html/Shen_Progress-Aware_Online_Action_Segmentation_for_Egocentric_Procedural_Task_Videos_CVPR_2024_paper](https://openaccess.thecvf.com//content/CVPR2024/html/Shen_Progress-Aware_Online_Action_Segmentation_for_Egocentric_Procedural_Task_Videos_CVPR_2024_paper) - Yuhan Shen, Ehsan Elhamifar, CVPR 2024. [[code]](https://github.com/Yuhan-Shen/ProTAS)

- [EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video Retrieval](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5363_ECCV_2024_paper.php) - Thomas Hummel, Shyamgopal Karthik, Mariana-Iuliana Georgescu, Zeynep Akata, ECCV 2024. [[code]](https://github.com/ExplainableML/EgoCVR)

- [Retrieval-Augmented Egocentric Video Captioning](https://arxiv.org/abs/2401.00789v2) - Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie, CVPR 2024.

- [ActionVOS: Actions as Prompts for Video Object Segmentation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1553_ECCV_2024_paper.php) - Liangyang Ouyang, Ruicong Liu, Yifei Huang, Ryosuke Furuta, Yoichi Sato, ECCV 2024. [[code]](https://github.com/ut-vision/ActionVOS)

- [EgoLifter: Open-world 3D Segmentation for Egocentric Perception](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6006_ECCV_2024_paper.php) - Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney, ECCV 2024. [[project page]](https://egolifter.github.io/)

- [Learning to Segment Referred Objects from Narrated Egocentric Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Shen_Learning_to_Segment_Referred_Objects_from_Narrated_Egocentric_Videos_CVPR_2024_paper) - Yuhan Shen, Huiyu Wang, Xitong Yang, Matt Feiszli, Ehsan Elhamifar, Lorenzo Torresani, Effrosyni Mavroudi;, CVPR 2024.

- [A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval](https://arxiv.org/abs/2402.19106) - Andreea-Maria Oncescu, João F. Henriques, Andrew Zisserman, Samuel Albanie, A. Sophia Koepke, ICASSP 2024.

- [LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning](https://arxiv.org/abs/2312.03849) - Bolin Lai, Xiaoliang Dai, Lawrence Chen, Guan Pang, James M. Rehg, Miao Liu, ECCV 2024.

- [Vamos: Versatile Action Models for Video Understanding](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1860_ECCV_2024_paper.php) - Shijie Wang, Qi Zhao, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, Chen Sun, ECCV 2024. [[project page]](https://brown-palm.github.io/Vamos/)

- [PALM: Predicting Actions through Language Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10743_ECCV_2024_paper.php) - Sanghwan Kim, Daoji Huang, Yongqin Xian, Otmar Hilliges, Luc Van Gool, Xi Wang, ECCV 2024.

- [Text-Conditioned Resampler For Long Form Video Understanding](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11664.pdf) - Bruno Korbar, Yongqin Xian, Alessio Tonioni, Andrew Zisserman, Federico Tombari, ECCV 2024.

- [Grounded Question-Answering in Long Egocentric Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Di_Grounded_Question-Answering_in_Long_Egocentric_Videos_CVPR_2024_paper) - Shangzhe Di, Weidi Xie;, CVPR 2024. [[code]](https://github.com/Becomebright/GroundVQA)

- [Video ReCap: Recursive Captioning of Hour-Long Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Islam_Video_ReCap_Recursive_Captioning_of_Hour-Long_Videos_CVPR_2024_paper) - Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius, CVPR 2024. [[project page]](https://sites.google.com/view/vidrecap)

- [Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4830_ECCV_2024_paper.php) - Masashi Hatano, Ryo Hachiuma, Ryo Fujii, Hideo Saito, ECCV 2024. [[project page]](https://masashi-hatano.github.io/MM-CDFSL/)

- [Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1396_ECCV_2024_paper.php) - Bolin Lai, Fiona Ryan, Wenqi Jia, Miao Liu, James M Rehg, ECCV 2024. [[project page]](https://bolinlai.github.io/CSTS-EgoGazeAnticipation/)

- [Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos](https://arxiv.org/abs/2403.06351) - Mi Luo, Zihui Xue, Alex Dimakis, Kristen Grauman, ECCV 2024.

- [The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective](https://openaccess.thecvf.com//content/CVPR2024/html/Jia_The_Audio-Visual_Conversational_Graph_From_an_Egocentric-Exocentric_Perspective_CVPR_2024_paper) - Wenqi Jia, Miao Liu, Hao Jiang, Ishwarya Ananthabhotla, James M. Rehg, Vamsi Krishna Ithapu, Ruohan Gao, CVPR 2024. [[project page]](https://vjwq.github.io/AV-CONV/)

- [EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and Intermittent Observations Everywhere](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/248_ECCV_2024_paper.php) - Jiaxi Jiang*, Paul Streli, Manuel Meier, Christian Holz, ECCV 2024.

- [EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7241_ECCV_2024_paper.php) - Chenhongyi Yang, Anastasia Tkach, Shreyas Hampali, Linguang Zhang, Elliot J Crowley, Cem Keskin, ECCV 2024. [[code]](https://github.com/ChenhongyiYang/egoposeformer)

- [Egocentric Whole-Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement](https://arxiv.org/abs/2311.16495) - Jian Wang, Zhe Cao, Diogo Luvizon, Lingjie Liu, Kripasindhu Sarkar, Danhang Tang, Thabo Beeler, Christian Theobalt, CVPR 2024. [[project page]](https://people.mpi-inf.mpg.de/~jianwang/projects/egowholemocap/)

- [Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting](https://arxiv.org/abs/2402.18330) - Taeho Kang, Youngki Lee, CVPR 2024. [[code]](https://github.com/tho-kn/EgoTAP)

- [EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams](https://openaccess.thecvf.com//content/CVPR2024/html/Millerdurai_EventEgo3D_3D_Human_Motion_Capture_from_Egocentric_Event_Streams_CVPR_2024_paper) - Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik, CVPR 2024. [[project page]](https://4dqv.mpi-inf.mpg.de/EventEgo3D/)

- [Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting](https://openaccess.thecvf.com//content/CVPR2024/html/Kang_Attention-Propagation_Network_for_Egocentric_Heatmap_to_3D_Pose_Lifting_CVPR_2024_paper) - Taeho Kang, Youngki Lee, CVPR 2024. [[code]](https://github.com/tho-kn/EgoTAP)

- [Spherical World-Locking for Audio-Visual Localization in Egocentric Videos](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3489_ECCV_2024_paper.php) - Heeseung Yun, Ruohan Gao, Ishwarya Ananthabhotla, Anurag Kumar, Jacob Donley, Chao Li, Gunhee Kim, Vamsi Krishna Ithapu, Calvin Murdock, ECCV 2024. [[project page]](https://hs-yn.github.io/SWL/)

- [Instance Tracking in 3D Scenes from Egocentric Videos](https://arxiv.org/abs/2312.04117) - Yunhan Zhao, Haoyu Ma, Shu Kong, Charless Fowlkes, CVPR 2024. [[code]](https://github.com/IT3DEgo/IT3DEgo/)

- [Ex2Eg-MAE: A Framework for Adaptation of Exocentric Video Masked Autoencoders for Egocentric Social Role Understanding](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10301_ECCV_2024_paper.php) - Minh Tran, Yelin Kim, Che-Chun Su, Min Sun, Cheng-Hao Kuo, Mohammad Soleymani, ECCV 2024.

- [LoCoNet: Long-Short Context Network for Active Speaker Detection](https://openaccess.thecvf.com//content/CVPR2024/html/Wang_LoCoNet_Long-Short_Context_Network_for_Active_Speaker_Detection_CVPR_2024_paper) - Xizi Wang, Feng Cheng, Gedas Bertasius, CVPR 2024. [[code]](https://github.com/SJTUwxz/LoCoNet_ASD)

- [Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Majumder_Learning_Spatial_Features_from_Audio-Visual_Correspondence_in_Egocentric_Videos_CVPR_2024_paper) - Sagnik Majumder, Ziad Al-Halah, Kristen Grauman, CVPR 2024. [[project page]](https://vision.cs.utexas.edu/projects/ego_av_corr/)

- [EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models](https://openaccess.thecvf.com//content/CVPR2024/html/Cheng_EgoThink_Evaluating_First-Person_Perspective_Thinking_Capability_of_Vision-Language_Models_CVPR_2024_paper) - Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, Yang Liu, CVPR 2024. [[code]](https://github.com/AdaCheng/EgoThink) [[project page]](https://adacheng.github.io/EgoThink/)

- [4Diff: 3D-Aware Diffusion Model for Third-to-First Viewpoint Translation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3536_ECCV_2024_paper.php) - Feng Cheng, Mi Luo, Huiyu Wang, Alex Dimakis, Lorenzo Torresani, Gedas Bertasius, Kristen Grauman, ECCV 2024. [[project page]](https://klauscc.github.io/4diff)

- [Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8903_ECCV_2024_paper.php) - Changan Chen, Puyuan Peng, Ami Baid, Zihui Xue, Wei-Ning Hsu, David Harwath, Kristen Grauman, ECCV 2024.

- [SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7833_ECCV_2024_paper.php) - Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins, Tsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang, Luke Holland, Duncan Frost, Campbell Orme, Jakob Engel, Edward Miller, Richard Newcombe, Vasileios Balntas, ECCV 2024. [[project page]](https://www.projectaria.com/scenescript/)

- [Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches and a Head-Mounted Camera](https://openaccess.thecvf.com/content/CVPR2024/html/Lee_Mocap_Everyone_Everywhere_Lightweight_Motion_Capture_With_Smartwatches_and_a_CVPR_2024_paper) - Jiye Lee, Hanbyul Joo, CVPR 2024. [[code]](https://github.com/jiyewise/MocapEvery)

- [VideoLLM-online: Online Video Large Language Model for Streaming Video](https://openaccess.thecvf.com//content/CVPR2024/html/Chen_VideoLLM-online_Online_Video_Large_Language_Model_for_Streaming_Video_CVPR_2024_paper) - Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, Mike Zheng Shou, CVPR 2024. [[video]](https://showlab.github.io/videollm-online/)

- [Error Detection in Egocentric Procedural Task Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Lee_Error_Detection_in_Egocentric_Procedural_Task_Videos_CVPR_2024_paper) - Shih-Po Lee, Zijia Lu, Zekun Zhang, Minh Hoai, Ehsan Elhamifar, CVPR 2024. [[code]](https://github.com/robert80203/EgoPER_official)

- [EgoPAT3Dv2](https://arxiv.org/abs/2403.05046) - Irving Fang, Yuzhong Chen, Yifan Wang, Jianghan Zhang, Qiushi Zhang, Jiali Xu, Xibo He, Weibo Gao, Hao Su, Yiming Li, Chen Feng, ICRA 2024. [[project page]](https://ai4ce.github.io/EgoPAT3Dv2/)

- [EgoGen: An Egocentric Synthetic Data Generator](https://arxiv.org/abs/2401.08739) - Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan Zhang, Marc Pollefeys, Siyu Tang, CVPR 2024. [[project page]](https://ego-gen.github.io/)

- [Active Object Detection with Knowledge Aggregation and Distillation from Large Models](https://openaccess.thecvf.com//content/CVPR2024/html/Yang_Active_Object_Detection_with_Knowledge_Aggregation_and_Distillation_from_Large_CVPR_2024_paper) - Dejie Yang, Yang Liu, CVPR 2024. [[code]](https://github.com/idejie/KAD)

- [EgoPAT3Dv2](https://ai4ce.github.io/EgoPAT3Dv2/) - The EgoPAT3Dv2 dataset includes 12 distinct scenes and more than 5,400 clips. It features data in various modalities, including RGB, depth, IMU, and point clouds. The dataset captures rearrangement tasks performed by different individuals in diverse scenes, recorded using Microsoft Azure Kinect devices mounted overhead. ICRA 2024. [[paper]](https://arxiv.org/abs/2403.05046)

- [EgoExo-Fitness] - EgoExo-Fitness is a full-body action understanding dataset with synchronized egocentric and third-person fitness videos, enriched with detailed annotations. It offers two-level action boundaries, technical keypoint checks, and quality scores to evaluate “what,” “when,” and “how well” actions are performed. ECCV 2024. [[paper]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3057_ECCV_2024_paper.php) [[code]](https://github.com/iSEE-Laboratory/EgoExo-Fitness/tree/main)

- [Nymeria](https://www.projectaria.com/datasets/nymeria/) - It captures synchronized multimodal data, including egocentric and third-person perspectives, for 300 hours of natural daily activities across 50 locations. It features detailed motion capture, hierarchical language descriptions (310.5K sentences, 8.64M words), and scenarios like cooking and hiking from 264 participants using advanced wearable devices. ECCV 2024. [[paper]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3541_ECCV_2024_paper.php)

- [EgoPet](https://www.amirbar.net/egopet/) - It offers over 84 hours of egocentric video footage from animals like dogs, cats, eagles, and turtles, showcasing their daily lives. It includes 6,646 video segments sourced from TikTok and YouTube, with cats and dogs representing the majority of data. This rich and diverse dataset highlights unique perspectives and behaviors across various species, enabling detailed analysis of animal interactions. ECCV 2024. [[paper]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5469_ECCV_2024_paper.php)

- [EgoBody3M] - First large-scale real-image dataset for egocentric body tracking, with a realistic VR headset configuration and diverse subjects and motions. The dataset contains 2688 sequences

from 120 subjects. ECCV 2024. [[paper]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10261_ECCV_2024_paper.php)

- [Ego-Exo4D](https://ego-exo4d-data.org/) - Ego-Exo4D, a vast multimodal multiview video dataset capturing skilled human activities in both egocentric and exocentric perspectives (e.g., sports, music, dance). With 800+ participants in 13 cities, it offers 1,422 hours of combined footage, featuring diverse activities in 131 natural scene contexts, ranging from 1 to 42 minutes per video. CVPR 2024. [[paper]](https://arxiv.org/abs/2311.18259)

- [UnrealEgo2-UnrealEgo-RW](https://4dqv.mpi-inf.mpg.de/UnrealEgo2/) - UnrealEgo2 Dataset: An expanded dataset capturing over 15,200 motions of realistic 3D human models with a glasses-based device, offering 1.25 million stereo views and comprehensive joint annotations. UnrealEgo-RW Dataset: A real-world dataset utilizing a compact mobile device with fisheye cameras, designed for versatile egocentric image capture in various environments. CVPR 2024. [[paper]](https://openaccess.thecvf.com//content/CVPR2024/html/Akada_3D_Human_Pose_Perception_from_Egocentric_Stereo_Videos_CVPR_2024_paper) [[code]](https://unrealego.mpi-inf.mpg.de/)

- [TF2023] - A novel dataset featuring synchronized first-person and third-person views, including masks of camera wearers linked to their respective views. It consists of 208,794 training and 87,449 testing image pairs, with no actor overlap between sets. Each scene averages 4.29 actors, focusing on complex interactions like puzzle games, enhancing its value for cross-view matching in egocentric vision. CVPR 2024. [[paper]](https://openaccess.thecvf.com//content/CVPR2024/html/Zhao_Fusing_Personal_and_Environmental_Cues_for_Identification_and_Segmentation_of_CVPR_2024_paper) [[code]](https://github.com/ziweizhao1993/PEN)

- [TACO](https://taco2024.github.io/) - A large-scale dataset of real-world bimanual tool-object interactions, featuring 131 tool-action-object triplets across 2.5K motion sequences and 5.2M frames with egocentric and 3rd-person views. TACO enables benchmarks in action recognition, hand-object motion forecasting, and grasp synthesis, advancing generalization research in human-object interactions. CVPR 2024. [[paper]](https://openaccess.thecvf.com//content/CVPR2024/html/Liu_TACO_Benchmarking_Generalizable_Bimanual_Tool-ACtion-Object_Understanding_CVPR_2024_paper)

- [HOT3D](https://www.projectaria.com/datasets/hot3d/) - HOT3D is benchmark dataset for egocentric vision-based understanding of 3D hand-object interactions. The dataset offers over 833 minutes (more than 3.7M images) of multi-view RGB/monochrome image streams showing 19 subjects interacting with 33 diverse rigid objects, multi-modal signals such as eye gaze or scene point clouds, as well as comprehensive ground truth annotations including 3D poses of objects, hands, and cameras, and 3D models of hands and objects.  2024. [[paper]](https://arxiv.org/abs/2406.09598) [[code]](https://github.com/TimSchoonbeek/IndustReal)

- [ADL4D] - ADL4D dataset offers a novel perspective on human-object interactions, providing video sequences of everyday activities involving multiple people and objects interacting simultaneously.  2024. [[paper]](https://arxiv.org/abs/2402.17758)