Informazioni cronologiche,Indirizzo email,Title,Author list,Conference,Year,Category,Section,Sub-section,Short description (1-2 lines),Link paper,Link page,Link code,Link video,Note,Git Command
,,Egocentric Vision-based Action Recognition: A survey," Adrián Núñez-Marcos, Gorka Azkune, Ignacio Arganda-Carreras",Neurocomputing,2021,Surveys,,,,https://www.sciencedirect.com/science/article/pii/S0925231221017586,,,,,
,,Predicting the future from first person (egocentric) vision: A survey," Ivan Rodin, Antonino Furnari, Dimitrios Mavroedis, Giovanni Maria Farinella",CVIU,2021,Surveys,,,,https://arxiv.org/abs/2107.13411,,,,,
,,Analysis of the hands in egocentric vision: A survey," Andrea Bandini, José Zariffa",TPAMI,2020,Surveys,,,,https://arxiv.org/abs/1912.10867,,,,,
,,Summarization of Egocentric Videos: A Comprehensive Survey," Ana Garcia del Molino, Cheston Tan, Joo-Hwee Lim, Ah-Hwee Tan",THMS,2017,Surveys,,,,https://ieeexplore.ieee.org/abstract/document/7750564,,,,,
,,A survey of activity recognition in egocentric lifelogging datasets," El Asnaoui Khalid, Aksasse Hamid, Aksasse Brahim, Ouanan Mohammed",WITS,2017,Surveys,,,,https://ieeexplore.ieee.org/abstract/document/7934659,,,,,
,,Recognition of Activities of Daily Living with Egocentric Vision: A Review," Thi-Hoa-Cuc Nguyen, Jean-Christophe Nebel, Francisco Florez-Revuelta",Sensors,2016,Surveys,,,,https://www.mdpi.com/1424-8220/16/1/72,,,,,
,,The Evolution of First Person Vision Methods: A Survey," Alejandro Betancourt, Pietro Morerio, Carlo S. Regazzoni, Matthias Rauterberg",TCSVT,2015,Surveys,,,,https://arxiv.org/abs/1409.1484,,,,,
,,Stacked Temporal Attention: Improving First-person Action Recognition by Emphasizing Discriminative Clips," Lijin Yang, Yifei Huang, Yusuke Sugano, Yoichi Sato",BMVC,2021,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/abs/2112.01038,,,,,
,,With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition," Evangelos Kazakos, Jaesung Huh, Arsha Nagrani, Andrew Zisserman, Dima Damen",BMVC,2021,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/abs/2111.01024,,,,,
,,Interactive Prototype Learning for Egocentric Action Recognition," Xiaohan Wang, Linchao Zhu, Heng Wang, Yi Yang",ICCV,2021,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Interactive_Prototype_Learning_for_Egocentric_Action_Recognition_ICCV_2021_paper.html,,,,,
,,Learning to Recognize Actions on Objects in Egocentric Video with Attention Dictionaries," Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz",T-PAMI,2021,Papers,Action/Activity Recognition,Action Recognition,,https://ieeexplore.ieee.org/abstract/document/9353268,,,,,
,,Slow-Fast Auditory Streams For Audio Recognition," Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen",ICASSP,2021,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/abs/2103.03516,,,,,
,,Integrating Human Gaze Into Attention for Egocentric Activity Recognition," Kyle Min, Jason J. Corso",WACV,2021,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content/WACV2021/html/Min_Integrating_Human_Gaze_Into_Attention_for_Egocentric_Activity_Recognition_WACV_2021_paper.html,,,,,
,,Self-Supervised Joint Encoding of Motion and Appearance for First Person Action Recognition," Mirco Planamente, Andrea Bottino, Barbara Caputo",ICPR,2020,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/pdf/2002.03982.pdf,,,,,
,,Gate-Shift Networks for Video Action Recognition," Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz",CVPR,2020,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content_CVPR_2020/html/Sudhakaran_Gate-Shift_Networks_for_Video_Action_Recognition_CVPR_2020_paper.html,,https://github.com/swathikirans/GSM,,,
,,Trear: Transformer-based RGB-D Egocentric Action Recognition," Xiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, Wanqing Li",TCDS,2020,Papers,Action/Activity Recognition,Action Recognition,,https://ieeexplore.ieee.org/abstract/document/9312201?casa_token=VjrXPrZDuSgAAAAA:ezQgxMoeH7q3fxl8su7zg1yghkp60nbxCwU3FxyZEKWghbUVozmKmS_YE99AYceBr3lxA6Ud,,,,,
,,EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition," Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima",ICCV,2019,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf,,https://github.com/ekazakos/temporal-binding-network,,,
,,Learning Spatiotemporal Attention for Egocentric Action Recognition," Minlong Lu, Danping Liao, Ze-Nian Li",WICCV,2019,Papers,Action/Activity Recognition,Action Recognition,,http://openaccess.thecvf.com/content_ICCVW_2019/papers/EPIC/Lu_Learning_Spatiotemporal_Attention_for_Egocentric_Action_Recognition_ICCVW_2019_paper.pdf,,,,,
,,Multitask Learning to Improve Egocentric Action Recognition," Georgios Kapidis, Ronald Poppe, Elsbeth van Dam, Lucas Noldus, Remco Veltkamp",WICCV,2019,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/abs/1909.06761,,,,,
,,Seeing and Hearing Egocentric Actions: How Much Can We Learn?," Alejandro Cartas, Jordi Luque, Petia Radeva, Carlos Segura, Mariella Dimiccoli",WICCV,2019,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/abs/1910.06693,,,,,
,,Deep Attention Network for Egocentric Action Recognition," Minlong Lu, Simon Fraser, Ze-Nian Li, Yueming Wang, Gang Pan",TIP,2019,Papers,Action/Activity Recognition,Action Recognition,,https://ieeexplore.ieee.org/abstract/document/8653357,,,,,
,,LSTA: Long Short-Term Attention for Egocentric Action Recognition," Sudhakaran, Swathikiran and Escalera, Sergio and Lanz, Oswald",CVPR,2019,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content_CVPR_2019/papers/Sudhakaran_LSTA_Long_Short-Term_Attention_for_Egocentric_Action_Recognition_CVPR_2019_paper.pdf,,https://github.com/swathikirans/LSTA,,,
,,Long-Term Feature Banks for Detailed Video Understanding," Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krähenbühl, Ross Girshick",CVPR,2019,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/abs/1812.05038,,,,,
,,Attention is All We Need: Nailing Down Object-centric Attention for Egocentric Activity Recognition," Swathikiran Sudhakaran, Oswald Lanz",BMVC,2018,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/abs/1807.11794,,,,,
,,Egocentric Activity Recognition on a Budget," Possas, Rafael and Caceres, Sheila Pinto and Ramos, Fabio",CVPR,2018,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content_cvpr_2018/papers/Possas_Egocentric_Activity_Recognition_CVPR_2018_paper.pdf,,,,,
,,In the eye of beholder: Joint learning of gaze and actions in first person video," Li, Y., Liu, M., & Rehg, J. M.",ECCV,2018,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content_ECCV_2018/papers/Yin_Li_In_the_Eye_ECCV_2018_paper.pdf,,,,,
,,Egocentric Gesture Recognition Using Recurrent 3D Convolutional Neural Networks with Spatiotemporal Transformer Modules," Cao, Congqi and Zhang, Yifan and Wu, Yi and Lu, Hanqing and Cheng, Jian",ICCV,2017,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content_ICCV_2017/papers/Cao_Egocentric_Gesture_Recognition_ICCV_2017_paper.pdf,,,,,
,,Action recognition in RGB-D egocentric videos," Yansong Tang, Yi Tian, Jiwen Lu, Jianjiang Feng, Jie Zhou",ICIP,2017,Papers,Action/Activity Recognition,Action Recognition,,https://ieeexplore.ieee.org/document/8296915,,,,,
,,Trajectory Aligned Features For First Person Action Recognition," S. Singh, C. Arora, and C.V. Jawahar", Pattern Recognition,2017,Papers,Action/Activity Recognition,Action Recognition,,http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/JournalPublications/2016/Suriya_2016_Trajectory_Features.pdf,,,,,
,,Modeling Sub-Event Dynamics in First-Person Action Recognition," Hasan F. M. Zaki, Faisal Shafait, Ajmal Mian",CVPR,2017,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content_cvpr_2017/html/Zaki_Modeling_Sub-Event_Dynamics_CVPR_2017_paper.html,,,,,
,,First Person Action Recognition Using Deep Learned Descriptors," S. Singh, C. Arora, and C.V. Jawahar",CVPR,2016,Papers,Action/Activity Recognition,Action Recognition,,https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S12-15.pdf,,https://github.com/suriyasingh/EgoConvNet,,,
,,Delving into egocentric actions," Li, Y., Ye, Z., & Rehg, J. M.",CVPR,2015,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content_cvpr_2015/papers/Li_Delving_Into_Egocentric_2015_CVPR_paper.pdf,,,,,
,,Pooled Motion Features for First-Person Videos," Michael S. Ryoo, Brandon Rothrock and Larry H. Matthies",CVPR,2015,Papers,Action/Activity Recognition,Action Recognition,,https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ryoo_Pooled_Motion_Features_2015_CVPR_paper.pdf,,,,,
,,Generating Notifications for Missing Actions: Don't forget to turn the lights off!," Soran, Bilge, Ali Farhadi, and Linda Shapiro",ICCV,2015,Papers,Action/Activity Recognition,Action Recognition,,https://homes.cs.washington.edu/~ali/alarm-iccv.pdf,,,,,
,,First-Person Activity Recognition: What Are They Doing to Me?, M. S. Ryoo and L. Matthies,CVPR,2013,Papers,Action/Activity Recognition,Action Recognition,,http://cvrc.ece.utexas.edu/mryoo/papers/cvpr2013_ryoo.pdf,,,,,
,,Detecting activities of daily living in first-person camera views," Pirsiavash, H., & Ramanan, D.",CVPR,2012,Papers,Action/Activity Recognition,Action Recognition,,https://www.cs.cmu.edu/~deva/papers/ADL_2012.pdf,,,,,
,,Learning to recognize daily actions using gaze," Fathi, A., Li, Y., & Rehg, J. M",ECCV,2012,Papers,Action/Activity Recognition,Action Recognition,,http://ai.stanford.edu/~alireza/publication/ECCV12.pdf,,,,,
,,Egocentric Human-Object Interaction Detection Exploiting Synthetic Data," Rosario Leonardi, Francesco Ragusa, Antonino Furnari, Giovanni Maria Farinella",ICIAP,2022,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2204.07061,,,,,
,,Learning Visual Affordance Grounding from Demonstration Videos," Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao",,2021,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2108.05675,,,,,
,,Domain and View-point Agnostic Hand Action Recognition," Alberto Sabater, Iñigo Alonso, Luis Montesano, Ana C. Murillo",,2021,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2103.02303,,,,,
,,Understanding Egocentric Hand-Object Interactions from Hand Estimation," Yao Lu, Walterio W. Mayol-Cuevas",,2021,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2109.14657,,,,,
,,Egocentric Hand-object Interaction Detection and Application," Yao Lu, Walterio W. Mayol-Cuevas",,2021,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2109.14734,,,,,
,,The MECCANO Dataset: Understanding Human-Object Interactions from Egocentric Videos in an Industrial-like Domain," Francesco Ragusa, Antonino Furnari, Salvatore Livatino, Giovanni Maria Farinella",WACV,2021,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2010.05654,,,,,
,,Is First Person Vision Challenging for Object Tracking?," Matteo Dunnhofer, Antonino Furnari, Giovanni Maria Farinella, Christian Micheloni",WICCV,2021,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2011.12263,,,,,
,,Real Time Egocentric Object Segmentation: THU-READ Labeling and Benchmarking Results," E. Gonzalez-Sosa, G. Robledo, D. Gonzalez-Morin, P. Perez-Garcia, A. Villegas",WCVPR,2021,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2106.04957,,,,,
,,Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video," Miao Liu, Siyu Tang, Yin Li, James M. Rehg",ECCV,2020,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460681.pdf,,,,,
,,Understanding Human Hands in Contact at Internet Scale," Dandan Shan, Jiaqi Geng, Michelle Shu, David F. Fouhey",CVPR,2020,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content_CVPR_2020/html/Shan_Understanding_Human_Hands_in_Contact_at_Internet_Scale_CVPR_2020_paper.html,,,,,
,,Generalizing Hand Segmentation in Egocentric Videos with Uncertainty-Guided Model Adaptation, Minjie Cai and Feng Lu and Yoichi Sato,CVPR,2020,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_Generalizing_Hand_Segmentation_in_Egocentric_Videos_With_Uncertainty-Guided_Model_Adaptation_CVPR_2020_paper.pdf,,https://github.com/cai-mj/UMA,,,
,,Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild," Dominik Kulon, Riza Alp Güler, Iasonas Kokkinos, Michael Bronstein, Stefanos Zafeiriou",CVPR,2020,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content_CVPR_2020/papers/Kulon_Weakly-Supervised_Mesh-Convolutional_Hand_Reconstruction_in_the_Wild_CVPR_2020_paper.pdf,,,,,
,,Hand-Priming in Object Localization for Assistive Egocentric Vision," Lee, Kyungjun and Shrivastava, Abhinav and Kacorri, Hernisa",WACV,2020,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content_WACV_2020/papers/Lee_Hand-Priming_in_Object_Localization_for_Assistive_Egocentric_Vision_WACV_2020_paper.pdf,,,,,
,,Learning joint reconstruction of hands and manipulated objects," Yana Hasson, Gül Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J. Black, Ivan Laptev, Cordelia Schmid",CVPR,2020,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content_CVPR_2019/papers/Hasson_Learning_Joint_Reconstruction_of_Hands_and_Manipulated_Objects_CVPR_2019_paper.pdf,,,,,
,,H+O: Unified Egocentric Recognition of 3D Hand-Object Poses and Interactions," Tekin, Bugra and Bogo, Federica and Pollefeys, Marc",CVPR,2019,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content_CVPR_2019/papers/Tekin_HO_Unified_Egocentric_Recognition_of_3D_Hand-Object_Poses_and_Interactions_CVPR_2019_paper.pdf,,,https://youtu.be/ko6kNZ9DuAk?t=3240,,
,,From Lifestyle VLOGs to Everyday Interaction, David F. Fouhey and Weicheng Kuo and Alexei A. Efros and Jitendra Malik,CVPR,2018,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0733.pdf,,,,,
,,Analysis of Hand Segmentation in the Wild,"Aisha Urooj, Ali Borj",CVPR,2018,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/pdf/1803.03317,,,,,
,,First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations," Garcia-Hernando, Guillermo and Yuan, Shanxin and Baek, Seungryul and Kim, Tae-Kyun",CVPR,2018,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content_cvpr_2018/papers/Garcia-Hernando_First-Person_Hand_Action_CVPR_2018_paper.pdf,,https://github.com/guiggh/hand_pose_action,,,
,,Jointly Recognizing Object Fluents and Tasks in Egocentric Videos," Liu, Yang and Wei, Ping and Zhu, Song-Chun",ICCV,2017,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Jointly_Recognizing_Object_ICCV_2017_paper.pdf,,,,,
,,Egocentric Gesture Recognition Using Recurrent 3D Convolutional Neural Networks with Spatiotemporal Transformer Modules," Cao, Congqi and Zhang, Yifan and Wu, Yi and Lu, Hanqing and Cheng, Jian",ICCV,2017,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content_ICCV_2017/papers/Cao_Egocentric_Gesture_Recognition_ICCV_2017_paper.pdf,,,,,
,,First Person Action-Object Detection with EgoNet," Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi",,2017,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/1603.04908,,,,,
,,Understanding Hand-Object Manipulation with Grasp Types and Object Attributes, Minjie Cai and Kris M. Kitani and Yoichi Sato, Robotics: Science and Systems,2018,Papers,Action/Activity Recognition,Hand-Object Interactions,,http://www.cs.cmu.edu/~kkitani/pdf/CKY-RSS16.pdf,,,,,
,,Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions," Bambach, S., Lee, S., Crandall, D. J., & Yu, C.",ICCV,2015,Papers,Action/Activity Recognition,Hand-Object Interactions,,http://homes.sice.indiana.edu/sbambach/papers/iccv-egohands.pdf,,,,,
,,Understanding Everyday Hands in Action From RGB-D Images," Gregory Rogez, James S. Supancic III, Deva Ramanan",ICCV,2015,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content_iccv_2015/html/Rogez_Understanding_Everyday_Hands_ICCV_2015_paper.html,,,,,
,,"You-Do, I-Learn: Discovering Task Relevant Objects and their Modes of Interaction from Multi-User Egocentric Video"," Dima Damen, Teesid Leelasawassuk, Osian Haines, Andrew Calway, and Walterio Mayol-Cuevas",BMVC,2014,Papers,Action/Activity Recognition,Hand-Object Interactions,,http://www.bmva.org/bmvc/2014/files/paper059.pdf,,,,,
,,Detecting Snap Points in Egocentric Video with a Web Photo Prior, Bo Xiong and Kristen Grauman,ECCV,2014,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://www.cs.utexas.edu/~grauman/papers/bo-eccv2014.pdf,,http://vision.cs.utexas.edu/projects/ego_snappoints/#code,,,
,,3D Hand Pose Detection in Egocentric RGB-D Images," Grégory Rogez, Maryam Khademi, J. S. Supančič III, J. M. M. Montiel, Deva Ramanan",WECCV,2014,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://link.springer.com/chapter/10.1007/978-3-319-16178-5_25,,,,,
,,Pixel-level hand detection in ego-centric videos,"Li, Cheng, Kris M. Kitani",CVPR,2013,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Pixel-Level_Hand_Detection_2013_CVPR_paper.pdf,,https://github.com/irllabs/handtrack,https://youtu.be/N756YmLpZyY,,
,,Learning to recognize objects in egocentric activities," Fathi, A., Ren, X., & Rehg, J. M.",CVPR,2011,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://homes.cs.washington.edu/~xren/publication/fathi_cvpr11_egocentric_objects.pdf,,,,,
,,Context-based vision system for place and object recognition," Torralba, A., Murphy, K. P., Freeman, W. T., & Rubin, M. A.",ICCV,2003,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://www.cs.ubc.ca/~murphyk/Papers/iccv03.pdf,,,,,
,,Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition," Mirco Planamente, Chiara Plizzari, Emanuele Alberti, Barbara Caputo",WACV,2022,Papers,Action/Activity Recognition,Usupervised Domain Adaptation,,https://arxiv.org/abs/2110.10101,,,,,
,,Differentiated Learning for Multi-Modal Domain Adaptation," Jianming Lv, Kaijie Liu, Shengfeng He",MM,2021,Papers,Action/Activity Recognition,Usupervised Domain Adaptation,,https://dl.acm.org/doi/pdf/10.1145/3474085.3475660?casa_token=wOh7PYXIrGoAAAAA:WBP-sajm70r9KKNqNcwM7RIMW9D_re7MC56V10yq3_GCh4JafS_JegifZJ8--87l5TEcucuGaTYM,,,,,
,,Domain Adaptation in Multi-View Embedding for Cross-Modal Video Retrieval," Jonathan Munro, Michael Wray, Diane Larlus, Gabriela Csurka, Dima Damen",,2021,Papers,Action/Activity Recognition,Usupervised Domain Adaptation,,https://arxiv.org/abs/2110.12812,,,,,
,,Contrast and Mix: Temporal Contrastive Video Domain Adaptation with Background Mixing," Aadarsh Sahoo, Rutav Shah, Rameswar Panda, Kate Saenko, Abir Das",NIPS,2021,Papers,Action/Activity Recognition,Usupervised Domain Adaptation,,https://openreview.net/forum?id=a1wQOh27zcy,,,,,
,,Learning Cross-modal Contrastive Features for Video Domain Adaptation," Donghyun Kim, Yi-Hsuan Tsai, Bingbing Zhuang, Xiang Yu, Stan Sclaroff, Kate Saenko, Manmohan Chandraker",ICCV,2021,Papers,Action/Activity Recognition,Usupervised Domain Adaptation,,https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Learning_Cross-Modal_Contrastive_Features_for_Video_Domain_Adaptation_ICCV_2021_paper.pdf,,,,,
,,Spatio-temporal Contrastive Domain Adaptation for Action Recognition," Xiaolin Song, Sicheng Zhao, Jingyu Yang, Huanjing Yue, Pengfei Xu, Runbo Hu, Hua Chai",CVPR,2021,Papers,Action/Activity Recognition,Usupervised Domain Adaptation,,https://openaccess.thecvf.com/content/CVPR2021/html/Song_Spatio-temporal_Contrastive_Domain_Adaptation_for_Action_Recognition_CVPR_2021_paper.html,,,,,
,,Multi-Modal Domain Adaptation for Fine-Grained Action Recognition," Jonathan Munro, Dima Damen",CVPR,2020,Papers,Action/Activity Recognition,Usupervised Domain Adaptation,,https://openaccess.thecvf.com/content_CVPR_2020/html/Munro_Multi-Modal_Domain_Adaptation_for_Fine-Grained_Action_Recognition_CVPR_2020_paper.html,,,,,
,,Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition," Mirco Planamente, Chiara Plizzari, Emanuele Alberti, Barbara Caputo",WACV,2022,Papers,Action/Activity Recognition,Domain Generalization,,https://arxiv.org/abs/2110.10101,,,,,
,,Action Anticipation Using Pairwise Human-Object Interactions and Transformers, Debaditya Roy; Basura Fernando,TIP,2021,Papers,Action Anticipation,Short-Term Action Anticipation,,https://ieeexplore.ieee.org/abstract/document/9546623?casa_token=S642phYzKBsAAAAA:N3U0R4Hj7qiWztApTVHFJitkK8zFux5RTqTzjE6fRaT8luL4gVW-l-Fzoqd-K4u0x0bHvGgpjPI,,,,,
,,Higher Order Recurrent Space-Time Transformer for Video Action Prediction," Tsung-Ming Tai, Giuseppe Fiameni, Cheng-Kuang Lee, Oswald Lanz",ArXiv,2021,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2104.08665,,,,,
,,Anticipating Human Actions by Correlating Past With the Future With Jaccard Similarity Measures," Basura Fernando, Samitha Herath",CVPR,2021,Papers,Action Anticipation,Short-Term Action Anticipation,,https://openaccess.thecvf.com/content/CVPR2021/html/Fernando_Anticipating_Human_Actions_by_Correlating_Past_With_the_Future_With_CVPR_2021_paper.html,,,,,
,,Towards Streaming Egocentric Action Anticipation," Antonino Furnari, Giovanni Maria Farinella",arXiv,2021,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2110.05386,,,,,
,,Multimodal Global Relation Knowledge Distillation for Egocentric Action Anticipation," Y Huang, X Yang, C Xu",ACM,2021,Papers,Action Anticipation,Short-Term Action Anticipation,,https://dl.acm.org/doi/abs/10.1145/3474085.3475327?casa_token=S_eM0ZcL9G8AAAAA:n9-Xa3-WzAD5NGx9h9WA7ZnBFW5Xzv-QYu-wWYtUaqpYAagALI37qL1rc3WWawgiNf_0VrtOWX0,,,,,
,,Multi-Modal Temporal Convolutional Network for Anticipating Actions in Egocentric Videos," Olga Zatsarynna, Yazan Abu Farha, Juergen Gall",CVPRW,2021,Papers,Action Anticipation,Short-Term Action Anticipation,,https://openaccess.thecvf.com/content/CVPR2021W/Precognition/html/Zatsarynna_Multi-Modal_Temporal_Convolutional_Network_for_Anticipating_Actions_in_Egocentric_Videos_CVPRW_2021_paper.html,,,,,
,,Self-Regulated Learning for Egocentric Video Activity Anticipation, Zhaobo Qi; Shuhui Wang; Chi Su; Li Su; Qingming Huang; Qi Tian,T-PAMI,2021,Papers,Action Anticipation,Short-Term Action Anticipation,,https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34,,,,,
,,Anticipative Video Transformer," Rohit Girdhar, Kristen Grauman",ICCV,2021,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2106.02036,,,,,
,,What If We Could Not See? Counterfactual Analysis for Egocentric Action Anticipation," T Zhang, W Min, J Yang, T Liu, S Jiang, Y Rui",IJCAI,2021,Papers,Action Anticipation,Short-Term Action Anticipation,,ijcai.org/proceedings/2021/182,,,,,
,,Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video," Antonino Furnari, Giovanni Maria Farinella",T-PAMI,2020,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2005.02190,,,,,
,,Knowledge Distillation for Action Anticipation via Label Smoothing," Guglielmo Camporese, Pasquale Coscia, Antonino Furnari, Giovanni Maria Farinella, Lamberto Ballan",ICPR,2020,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2004.07711,,,,,
,,An Egocentric Action Anticipation Framework via Fusing Intuition and Analysis," Tianyu Zhang, Weiqing Min, Ying Zhu, Yong Rui, Shuqiang Jiang",ACM,2020,Papers,Action Anticipation,Short-Term Action Anticipation,,https://dl.acm.org/doi/10.1145/3394171.3413964,,,,,
,,What Would You Expect? Anticipating Egocentric Actions with Rolling-Unrolling LSTMs and Modality Attention," Antonino Furnari, Giovanni Maria Farinella",ICCV,2019,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/pdf/1905.09035,,https://github.com/fpv-iplab/rulstm,,,
,,Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video," Miao Liu, Siyu Tang, Yin Li, James M. Rehg",ECCV,2020,Papers,Action Anticipation,Short-Term Action Anticipation,,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460681.pdf,,,,,
,,Leveraging the Present to Anticipate the Future in Videos," Antoine Miech, Ivan Laptev, Josef Sivic, Heng Wang, Lorenzo Torresani, Du Tran",CVPRW,2019,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2004.07711,,,,,
,,Zero-Shot Anticipation for Instructional Activities," Fadime Sener, Angela Yao",ICCV,2019,Papers,Action Anticipation,Short-Term Action Anticipation,,https://ieeexplore.ieee.org/document/9008304,,,,,
,,Learning to Anticipate Egocentric Actions by Imagination," Yu Wu, Linchao Zhu, Xiaohan Wang, Yi Yang, Fei Wu",TIP,2021,Papers,Action Anticipation,Long-Term Action Anticipation,,https://arxiv.org/pdf/2101.04924.pdf,,,,,
,,On Diverse Asynchronous Activity Anticipation, He Zhao and Richard P. Wildes,ECCV,2020,Papers,Action Anticipation,Long-Term Action Anticipation,,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123740766.pdf,,,,,
,,Time-Conditioned Action Anticipation in One Shot," Qiuhong Ke, Mario Fritz, Bernt Schiele",CVPR,2019,Papers,Action Anticipation,Long-Term Action Anticipation,,https://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Time-Conditioned_Action_Anticipation_in_One_Shot_CVPR_2019_paper.html,,,,,
,,When Will You Do What? - Anticipating Temporal Occurrences of Activities," Anticipating Temporal Occurrences of Activities](https://openaccess.thecvf.com/content_cvpr_2018/html/Abu_Farha_When_Will_You_CVPR_2018_paper.html) - Yazan Abu Farha, Alexander Richard, Juergen Gall",CVPR,2018,Papers,Action Anticipation,Long-Term Action Anticipation,,https://openaccess.thecvf.com/content_cvpr_2018/html/Abu_Farha_When_Will_You_CVPR_2018_paper.html,,,,,
,,Joint Prediction of Activity Labels and Starting Times in Untrimmed Videos," Tahmida Mahmud, Mahmudul Hasan, Amit K. Roy-Chowdhury",ICCV,2017,Papers,Action Anticipation,Long-Term Action Anticipation,,https://openaccess.thecvf.com/content_ICCV_2017/papers/Mahmud_Joint_Prediction_of_ICCV_2017_paper.pdf,,,,,
,,First-Person Activity Forecasting with Online Inverse Reinforcement Learning," Nicholas Rhinehart, Kris M. Kitani",ICCV,2017,Papers,Action Anticipation,Long-Term Action Anticipation,,https://arxiv.org/pdf/1612.07796,,,https://youtu.be/rvVoW3iuq-s,,
,,Unsupervised gaze prediction in egocentric videos by energy-based surprise modeling," Aakur, S.N., Bagavathi, A.",ArXiv,2020,Papers,Action Anticipation,Future Gaze Prediction,,http://arxiv.org/abs/2001.11580,,,,,
,,Digging Deeper into Egocentric Gaze Prediction, Hamed R. Tavakoli and Esa Rahtu and Juho Kannala and Ali Borji,WACV,2019,Papers,Action Anticipation,Future Gaze Prediction,,https://arxiv.org/pdf/1904.06090,,,,,
,,Predicting Gaze in Egocentric Video by Learning Task-dependent Attention Transition," Huang, Y., Cai, M., Li, Z., & Sato, Y.",ECCV,2018,Papers,Action Anticipation,Future Gaze Prediction,,https://arxiv.org/pdf/1803.09125,,https://github.com/hyf015/egocentric-gaze-prediction,,,
,,Deep future gaze: Gaze anticipation on egocentric videos using adversarial networks," Zhang, M., Teck Ma, K., Hwee Lim, J., Zhao, Q., & Feng, J.",CVPR,2017,Papers,Action Anticipation,Future Gaze Prediction,,https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Deep_Future_Gaze_CVPR_2017_paper.pdf,,https://github.com/Mengmi/deepfuturegaze_gan,,,
,,Learning to predict gaze in egocentric video," Li, Yin, Alireza Fathi, and James M. Rehg",ICCV,2013,Papers,Action Anticipation,Future Gaze Prediction,,http://ai.stanford.edu/~alireza/publication/Li-Fathi-Rehg-ICCV13.pdf,,,,,
,,Forecasting Action through Contact Representations from First Person Video, Eadom Dessalene; Chinmaya Devaraj; Michael Maynord; Cornelia Fermuller; Yiannis Aloimonos,T-PAMI,2021,Papers,Action Anticipation,Trajectory prediction,,https://ieeexplore.ieee.org/abstract/document/9340014?casa_token=PUk2a8mN4CoAAAAA:ICkziPRIBtlxgzsyJm9ZVxUIzGnEq0phTHLOP8G8TxFlTIp159calFp8jZOdUCnxeWTknFjlB0w,,,,,
,,Multimodal Future Localization and Emergence Prediction for Objects in Egocentric View With a Reachability Prior," Makansi, Osama and Cicek, Ozgun and Buchicchio, Kevin and Brox, Thomas",CVPR,2020,Papers,Action Anticipation,Trajectory prediction,,https://openaccess.thecvf.com/content_CVPR_2020/papers/Makansi_Multimodal_Future_Localization_and_Emergence_Prediction_for_Objects_in_Egocentric_CVPR_2020_paper.pdf,,https://github.com/lmb-freiburg/FLN-EPN-RPN,,,
,,Understanding Human Hands in Contact at Internet Scale," Dandan Shan, Jiaqi Geng, Michelle Shu, David F. Fouhey",CVPR,2020,Papers,Action Anticipation,Trajectory prediction,,https://openaccess.thecvf.com/content_CVPR_2020/html/Shan_Understanding_Human_Hands_in_Contact_at_Internet_Scale_CVPR_2020_paper.html,,,,,
,,Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video," Miao Liu, Siyu Tang, Yin Li, James M. Rehg",ECCV,2020,Papers,Action Anticipation,Trajectory prediction,,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460681.pdf,,,,,
,,How Can I See My Future? FvTraj: Using First-person View for Pedestrian Trajectory Prediction," Huikun Bi, Ruisi Zhang, Tianlu Mao, Zhigang Deng, Zhaoqi Wang",ECCV,2020,Papers,Action Anticipation,Trajectory prediction,,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520562.pdf,,,https://youtu.be/HcsyH7zMHAw,,
,,Future Person Localization in First-Person Videos, Takuma Yagi; Karttikeya Mangalam; Ryo Yonetani; Yoichi Sato,CVPR,2018,Papers,Action Anticipation,Trajectory prediction,,https://ieeexplore.ieee.org/document/8578890,,,,,
,,Egocentric Future Localization," Park, Hyun Soo and Hwang, Jyh-Jing and Niu, Yedong and Shi, Jianbo",CVPR,2016,Papers,Action Anticipation,Trajectory prediction,,https://openaccess.thecvf.com/content_cvpr_2016/papers/Park_Egocentric_Future_Localization_CVPR_2016_paper.pdf,,,,,
,,Going deeper into first-person activity recognition," Ma, M., Fan, H., & Kitani, K. M.",CVPR,2016,Papers,Action Anticipation,Trajectory prediction,,http://www.cs.cmu.edu/~kkitani/pdf/MFK-CVPR2016.pdf,,,,,
,,EGO-TOPO: Environment Affordances from Egocentric Video," Nagarajan, Tushar and Li, Yanghao and Feichtenhofer, Christoph and Grauman, Kristen",CVPR,2020,Papers,Action Anticipation,Region prediction,,https://openaccess.thecvf.com/content_CVPR_2020/papers/Nagarajan_Ego-Topo_Environment_Affordances_From_Egocentric_Video_CVPR_2020_paper.pdf,,,,,
,, Forecasting human object interaction: Joint prediction of motor attention and egocentric activity," Liu, M., Tang, S., Li, Y., Rehg, J.",arXiv,2019,Papers,Action Anticipation,Region prediction,,http://arxiv.org/abs/1911.10967,,,,,
,,Forecasting Hands and Objects in Future Frames," Chenyou Fan, Jangwon Lee, Michael S. Ryoo",ECCVW,2018,Papers,Action Anticipation,Region prediction,,https://openaccess.thecvf.com/content_eccv_2018_workshops/w15/html/Fan_Forecasting_Hands_and_Objects_in_Future_Frames_ECCVW_2018_paper.html,,,,,
,,Next-active-object prediction from egocentric videos," Antonino Furnari, Sebastiano Battiato, Kristen Grauman, Giovanni Maria Farinella",JVCIR,2017,Papers,Action Anticipation,Region prediction,,https://www.sciencedirect.com/science/article/abs/pii/S1047320317301967,,,,,
,,First Person Action-Object Detection with EgoNet," G Bertasius, HS Park, SX Yu, J Shi",arXiv,2016,Papers,Action Anticipation,Region prediction,,https://arxiv.org/abs/1603.04908,,,,,
,,Unsupervised Learning of Important Objects From First-Person Videos," Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi",ICCV,2017,Papers,Action Anticipation,Region prediction,,https://openaccess.thecvf.com/content_iccv_2017/html/Bertasius_Unsupervised_Learning_of_ICCV_2017_paper.html,,,,,
,,Attention Bottlenecks for Multimodal Fusion," Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, Chen Sun",NIPS,2021,Papers,Multi-Modalities,Audio-Visual,,https://arxiv.org/abs/2107.00135,,,,,
,,Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition," Mirco Planamente, Chiara Plizzari, Emanuele Alberti, Barbara Caputo",WACV,2022,Papers,Multi-Modalities,Audio-Visual,,https://arxiv.org/abs/2110.10101,,,,,
,,With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition," Evangelos Kazakos, Jaesung Huh, Arsha Nagrani, Andrew Zisserman, Dima Damen",BMVC,2021,Papers,Multi-Modalities,Audio-Visual,,https://arxiv.org/abs/2111.01024,,,,,
,,Slow-Fast Auditory Streams For Audio Recognition," Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen",ICASSP,2021,Papers,Multi-Modalities,Audio-Visual,,https://arxiv.org/abs/2103.03516,,,,,
,,Multi-modal Egocentric Activity Recognition using Audio-Visual Features," Mehmet Ali Arabacı, Fatih Özkan, Elif Surer, Peter Jančovič, Alptekin Temizel",MTA,2020,Papers,Multi-Modalities,Audio-Visual,,https://arxiv.org/pdf/1807.00612.pdf,,,,,
,,EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition," Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima",ICCV,2019,Papers,Multi-Modalities,Audio-Visual,,https://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf,,,,,
,,Seeing and Hearing Egocentric Actions: How Much Can We Learn?,"Alejandro Cartas, Jordi Luque, Petia Radeva, Carlos Segura, Mariella Dimiccoli",WICCV,2019,Papers,Multi-Modalities,Audio-Visual,,https://arxiv.org/abs/1910.06693,,,,,
,,Trear: Transformer-based RGB-D Egocentric Action Recognition," Xiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, Wanqing Li",TCDS,2020,Papers,Multi-Modalities,Depth,,https://ieeexplore.ieee.org/abstract/document/9312201?casa_token=VjrXPrZDuSgAAAAA:ezQgxMoeH7q3fxl8su7zg1yghkp60nbxCwU3FxyZEKWghbUVozmKmS_YE99AYceBr3lxA6Ud,,,,,
,,First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations," Garcia-Hernando, Guillermo and Yuan, Shanxin and Baek, Seungryul and Kim, Tae-Kyun",CVPR,2018,Papers,Multi-Modalities,Depth,,https://openaccess.thecvf.com/content_cvpr_2018/papers/Garcia-Hernando_First-Person_Hand_Action_CVPR_2018_paper.pdf,,https://github.com/guiggh/hand_pose_action,,,
,,Multi-stream Deep Neural Networks for RGB-D Egocentric Action Recognition," Yansong Tang, Zian Wang, Jiwen Lu, Jianjiang Feng, Jie Zhou",TCSVT,2018,Papers,Multi-Modalities,Depth,,http://www.cs.toronto.edu/~zianwang/MDNN/TCSVT18_MDNN.pdf,,,,,
,,Action recognition in RGB-D egocentric videos," Yansong Tang, Yi Tian, Jiwen Lu, Jianjiang Feng, Jie Zhou",ICIP,2017,Papers,Multi-Modalities,Depth,,https://ieeexplore.ieee.org/document/8296915,,,,,
,,Scene Semantic Reconstruction from Egocentric RGB-D-Thermal Videos," Rachel Luo, Ozan Sener, Silvio Savarese",3DV,2017,Papers,Multi-Modalities,Depth,,https://ieeexplore.ieee.org/abstract/document/8374614,,,,,
,,3D Hand Pose Detection in Egocentric RGB-D Images," Grégory Rogez, Maryam Khademi, J. S. Supančič III, J. M. M. Montiel, Deva Ramanan",WECCV,2014,Papers,Multi-Modalities,Depth,,https://link.springer.com/chapter/10.1007/978-3-319-16178-5_25,,,,,
,,Scene Semantic Reconstruction from Egocentric RGB-D-Thermal Videos," Rachel Luo, Ozan Sener, Silvio Savarese",3DV,2017,Papers,Multi-Modalities,Thermal,,https://ieeexplore.ieee.org/abstract/document/8374614,,,,,
,,E(GO)^2MOTION: Motion Augmented Event Stream for Egocentric Action Recognition," Chiara Plizzari, Mirco Planamente, Gabriele Goletto, Marco Cannici, Emanuele Gusso, Matteo Matteucci, Barbara Caputo",CVPR,2022,Papers,Multi-Modalities,Event,,https://arxiv.org/abs/2112.03596,,,,,
,,UnweaveNet: Unweaving Activity Stories," Will Price, Carl Vondrick, Dima Damen",,2021,Papers,Temporal Segmentation (Action Detection),,,https://arxiv.org/pdf/2112.10194.pdf,,,,,
,,Temporal Action Segmentation from Timestamp Supervision," Zhe Li, Yazan Abu Farha, Jurgen Gall",CVPR,2021,Papers,Temporal Segmentation (Action Detection),,,https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Temporal_Action_Segmentation_From_Timestamp_Supervision_CVPR_2021_paper.pdf,,,,,
,,Personal-Location-Based Temporal Segmentation of Egocentric Video for Lifelogging Applications," A. Furnari, G. M. Farinella, S. Battiato", Journal of Visual Communication and Image Representation,2017,Papers,Temporal Segmentation (Action Detection),,,https://iplab.dmi.unict.it/PersonalLocationSegmentation/downloads/furnari2018personal.pdf,,,,,
,,Temporal segmentation and activity classification from first-person sensing," Spriggs, Ekaterina H., Fernando De La Torre, and Martial Hebert, Computer Vision and Pattern Recognition Workshops",WCVPR,2009,Papers,Temporal Segmentation (Action Detection),,,https://ieeexplore.ieee.org/document/5204354,,,,,
,,Domain Adaptation in Multi-View Embedding for Cross-Modal Video Retrieval," Jonathan Munro, Michael Wray, Diane Larlus, Gabriela Csurka, Dima Damen",,2021,Papers,Retrieval ,,,https://arxiv.org/abs/2110.12812,,,,,
,,On Semantic Similarity in Video Retrieval," Michael Wray, Hazel Doughty, Dima Damen",CVPR,2021,Papers,Retrieval ,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Wray_On_Semantic_Similarity_in_Video_Retrieval_CVPR_2021_paper.pdf,,,,,
,,Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings," Michael Wray, Diane Larlus, Gabriela Csurka, Dima Damen",ICCV,2019,Papers,Retrieval ,,,https://openaccess.thecvf.com/content_ICCV_2019/papers/Wray_Fine-Grained_Action_Retrieval_Through_Multiple_Parts-of-Speech_Embeddings_ICCV_2019_paper.pdf,,,,,
,,Egocentric Video-Language Pretraining," Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao, Weijie Kong, Chengfei Cai, Hongfa Wang, Dima Damen, Bernard Ghanem, Wei Liu",,2022,Papers,Video-Language,,,https://arxiv.org/pdf/2206.01670.pdf,,,,,
,,Episodic Memory Question Answering," Samyak Datta, Sameer Dharur, Vincent Cartillier, Ruta Desai, Mukul Khanna, Dhruv Batra",CVPR,2022,Papers,Video-Language,,,https://arxiv.org/pdf/2205.01652.pdf,,,,,
,,Unifying Few- and Zero-Shot Egocentric Action Recognition,"Tyler R. Scott, Michael Shvartsman, Karl Ridgeway",CVPRW,2021,Papers,Few-Shot Action Recognition,,,https://arxiv.org/abs/2006.11393,,,,,
,,1000 Pupil Segmentations in a Second Using Haar Like Features and Statistical Learning," Wolfgang Fuhl, Johannes Schneider, Enkelejda Kasneci",WICCV,2021,Papers,Gaze,,,https://openaccess.thecvf.com/content/ICCV2021W/EPIC/html/Fuhl_1000_Pupil_Segmentations_in_a_Second_Using_Haar_Like_Features_ICCVW_2021_paper.html,,,,,
,,Ego-Exo: Transferring Visual Representations From Third-Person to First-Person Videos," Yanghao Li, Tushar Nagarajan, Bo Xiong, Kristen Grauman",CVPR,2021,Papers,From Third-Person to First-Person ,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Ego-Exo_Transferring_Visual_Representations_From_Third-Person_to_First-Person_Videos_CVPR_2021_paper.pdf,,,,,
,,Actor and Observer: Joint Modeling of First and Third-Person Videos, Gunnar A. Sigurdsson and Abhinav Gupta and Cordelia Schmid and Ali Farhadi and Karteek Alahari,CVPR,2018,Papers,From Third-Person to First-Person ,,,https://openaccess.thecvf.com/content_cvpr_2018/papers/Sigurdsson_Actor_and_Observer_CVPR_2018_paper.pdf,,https://github.com/gsig/actor-observer,,,
,,Making Third Person Techniques Recognize First-Person Actions in Egocentric Videos," Sagar Verma, Pravin Nagar, Divam Gupta, Chetan Arora",ICIP,2018,Papers,From Third-Person to First-Person ,,,https://ieeexplore.ieee.org/abstract/document/8451249?casa_token=p1k79yrTIkMAAAAA:QHlXMC8Y7qrCEDsdypGNZbh7zeoEPVEs2k6j5a0g1MkvA76Uf6_VDIfCzbiG2bWdU8EoFyagbK4,,,,,
,,Dynamics-regulated kinematic policy for egocentric pose estimation," Zhengyi Luo, Ryo Hachiuma, Ye Yuan, Kris Kitani",NIPS,2021,Papers,User Data from an Egocentric Point of View,,,https://proceedings.neurips.cc/paper/2021/hash/d1fe173d08e959397adf34b1d77e88d7-Abstract.html,,,,,
,,Estimating Egocentric 3D Human Pose in Global Space," Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, Christian Theobalt",ICCV,2021,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Estimating_Egocentric_3D_Human_Pose_in_Global_Space_ICCV_2021_paper.html,,,,,
,,Egocentric Pose Estimation From Human Vision Span," Hao Jiang, Vamsi Krishna Ithapu",ICCV,2021,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com/content/ICCV2021/html/Jiang_Egocentric_Pose_Estimation_From_Human_Vision_Span_ICCV_2021_paper.html,,,,,
,,EgoRenderer: Rendering Human Avatars From Egocentric Camera Images," Tao Hu, Kripasindhu Sarkar, Lingjie Liu, Matthias Zwicker, Christian Theobalt",ICCV,2021,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com/content/ICCV2021/html/Hu_EgoRenderer_Rendering_Human_Avatars_From_Egocentric_Camera_Images_ICCV_2021_paper.html,,,,,
,,Whose Hand Is This? Person Identification From Egocentric Hand Gestures," Satoshi Tsutsui, Yanwei Fu, David J. Crandall",WACV,2021,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com/content/WACV2021/html/Tsutsui_Whose_Hand_Is_This_Person_Identification_From_Egocentric_Hand_Gestures_WACV_2021_paper.html,,,,,
,,Recognizing Camera Wearer from Hand Gestures in Egocentric Videos," Daksh Thapar, Aditya Nigam, Chetan Arora",MM,2020,Papers,User Data from an Egocentric Point of View,,,https://dl.acm.org/doi/pdf/10.1145/3394171.3413654?casa_token=tlspOQU5qekAAAAA:rM0hbyyg1cvY5KRK16blErILxTO_OJpU9CIr8W9nDxBbdvjJBNxyKJ5GcNWTjrgJwV_H_Me8cFlj,,,,,
,,You2Me: Inferring Body Pose in Egocentric Video via First and Second Person Interactions," Ng, Evonne and Xiang, Donglai and Joo, Hanbyul and Grauman, Kristen",CVPR,2020,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com/content_CVPR_2020/papers/Ng_You2Me_Inferring_Body_Pose_in_Egocentric_Video_via_First_and_CVPR_2020_paper.pdf,,https://github.com/facebookresearch/you2me#,,,
,,Ego-Pose Estimation and Forecasting as Real-Time PD Control, Ye Yuan and Kris Kitani,ICCV,2019,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com/content_ICCV_2019/papers/Yuan_Ego-Pose_Estimation_and_Forecasting_As_Real-Time_PD_Control_ICCV_2019_paper.pdf,,https://github.com/Khrylx/EgoPose,,,
,,xR-EgoPose: Egocentric 3D Human Pose From an HMD Camera," Tome, Denis and Peluse, Patrick and Agapito, Lourdes and Badino, Hernan",ICCV,2019,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com/content_ICCV_2019/papers/Tome_xR-EgoPose_Egocentric_3D_Human_Pose_From_an_HMD_Camera_ICCV_2019_paper.pdf,,,,,
,,3D Ego-Pose Estimation via Imitation Learning," Ye Yuan, Kris Kitani",ECCV,2018,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com/content_ECCV_2018/html/Ye_Yuan_3D_Ego-Pose_Estimation_ECCV_2018_paper.html,,,,,
,,Egocentric Indoor Localization From Room Layouts and Image Outer Corners," Xiaowei Chen, Guoliang Fan",WICCV,2021,Papers,Localization ,,,https://openaccess.thecvf.com/content/ICCV2021W/EPIC/html/Chen_Egocentric_Indoor_Localization_From_Room_Layouts_and_Image_Outer_Corners_ICCVW_2021_paper.html,,,,,
,,Egocentric Activity Recognition and Localization on a 3D Map," Miao Liu, Lingni Ma, Kiran Somasundaram, Yin Li, Kristen Grauman, James M. Rehg, Chao Li",,2021,Papers,Localization ,,,https://arxiv.org/abs/2105.09544,,,,,
,,Egocentric Shopping Cart Localization," E. Spera, A. Furnari, S. Battiato, G. M. Farinella",ICPR,2018,Papers,Localization ,,,https://iplab.dmi.unict.it/EgocentricShoppingCartLocalization/home/_paper/egocentric%20shopping%20cart%20localization.pdf,,,,,
,,Recognizing personal locations from egocentric videos," Furnari, A., Farinella, G. M., & Battiato, S.",IEEE Transactions on Human-Machine Systems,2017,Papers,Localization ,,,https://ieeexplore.ieee.org/document/7588113,,,,,
,,Context-based vision system for place and object recognition," Torralba, A., Murphy, K. P., Freeman, W. T., & Rubin, M. A.",ICCV,2003,Papers,Localization ,,,https://www.cs.ubc.ca/~murphyk/Papers/iccv03.pdf,,,,,
,,Anonymizing Egocentric Videos," Daksh Thapar, Aditya Nigam, Chetan Arora",ICCV,2021,Papers,Privacy protection,,,https://openaccess.thecvf.com/content/ICCV2021/papers/Thapar_Anonymizing_Egocentric_Videos_ICCV_2021_paper.pdf,,,,,
,,Mitigating Bystander Privacy Concerns in Egocentric Activity Recognition with Deep Learning and Intentional Image Degradation," Dimiccoli, M., Marín, J., & Thomaz, E., Proceedings of the ACM on Interactive, Mobile", Wearable and Ubiquitous Technologies,2018,Papers,Privacy protection,,,http://users.ece.utexas.edu/~ethomaz/papers/j2.pdf,,,,,
,,Privacy-Preserving Human Activity Recognition from Extreme Low Resolution," Ryoo, M. S., Rothrock, B., Fleming, C., & Yang, H. J.",AAAI,2017,Papers,Privacy protection,,,https://arxiv.org/pdf/1604.03196,,,,,
,,EgoCom: A Multi-person Multi-modal Egocentric Communications Dataset, Curtis G. Northcutt and Shengxin Zha and Steven Lovegrove and Richard Newcombe,PAMI,2020,Papers,Social Interactions,,,https://ieeexplore.ieee.org/document/9200754,,,,,
,,Deep Dual Relation Modeling for Egocentric Interaction Recognition," Li, Haoxin and Cai, Yijun and Zheng, Wei-Shi",CVPR,2019,Papers,Social Interactions,,,https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Deep_Dual_Relation_Modeling_for_Egocentric_Interaction_Recognition_CVPR_2019_paper.pdf,,,,,
,,Recognizing Micro-Actions and Reactions from Paired Egocentric Videos," Yonetani, Ryo and Kitani, Kris M. and Sato, Yoichi",CVPR,2016,Papers,Social Interactions,,,https://openaccess.thecvf.com/content_cvpr_2016/papers/Yonetani_Recognizing_Micro-Actions_and_CVPR_2016_paper.pdf,,,,,
,,Social interactions: A first-person perspective," Fathi, A., Hodgins, J. K., & Rehg, J. M.",CVPR,2012,Papers,Social Interactions,,,http://www.cs.utexas.edu/~cv-fall2012/slides/jake-expt.pdf,,,,,
,,"Ego4D: Around the World in 3,000 Hours of Egocentric Video"," Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, Jitendra Malik",CVPR,2023,Papers,Multiple Egocentric Tasks,,,https://openaccess.thecvf.com/content/CVPR2023/html/Gong_MMG-Ego4D_Multimodal_Generalization_in_Egocentric_Action_Recognition_CVPR_2023_paper.html,,,https://drive.google.com/file/d/1oknfQIH9w1rXy6I1j5eUE6Cqh96UwZ4L/view?usp=sharing,,
,,Learning Visual Affordance Grounding from Demonstration Videos," Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao",,2021,Papers,Activity-context,,,https://arxiv.org/abs/2108.05675,,,,,
,,Shaping embodied agent behavior with activity-context priors from egocentric video," Tushar Nagarajan, Kristen Grauman",NIPS,2021,Papers,Activity-context,,,https://proceedings.neurips.cc/paper/2021/file/f8b7aa3a0d349d9562b424160ad18612-Paper.pdf,,,,,
,,EGO-TOPO: Environment Affordances from Egocentric Video," Tushar Nagarajan, Yanghao Li, Christoph Feichtenhofer, Kristen Grauman",CVPR,2020,Papers,Activity-context,,,https://openaccess.thecvf.com/content_CVPR_2020/html/Nagarajan_Ego-Topo_Environment_Affordances_From_Egocentric_Video_CVPR_2020_paper.html,,,,,
,,Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language," Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Pete Florence",,2022,Papers,Video summarization,,,https://arxiv.org/abs/2204.00598,,,,,
,,Egocentric video summarisation via purpose-oriented frame scoring and selection, V. Javier Traver and Dima Damen,,2022,Papers,Video summarization,,,https://www.sciencedirect.com/science/article/pii/S0957417421014159,,,,,
,,Multi-stream dynamic video Summarization," Mohamed Elfeki, Liqiang Wang, Ali Borji",WACV,2022,Papers,Video summarization,,,https://openaccess.thecvf.com/content/WACV2022/html/Elfeki_Multi-Stream_Dynamic_Video_Summarization_WACV_2022_paper.html,,,,,
,,"Together Recognizing, Localizing and Summarizing Actions in Egocentric Videos", Abhimanyu Sahu; Ananda S. Chowdhury,TIP,2021,Papers,Video summarization,,,https://ieeexplore.ieee.org/abstract/document/9399266?casa_token=R8LKJM45-MgAAAAA:Pfjxjt8k7l4SD_iopfL9JYsq2k6ShpZGATkXg-z5B4BTuPQV3A4HqhtZ2VqhVPtiIVbPIi_oaPU,,,,,
,,First person video summarization using different graph representations," Abhimanyu Sahu, Ananda S.Chowdhury", Pattern Recognition Letters,2021,Papers,Video summarization,,,https://www.sciencedirect.com/science/article/pii/S0167865521001008?casa_token=H7rMpQAduAsAAAAA:Aq6ryy4IihojkZ9Tj3LoYRxT66VO3KmdBIiRTJoDvd_WBNIsHJxhruPTSNrzR6NniRg8iYyk,,,,,
,,Summarizing egocentric videos using deep features and optimal clustering," Abhimanyu Sahu, Ananda S.Chowdhury",Neurocomputing,2020,Papers,Video summarization,,,https://www.sciencedirect.com/science/article/pii/S0925231220302952,,,,,
,,Text Synopsis Generation for Egocentric Videos, Aidean Sharghi; Niels da Vitoria Lobo; Mubarak Shah,ICPR,2020,Papers,Video summarization,,,https://ieeexplore.ieee.org/abstract/document/9412111?casa_token=Vf3uXoASupsAAAAA:CHg-misMhCLcZn-CWdUFFBLJ_SGlvsmZrAc-lfujd5yxVQSF0pr13RAYSdmrOTfYaTB0xKTj_Wg,,,,,
,,Shot Level Egocentric Video Co-summarization, Abhimanyu Sahu; Ananda S. Chowdhury,ICPR,2018,Papers,Video summarization,,,https://ieeexplore.ieee.org/document/8546119,,,,,
,,Personalized Egocentric Video Summarization of Cultural Tour on User Preferences Input, Patrizia Varini; Giuseppe Serra; Rita Cucchiara,IEEE Transactions on Multimedia,2017,Papers,Video summarization,,,https://ieeexplore.ieee.org/abstract/document/7931584?casa_token=gsvjlImpjQQAAAAA:F420NFVZd0V3igjGLVv8VpXnD1Ul5SakMxlfwcdAYCwNTsEjPgrLAKMhnUKX2VgOpoJgRm03XzI,,,,,
,,Highlight Detection with Pairwise Deep Ranking for First-Person Video Summarization, Ting Yao; Tao Mei; Yong Rui,CVPR,2016,Papers,Video summarization,,,https://ieeexplore.ieee.org/document/7780481,,,,,
,,Video Summarization with Long Short-term Memory," Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman",ECCV,2016,Papers,Video summarization,,,https://arxiv.org/abs/1605.08110,,,,,
,,Discovering Picturesque Highlights from Egocentric Vacation Videos," Vinay Bettadapura, Daniel Castro, Irfan Essa",arXiv,2016,Papers,Video summarization,,,https://arxiv.org/abs/1601.04406,,,,,
,,Spatial and temporal scoring for egocentric video summarization," Zhao Guo, Lianli Gao, Xiantong Zhen, Fuhao Zou, Fumin Shen, Kai Zheng",Neurocomputing,2016,Papers,Video summarization,,,https://www.sciencedirect.com/science/article/pii/S0925231216304805?casa_token=2uf2ekbvb7cAAAAA:YxtgDl8G6D-uunhYGOGv_aMgJeWefuO9klkQdMIh-jXz3V4JzEocy_Og3pPbaWMIlG2URM5t,,,,,
,,Gaze-Enabled Egocentric Video Summarization via Constrained Submodular Maximization," Jia Xu, Lopamudra Mukherjee, Yin Li, Jamieson Warner, James M. Rehg, Vikas Singh",CVPR,2015,Papers,Video summarization,,,https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Xu_Gaze-Enabled_Egocentric_Video_2015_CVPR_paper.html,,,,,
,,Predicting Important Objects for Egocentric Video Summarization, Yong Jae Lee & Kristen Grauman,IJCV,2015,Papers,Video summarization,,,https://link.springer.com/article/10.1007/s11263-014-0794-5?sa_campaign=email/event/articleAuthor/onlineFirst&error=cookies_not_supported&error=cookies_not_supported&code=9f9cd56d-eec9-49eb-bb9f-229724e371da&code=a2d596a3-5527-4ece-addc-1db7b036c200,,,,,
,,Video Summarization by Learning Submodular Mixtures of Objectives," Michael Gygli, Helmut Grabner, Luc Van Gool",CVPR,2015,Papers,Video summarization,,,https://openaccess.thecvf.com/content_cvpr_2015/papers/Gygli_Video_Summarization_by_2015_CVPR_paper.pdf,,,,,
,,Storyline Representation of Egocentric Videos with an Applications to Story-Based Search, Bo Xiong; Gunhee Kim; Leonid Sigal,ICCV,2015,Papers,Video summarization,,,https://ieeexplore.ieee.org/document/7410871,,,,,
,,Detecting Snap Points in Egocentric Video with a Web Photo Prior, Bo Xiong and Kristen Grauman,ECCV,2014,Papers,Video summarization,,,https://www.cs.utexas.edu/~grauman/papers/bo-eccv2014.pdf,,,,,
,,Creating Summaries from User Videos," Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool",ECCV,2014,Papers,Video summarization,,,https://gyglim.github.io/me/papers/GygliECCV14_vsum.pdf,,,,,
,,Quasi Real-Time Summarization for Consumer Videos," Bin Zhao,  Eric P. Xing",CVPR,2014,Papers,Video summarization,,,https://www.cs.cmu.edu/~epxing/papers/2014/Zhao_Xing_cvpr14a.pdf,,,,,
,,Story-Driven Summarization for Egocentric Video, Zheng Lu and Kristen Grauman,CVPR,2013,Papers,Video summarization,,,https://www.cs.utexas.edu/~grauman/papers/lu-grauman-cvpr2013.pdf,,,,,
,,Discovering Important People and Objects for Egocentric Video Summarization," Yong Jae Lee, Joydeep Ghosh, and Kristen Grauman",CVPR,2012,Papers,Video summarization,,,http://vision.cs.utexas.edu/projects/egocentric/egocentric_cvpr2012.pdf,,,,,
,,Wearable hand activity recognition for event summarization," Mayol, W. W., & Murray, D. W., IEEE International Symposium on Wearable Computers",IEEE International Symposium on Wearable Computers,2005,Papers,Video summarization,,,https://ieeexplore.ieee.org/document/1550796,,,,,
,,Wearable System for Personalized and Privacy-preserving Egocentric Visual Context Detection using On-device Deep Learning," Mina Khan, Glenn Fernandes, Akash Vaish, Mayank Manuja, Pattie Maes",UMAP,2021,Papers,Applications,,,https://dl.acm.org/doi/abs/10.1145/3450614.3461684,,,,,
,,Learning Robot Activities From First-Person Human Videos Using Convolutional Future Regression," Jangwon Lee, Michael S. Ryoo",CVPR,2017,Papers,Applications,,,https://openaccess.thecvf.com/content_cvpr_2017_workshops/w5/html/Lee_Learning_Robot_Activities_CVPR_2017_paper.html,,,,,
,,R3M: A Universal Visual Representation for Robot Manipulation,"Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn,  Abhinav Gupta ",,2022,Papers,Human to Robot,,,https://arxiv.org/abs/2203.12601,,,,,
,,Learning Robot Activities From First-Person Human Videos Using Convolutional Future Regression," Jangwon Lee, Michael S. Ryoo",CVPR,2017,Papers,Human to Robot,,,https://openaccess.thecvf.com/content_cvpr_2017_workshops/w5/html/Lee_Learning_Robot_Activities_CVPR_2017_paper.html,,,,,
,,One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning," Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang, Pieter Abbeel, Sergey Levine",RSS,2014,Papers,Human to Robot,,,http://www.roboticsproceedings.org/rss14/p02.pdf,,,,,
,,A Computational Model of Early Word Learning from the Infant's Point of View," Satoshi Tsutsui, Arjun Chandrasekaran, Md Alimoor Reza, David Crandall, Chen Yu",CogSci,2020,Papers,Asssitive Egocentric Vision,,,https://arxiv.org/abs/2006.02802,,,,,
,,Preserved action recognition in children with autism spectrum disorders: Evidence from an EEG and eye-tracking study," Mohammad Saber Sotoodeh, Hamidreza Taheri-Torbati, Nouchine Hadjikhani, Amandine Lassalle",Psychophysiology,2020,Papers,Asssitive Egocentric Vision,,,https://onlinelibrary.wiley.com/doi/10.1111/psyp.13740,,,,,
,,GSM," Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz",CVPR,2020,Papers,Popular Architectures,2D,,https://openaccess.thecvf.com/content_CVPR_2020/html/Sudhakaran_Gate-Shift_Networks_for_Video_Action_Recognition_CVPR_2020_paper.html,,https://github.com/swathikirans/GSM,,,
,,TSM," Ji Lin, Chuang Gan, Song Han",ICCV,2019,Papers,Popular Architectures,2D,,https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.html,,,,,
,,TBN," Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima",ICCV,2019,Papers,Popular Architectures,2D,,https://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf,,https://github.com/ekazakos/temporal-binding-network,,,
,,TRN," Bolei Zhou, Alex Andonian, Aude Oliva, Antonio Torralba",ECCV,2018,Papers,Popular Architectures,2D,,https://arxiv.org/pdf/1711.08496.pdf,,,,,
,,R(2+1)," Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri",CVPR,2018,Papers,Popular Architectures,2D,,https://openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html,,,,,
,,TSN," Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool",ECCV,2016,Papers,Popular Architectures,2D,,https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2,,,,,
,,SlowFast," Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He",ICCV,2019,Papers,Popular Architectures,3D,,https://openaccess.thecvf.com/content_ICCV_2019/html/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.html,,,,,
,,I3D," Joao Carreira, Andrew Zisserman",CVPR,2017,Papers,Popular Architectures,3D,,https://openaccess.thecvf.com/content_cvpr_2017/html/Carreira_Quo_Vadis_Action_CVPR_2017_paper.html,,,,,
,,LSTA," Sudhakaran, Swathikiran and Escalera, Sergio and Lanz, Oswald",CVPR,2019,Papers,Popular Architectures,RNN,,https://openaccess.thecvf.com/content_CVPR_2019/papers/Sudhakaran_LSTA_Long_Short-Term_Attention_for_Egocentric_Action_Recognition_CVPR_2019_paper.pdf,,https://github.com/swathikirans/LSTA,,,
,,RULSTM," Antonino Furnari, Giovanni Maria Farinella",ICCV,2019,Papers,Popular Architectures,RNN,,https://arxiv.org/pdf/1905.09035,,https://github.com/fpv-iplab/rulstm,,,
,,Ego-STAN," Jinman Park, Kimathi Kaai, Saad Hossain, Norikatsu Sumi, Sirisha Rambhatla, Paul Fieguth",WCVPR,2022,Papers,Popular Architectures,Transformer ,,https://arxiv.org/abs/2206.04785,,,,,
,,XViT," Adrian Bulat, Juan-Manuel Perez-Rua, Swathikiran Sudhakaran, Brais Martinez, Georgios Tzimiropoulos",NIPS,2021,Papers,Popular Architectures,Transformer ,,https://proceedings.neurips.cc/paper/2021/file/a34bacf839b923770b2c360eefa26748-Paper.pdf,,,,,
,,ViViT," Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid",ICCV,2021,Papers,Popular Architectures,Transformer ,,https://openaccess.thecvf.com/content/ICCV2021/html/Arnab_ViViT_A_Video_Vision_Transformer_ICCV_2021_paper.html,,,,,
,,TimeSformer," Gedas Bertasius, Heng Wang, Lorenzo Torresani",ICML,2021,Papers,Popular Architectures,Transformer ,,https://arxiv.org/abs/2102.05095,,,,,
,,Revisiting 3D Object Detection From an Egocentric Perspective," Boyang Deng, Charles R. Qi, Mahyar Najibi, Thomas Funkhouser, Yin Zhou, Dragomir Anguelov",NIPS,2021,Papers,Other EGO-Context,,,https://proceedings.neurips.cc/paper/2021/hash/db182d2552835bec774847e06406bfa2-Abstract.html,,,,,
,,Learning by Watching," Jimuyang Zhang, Eshed Ohn-Bar",CVPR,2021,Papers,Other EGO-Context,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_by_Watching_CVPR_2021_paper.pdf,,,,,
,,Assembly101,,CVPR ,2022,Papers,Datasets,,Procedural activity dataset featuring 4321 videos of people assembling and disassembling 101 “take-apart” toy vehicles.,https://arxiv.org/pdf/2203.14712.pdf,https://assembly101.github.io/,,,,
,,EgoPAT3D,,CVPR,2022,Papers,Datasets,,"A large multimodality dataset of more than 1 million frames of RGB-D and IMU streams, with evaluation metrics based on high-quality 2D and 3D labels from semi-automatic annotation.",https://arxiv.org/abs/2203.13116,https://ai4ce.github.io/EgoPAT3D/,,,,
,,EasyCom-Clustering,,,2022,Papers,Datasets,, The first large-scale egocentric video face clustering dataset.,https://arxiv.org/abs/2203.13166,https://arxiv.org/abs/2203.13166,,,,
,,AGD20K,,CVPR,2022,Papers,Datasets,, Affordance dataset constructed by collecting and labeling over 20K images from 36 affordance categories.,https://arxiv.org/abs/2203.09905,https://github.com/lhc1224/Cross-View-AG,,,,
,,AssistQ,,ECCV,2022,Papers,Datasets,,"A new dataset comprising 529 question-answer samples derived from 100 newly filmed first-person videos. Each question should be completed with multi-step guidances by inferring from visual details (e.g., buttons' position) and textural details (e.g., actions like press/turn).",https://arxiv.org/abs/2203.04203,https://showlab.github.io/assistq/,,,,
,,HOI4D,,CVPR,2022,Papers,Datasets,,"A large-scale 4D egocentric dataset with rich annotations, to catalyze the research of category-level human-object interaction. HOI4D consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by 4 participants interacting with 800 different object instances from 16 categories over 610 different indoor rooms. Frame-wise annotations for panoptic segmentation, motion segmentation, 3D hand pose, category-level object pose and hand action have also been provided, together with reconstructed object meshes and scene point clouds.",https://arxiv.org/abs/2203.01577,https://hoi4d.github.io/,,,,
,,EgoPW,,CVPR,2022,Papers,Datasets,,"A dataset captured by a head-mounted fisheye camera and an auxiliary external camera, which provides an additional observation of the human body from a third-person perspective. ",https://arxiv.org/abs/2201.07929,https://arxiv.org/abs/2201.07929,,,,
,,First2Third-Pose,,,2022,Papers,Datasets,,"A new paired synchronized dataset of nearly 2,000 videos depicting human activities captured from both first- and third-view perspectives.",https://arxiv.org/abs/2201.02017,https://github.com/nudlesoup/First2Third-Pose,,,,
,,Ego4D,,CVPR,2022,Papers,Datasets,,"3,025 hours of daily-life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 855 unique camera wearers from 74 worldwide locations and 9 different countries.",https://www.cs.utexas.edu/~grauman/papers/ego4d-cvpr2022.pdf,https://ego4d-data.org,,,,
,,EgoCom,,TPAMI,2020,Papers,Datasets,,A natural conversations dataset containing multi-modal human communication data captured simultaneously from the participants' egocentric perspectives.,https://ieeexplore.ieee.org/document/9200754,https://github.com/facebookresearch/EgoCom-Dataset,,,,
,,TREK-100,,WICCV,2021,Papers,Datasets,,Object tracking in first person vision.,https://openaccess.thecvf.com/content/ICCV2021W/VOT/html/Dunnhofer_Is_First_Person_Vision_Challenging_for_Object_Tracking_ICCVW_2021_paper.html,https://machinelearning.uniud.it/datasets/trek100/,,,,
,,MECCANO,,WACV,2021,Papers,Datasets,,20 subject assembling a toy motorbike.,https://openaccess.thecvf.com/content/WACV2021/html/Ragusa_The_MECCANO_Dataset_Understanding_Human-Object_Interactions_From_Egocentric_Videos_in_WACV_2021_paper.html,https://iplab.dmi.unict.it/MECCANO/,,,,
,,EPIC-Kitchens 2020,,IJCV,2021,Papers,Datasets,,Subjects performing unscripted actions in their native environments.,https://link.springer.com/article/10.1007/s11263-021-01531-2,https://epic-kitchens.github.io/2020-100,,,,
,,EPIC-Tent,,ICCV,2019,Papers,Datasets,,29 participants assembling a tent while wearing two head-mounted cameras.,https://openaccess.thecvf.com/content_ICCVW_2019/html/EPIC/Jang_EPIC-Tent_An_Egocentric_Video_Dataset_for_Camping_Tent_Assembly_ICCVW_2019_paper.html,https://data.bristol.ac.uk/data/dataset/2ite3tu1u53n42hjfh3886sa86,,,,
,,EGO-CH,,Pattern Recognition Letters,2020,Papers,Datasets,,"70 subjects visiting two cultural sites in Sicily, Italy.",https://www.sciencedirect.com/science/article/pii/S0167865519303794,https://iplab.dmi.unict.it/EGO-CH/,,,,
,,EPIC-Kitchens 2018,,ECCV,2018,Papers,Datasets,,32 subjects performing unscripted actions in their native environments.,https://openaccess.thecvf.com/content_ECCV_2018/html/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.html,https://epic-kitchens.github.io/2018,,,,
,,Charade-Ego,,,,Papers,Datasets,, Paired first-third person videos.,,https://allenai.org/plato/charades/,,,,
,,EGTEA Gaze+,,,,Papers,Datasets,," 32 subjects, 86 cooking sessions, 28 hours.",,http://cbs.ic.gatech.edu/fpv/,,,,
,,ADL,,,,Papers,Datasets,, 20 subjects performing daily activities in their native environments.,,https://www.csee.umbc.edu/~hpirsiav/papers/ADLdataset/,,,,
,,CMU kitchen,,,,Papers,Datasets,," Multimodal, 18 subjects cooking 5 different recipes: brownies, eggs, pizza, salad, sandwich.",,http://kitchen.cs.cmu.edu/index.php,,,,
,,EgoSeg,,,,Papers,Datasets,," Long term actions (walking, running, driving, etc.)",,http://www.vision.huji.ac.il/egoseg/,,,,
,,First-Person Social Interactions,,,,Papers,Datasets,, 8 subjects at disneyworld.,,http://ai.stanford.edu/~alireza/Disney/,,,,
,,UEC Dataset,,,,Papers,Datasets,," Two choreographed datasets with different egoactions (walk, jump, climb, etc.) + 6 YouTube sports videos.",,http://www.cs.cmu.edu/~kkitani/datasets/,,,,
,,JPL,,,,Papers,Datasets,, Interaction with a robot.,,http://michaelryoo.com/jpl-interaction.html,,,,
,,FPPA,,,,Papers,Datasets,, Five subjects performing 5 daily actions.,,http://tamaraberg.com/prediction/Prediction.html,,,,
,,UT Egocentric,,,,Papers,Datasets,, 3-5 hours long videos capturing a person's day.,,http://vision.cs.utexas.edu/projects/egocentric/index.html,,,,
,,VINST/ Visual Diaries,,,,Papers,Datasets,, 31 videos capturing the visual experience of a subject walking from metro station to work.,,http://www.csc.kth.se/cvap/vinst/NovEgoMotion.html,,,,
,,Bristol Egocentric Object Interaction (BEOID),,,,Papers,Datasets,," 8 subjects, six locations. Interaction with objects and environment.",,https://www.cs.bris.ac.uk/~damen/BEOID/,,,,
,,Object Search Dataset,,,,Papers,Datasets,, 57 sequences of 55 subjects on search and retrieval tasks.,,https://github.com/Mengmi/deepfuturegaze_gan,,,,
,,UNICT-VEDI,,,,Papers,Datasets,, Different subjects visiting a museum.,,http://iplab.dmi.unict.it/VEDI/,,,,
,,UNICT-VEDI-POI,,,,Papers,Datasets,, Different subjects visiting a museum.,,http://iplab.dmi.unict.it/VEDI_POIs/,,,,
,,Simulated Egocentric Navigations,,,,Papers,Datasets,, Simulated navigations of a virtual agent within a large building.,,http://iplab.dmi.unict.it/SimulatedEgocentricNavigations/,,,,
,,EgoCart,,,,Papers,Datasets,, Egocentric images collected by a shopping cart in a retail store.,,http://iplab.dmi.unict.it/EgocentricShoppingCartLocalization/,,,,
,,Unsupervised Segmentation of Daily Living Activities,,,,Papers,Datasets,, Egocentric videos of daily activities.,,http://iplab.dmi.unict.it/dailylivingactivities,,,,
,,Visual Market Basket Analysis,,,,Papers,Datasets,, Egocentric images collected by a shopping cart in a retail store.,,http://iplab.dmi.unict.it/vmba/,,,,
,,Location Based Segmentation of Egocentric Videos,,,,Papers,Datasets,, Egocentric videos of daily activities.,,http://iplab.dmi.unict.it/PersonalLocationSegmentation/,,,,
,,Recognition of Personal Locations from Egocentric Videos,,,,Papers,Datasets,, Egocentric videos clips of daily.,,http://iplab.dmi.unict.it/PersonalLocations/,,,,
,,EgoGesture,,,,Papers,Datasets,, 2k videos from 50 subjects performing 83 gestures.,,http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html,,,,
,,EgoHands,,,,Papers,Datasets,, 48 videos of interactions between two people.,,http://vision.soic.indiana.edu/projects/egohands/,,,,
,,DoMSEV,,,,Papers,Datasets,, 80 hours/different activities.,,http://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2018-dataset/,,,,
,,DR(eye)VE,,,,Papers,Datasets,, 74 videos of people driving.,,http://aimagelab.ing.unimore.it/dreyeve,,,,
,,THU-READ,,,,Papers,Datasets,, 8 subjects performing 40 actions with a head-mounted RGBD camera.,,http://ivg.au.tsinghua.edu.cn/dataset/THU_READ.php,,,,
,,EgoDexter,,,,Papers,Datasets,,"4 sequences with 4 actors (2 female), and varying interactions with various objects and and cluttered background.",https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/index.htm,https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/EgoDexter.htm,,,,
,,First-Person Hand Action (FPHA),,,,Papers,Datasets,,3D hand-object interaction. Includes 1175 videos belonging to 45 different activity categories performed by 6 actors.,https://arxiv.org/pdf/1704.02463.pdf,https://guiggh.github.io/publications/first-person-hands/,,,,
,,UTokyo Paired Ego-Video (PEV),,,,Papers,Datasets,," 1,226 pairs of first-person clips extracted from the ones recorded synchronously during dyadic conversations.",,https://yonetaniryo.github.io/fpv_data.html,,,,
,,UTokyo Ego-Surf,,,,Papers,Datasets,, Contains 8 diverse groups of first-person videos recorded synchronously during face-to-face conversations.,,https://yonetaniryo.github.io/fpv_data.html,,,,
,,TEgO: Teachable Egocentric Objects Dataset,,,,Papers,Datasets,,  Contains egocentric images of 19 distinct objects taken by two people for training a teachable object recognizer.,,https://iamlabumd.github.io/tego/,,,,
,,Multimodal Focused Interaction Dataset,,,,Papers,Datasets,," Contains 377 minutes of continuous multimodal recording captured during 19 sessions, with 17 conversational partners in 18 different indoor/outdoor locations.",,https://cvip.computing.dundee.ac.uk/datasets/focusedinteraction/,,,,
,,GoPro,,,,Devices,,,,,https://gopro.com/it/it/,,,,
,,Narrative clip,,,,Devices,,,,,http://getnarrative.com/,,,,
,,Autographer5,,,,Devices,,,,,https://en.wikipedia.org/wiki/Autographer,,,,
,,Microsoft SenseCam,,,,Devices,,,,,https://www.microsoft.com/en-us/research/project/sensecam/,,,,
,,SMI eye-tracker,,,,Devices,,,,,https://imotions.com/hardware/smi-eye-tracking-glasses/,,,,
,,ASL Mobile eye,,,,Devices,,,,,https://est-kl.com/manufacturer/asl/mobile-eye-5.html,,,,
,,Tobii eye-tracker,,,,Devices,,,,,https://www.tobii.com/,,,,
,,Pupil Invisible,,,,Devices,,,,,https://pupil-labs.com/,,,,
,,Microsoft Hololens 2,,,,Devices,,,,,https://www.microsoft.com/it-it/hololens/,,,,
,,Google Glass,,,,Devices,,,,,https://www.google.com/glass/start/,,,,
,,Vusix Blade,,,,Devices,,,,,https://www.vuzix.com/products/vuzix-blade-smart-glasses-upgraded,,,,
,,Magic Leap,,,,Devices,,,,,https://www.magicleap.com/,,,,
,,Nreal Light,,,,Devices,,,,,https://www.nreal.ai/,,,,
,,Epson Moverio,,,,Devices,,,,,https://www.epson.it/products/see-through-mobile-viewer,,,,
,,Realwear,,,,Devices,,,,,https://www.realwear.com/,,,,
,,TCL Smart Glasses Thunderbird,,,,Devices,,,,,https://www.tcl.com/eu/en/glasses/tcl-nxtwear-g,,,,
,,OrCam,,,,Devices,,,,,https://www.orcam.com/it/,,,,
,,Xiaomi Smart Glasses,,,,Devices,,,,,,,,,
,,Ray-Ban Stories,,,,Devices,,,,,https://www.ray-ban.com/italy/ray-ban-stories,,,,
,,dynaEdge,,,,Devices,,,,,https://it.dynabook.com/generic/dynaedge/,,,,
,,Apple Vision Pro,,,,Devices,,,,,https://www.apple.com/apple-vision-pro/,,,,
,,Alpha Glass,,,,Devices,,,,,,,,,
,,GWD HiiDii,,,,Devices,,,,,https://www.gwdbi.com/,,,,
,,Spectacles,,,,Devices,,,,,https://www.spectacles.com/it/shop/spectacles-3/,,,,
,,Ego4D,,,,Challenges,,,"Episodic Memory, Hand-Object Interactions, AV Diarization, Social, Forecasting.",,https://ego4d-data.org,,,,
,,Epic Kitchen Challenge,,,,Challenges,,,"Action Recognition, Action Detection, Action Anticipation, Unsupervised Domain Adaptation for Action Recognition, Multi-Instance Retrieval",,https://epic-kitchens.github.io/,,,,
24/11/2022 11.11.23,mirco.pl.93@gmail.com,Touch and Go," Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu, Wenzhen Yuan, Andrew Owens",NeurIPS,2022,Papers,Datasets,,"we present a dataset, called Touch and Go, in which human data collectors walk through a variety of environments, probing objects with tactile sensors and simultaneously recording their actions on video ",https://arxiv.org/pdf/2211.12498.pdf,https://touch-and-go.github.io/,https://github.com/fredfyyang/Tactile-Driven-Image-Stylization/,,,
"17/07/2023, 12:48:31",mirco.pl.93@gmail.com,ManipulaTHOR: A Framework for Visual Object Manipulation,"Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi",CVPR,2021,Papers,Human to Robot,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Ehsani_ManipulaTHOR_A_Framework_for_Visual_Object_Manipulation_CVPR_2021_paper.pdf,https://ai2thor.allenai.org/manipulathor/,,,,
"17/07/2023, 12:59:55",mirco.pl.93@gmail.com,HOMAGE,"Nishant Rai, Haofeng Chen, Jingwei Ji, Rishi Desai, Kazuki Kozuka, Shun Ishizaka, Ehsan Adeli, Juan Carlos Niebles",CVPR,2021,Papers,Datasets,,Home Action Genome (HOMAGE): a multi-view action dataset with multiple modalities and view-points supplemented with hierarchical activity and atomic action labels together with dense scene composition labels,https://openaccess.thecvf.com/content/CVPR2021/papers/Rai_Home_Action_Genome_Cooperative_Compositional_Action_Understanding_CVPR_2021_paper.pdf,https://homeactiongenome.org/,,,,
"17/07/2023, 13:09:36",mirco.pl.93@gmail.com,ACTION-Net: Multipath Excitation for Action Recognition,"Zhengwei Wang, Qi She, Aljosa Smolic",CVPR,2021,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_ACTION-Net_Multipath_Excitation_for_Action_Recognition_CVPR_2021_paper.pdf,,https://github.com/V-Sense/ACTION-Net,,,
"17/07/2023, 13:30:37",mirco.pl.93@gmail.com,Knowledge Distillation for Human Action Anticipation,"Vinh Tran Stony Brook University, Stony Brook NY, Yang Wang,  Zekun Zhang, Minh Hoai",ICIP,2021,Papers,Action Anticipation,Short-Term Action Anticipation,,https://ieeexplore.ieee.org/abstract/document/9506693?casa_token=BecLRxlT_acAAAAA:AT3srfvaOxi3pcMZ5jNhPziC75sADNRNhHLRXYAUWAe9yo5pUNlKIJvDw1a4Geg087evZvEkEA,,,,,
"17/07/2023, 13:36:14",mirco.pl.93@gmail.com,How You Move Your Head Tells What You Do: Self-supervised Video Representation Learning with Egocentric Cameras and IMU Sensors,"Satoshi Tsutsui, Ruta Desai, Karl Ridgeway",WICCV,2021,Papers,Multi-Modalities,IMU,,https://arxiv.org/abs/2110.01680,,,,,
"17/07/2023, 13:48:09",mirco.pl.93@gmail.com,NeuralDiff: Segmenting 3D objects that move in egocentric videos,"Vadim Tschernezki, Diane Larlus, Andrea Vedaldi",3DV,2021,Papers,Applications,,,https://ieeexplore.ieee.org/abstract/document/9665956?casa_token=MCT217Rr7zAAAAAA:i-kwUh6jAScWI81EYHQU2vcze3AF5OPWNoy5EdU3t2UP2JQ2s55gDT6QYDbVapahhQdHKpTU7A,,,,,
"17/07/2023, 13:52:30",mirco.pl.93@gmail.com,Moving SLAM: Fully Unsupervised Deep Learning in Non-Rigid Scenes,"Dan Xu, Andrea Vedaldi, João F. Henriques",IROS,2021,Papers,Multi-Modalities,Depth,,https://ieeexplore.ieee.org/abstract/document/9636075?casa_token=UYaHkHj4TJcAAAAA:NketkffyE1V5tUn7UeG_ko9rQGzgiwgSPrliBqW2uhuihLNYtSJa2vU-ufaRzWxRvWZzCMLqGw,,,,,
"17/07/2023, 13:55:05",mirco.pl.93@gmail.com,What If We Could Not See? Counterfactual Analysis for Egocentric Action Anticipation,"Tianyu Zhang, Weiqing Min, Jiahao Yang, Tao Liu, Shuqiang Jiang, Yong Rui",IJCAI,2021,Papers,Action Anticipation,Short-Term Action Anticipation,,https://www.ijcai.org/proceedings/2021/0182.pdf,,,,,
"17/07/2023, 16:21:54",mirco.pl.93@gmail.com,H2O,"Taein Kwon, Bugra Tekin, Jan Stühmer, Federica Bogo, Marc Pollefeys",ICCV,2021,Papers,Datasets,,"H2O (2 Hands and Objects), provides synchronized multi-view RGB-D images, interaction labels, object classes, ground-truth 3D poses for left & right hands, 6D object poses, ground-truth camera poses, object meshes and scene point clouds.",https://openaccess.thecvf.com/content/ICCV2021/papers/Kwon_H2O_Two_Hands_Manipulating_Objects_for_First_Person_Interaction_Recognition_ICCV_2021_paper.pdf,https://taeinkwon.com/projects/h2o/,,,,
"17/07/2023, 16:27:06",mirco.pl.93@gmail.com,EPIC-Visor,"Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar, Richard Higgins, Sanja Fidler, David Fouhey, Dima Damen",NeurIPS,2022,Papers,Datasets,,"VISOR, a new dataset of pixel annotations and a benchmark suite for segmenting hands and active objects in egocentric video",https://proceedings.neurips.cc/paper_files/paper/2022/hash/590a7ebe0da1f262c80d0188f5c4c222-Abstract-Datasets_and_Benchmarks.html,https://epic-kitchens.github.io/VISOR/,,,,
"17/07/2023, 16:39:55",mirco.pl.93@gmail.com,Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos,"Muheng Li, Lei Chen, Yueqi Duan, Zhilan Hu, Jianjiang Feng, Jie Zhou, Jiwen Lu",CVPR,2022,Papers,Temporal Segmentation (Action Detection),,,https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Bridge-Prompt_Towards_Ordinal_Action_Understanding_in_Instructional_Videos_CVPR_2022_paper.pdf,,https://github.com/ttlmh/Bridge-Prompt,,,
"17/07/2023, 16:50:52",mirco.pl.93@gmail.com,Interact before Align: Leveraging Cross-Modal Knowledge for Domain Adaptive Action Recognition,"Lijin Yang, Yifei Huang, Yusuke Sugano, Yoichi Sato",CVPR,2022,Papers,Action/Activity Recognition,Usupervised Domain Adaptation,,https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Interact_Before_Align_Leveraging_Cross-Modal_Knowledge_for_Domain_Adaptive_Action_CVPR_2022_paper.pdf,,,,,
"17/07/2023, 17:03:29",mirco.pl.93@gmail.com,Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual Reality,"Amin Jourabloo, Fernando De la Torre, Jason Saragih, Shih-En Wei, Stephen Lombardi, Te-Li Wang, Danielle Belko, Autumn Trimble, Hernan Badino",CVPR,2022,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Jourabloo_Robust_Egocentric_Photo-Realistic_Facial_Expression_Transfer_for_Virtual_Reality_CVPR_2022_paper.pdf,,,,,
"17/07/2023, 17:11:40",mirco.pl.93@gmail.com,Joint Hand Motion and Interaction Hotspots Prediction From Egocentric Videos,"Shaowei Liu, Subarna Tripathi, Somdeb Majumdar, Xiaolong Wang",CVPR,2022,Papers,Action Anticipation,Region prediction,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Joint_Hand_Motion_and_Interaction_Hotspots_Prediction_From_Egocentric_Videos_CVPR_2022_paper.pdf,https://stevenlsw.github.io/hoi-forecast/,https://github.com/stevenlsw/hoi-forecast,,,
"17/07/2023, 19:07:44",mirco.pl.93@gmail.com,A Hybrid Egocentric Activity Anticipation Framework via Memory-Augmented Recurrent and One-Shot Representation Forecasting,"Tianshan Liu, Kin-Man Lam",CVPR,2022,Papers,Action Anticipation,Short-Term Action Anticipation,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_Hybrid_Egocentric_Activity_Anticipation_Framework_via_Memory-Augmented_Recurrent_and_CVPR_2022_paper.pdf,,,,,
"18/07/2023, 13:20:58",mirco.pl.93@gmail.com,Egocentric Deep Multi-Channel Audio-Visual Active Speaker Localization,"Hao Jiang, Calvin Murdock, Vamsi Krishna Ithapu",CVPR,2022,Papers,Social Interactions,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Egocentric_Deep_Multi-Channel_Audio-Visual_Active_Speaker_Localization_CVPR_2022_paper.pdf,,,,,
"18/07/2023, 13:34:00",mirco.pl.93@gmail.com,Egocentric Scene Understanding via Multimodal Spatial Rectifier,"Tien Do, Khiem Vuong, Hyun Soo Park",CVPR,2022,Papers,Multi-Modalities,Depth,,https://openaccess.thecvf.com/content/CVPR2022/papers/Do_Egocentric_Scene_Understanding_via_Multimodal_Spatial_Rectifier_CVPR_2022_paper.pdf,,https://github.com/tien-d/EgoDepthNormal,,,
"18/07/2023, 14:09:47",mirco.pl.93@gmail.com,SOS! Self-supervised Learning Over Sets Of Handled Objects In Egocentric Action Recognition,"Victor Escorcia, Ricardo Guerrero, Xiatian Zhu, Brais Martinez",ECCV,2022,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2204.04796,,,,,
"18/07/2023, 14:18:33",mirco.pl.93@gmail.com,Generative Adversarial Network for Future Hand Segmentation from Egocentric Video,"Wenqi Jia, Miao Liu, James M. Rehg",ECCV,2022,Papers,Segmentation,,,https://arxiv.org/abs/2203.11305,https://vjwq.github.io/EgoGAN/,,,,
"18/07/2023, 16:48:02",mirco.pl.93@gmail.com,EgoProceL ,"Siddhant Bansal, Chetan Arora, C.V. Jawahar",ECCV,2022,Papers,Datasets,,EgoProceL dataset focuses on the key-steps required to perform a task instead of every action in the video. EgoProceL consistis of 62 hours of videos captured by 130 subjects performing 16 tasks.,https://arxiv.org/pdf/2207.10883.pdf,https://sid2697.github.io/egoprocel/,,,,
"18/07/2023, 16:54:08",mirco.pl.93@gmail.com,EgoTaskQA: Understanding Human Tasks in Egocentric Videos,"Baoxiong Jia, Ting Lei, Song-Chun Zhu, Siyuan Huang",NeurIPS,2022,Papers,Video-Language,,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/161c94a58ca25bafcaf47893e8233deb-Abstract-Datasets_and_Benchmarks.html,https://sites.google.com/view/egotaskqa,,,,
"18/07/2023, 16:56:37",mirco.pl.93@gmail.com,My View is the Best View: Procedure Learning from Egocentric Videos,"Siddhant Bansal, Chetan Arora, C.V. Jawahar",ECCV,2022,Papers,Temporal Segmentation (Action Detection),,,https://arxiv.org/abs/2207.10883,,,,,
"18/07/2023, 17:03:01",mirco.pl.93@gmail.com,EgoHOS,"Lingzhi Zhang, Shenghao Zhou, Simon Stent, Jianbo Shi",ECCV,2022,Papers,Datasets,,"EgoHOS, a labeled dataset consisting of 11,243 egocentric images with per-pixel segmentation labels of hands and objects being interacted with during a diverse array of daily activities. Our dataset is the first to label detailed hand-object contact boundaries",https://arxiv.org/abs/2208.03826,https://github.com/owenzlz/EgoHOS,https://github.com/owenzlz/EgoHOS,,,
"18/07/2023, 17:07:41",mirco.pl.93@gmail.com,UnrealEgo,"Hiroyasu Akada, Jian Wang, Soshi Shimada, Masaki Takahashi, Christian Theobalt, Vladislav Golyanik",ECCV,2022,Papers,Datasets,,"UnrealEgo, i.e., a new large-scale naturalistic dataset for egocentric 3D human pose estimation. It is the first dataset to provide in-the-wild stereo images with the largest variety of motions among existing egocentric datasets.",https://arxiv.org/abs/2208.01633,https://4dqv.mpi-inf.mpg.de/UnrealEgo/,,,,
"18/07/2023, 17:10:06",mirco.pl.93@gmail.com,Rethinking Learning Approaches for Long-Term Action Anticipation,"Megha Nawhal, Akash Abdu Jyothi, Greg Mori",ECCV,2022,Papers,Action Anticipation,Long-Term Action Anticipation,,https://arxiv.org/abs/2210.11566,https://meghanawhal.github.io/projects/anticipatr.html,,,,
"18/07/2023, 17:28:53",mirco.pl.93@gmail.com,Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,"Zecheng Yu, Yifei Huang, Ryosuke Furuta, Takuma Yagi, Yusuke Goutsu, Yoichi Sato",WACV,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content/WACV2023/papers/Yu_Fine-Grained_Affordance_Annotation_for_Egocentric_Hand-Object_Interaction_Videos_WACV_2023_paper.pdf,https://github.com/zch-yu/epic-affordance-annotation,,,,
"18/07/2023, 17:30:47",mirco.pl.93@gmail.com,Intention-Conditioned Long-Term Human Egocentric Action Anticipation,"Esteve Valls Mascaro´, Hyemin Ahn, Dongheui Lee",WACV,2023,Papers,Action Anticipation,Long-Term Action Anticipation,,https://openaccess.thecvf.com/content/WACV2023/papers/Mascaro_Intention-Conditioned_Long-Term_Human_Egocentric_Action_Anticipation_WACV_2023_paper.pdf,,,,,
"18/07/2023, 18:20:11",mirco.pl.93@gmail.com,Egocentric Activity Recognition and Localization on a 3D Map,"Chang Chen, Jiaming Zhang, Kailun Yang, Kunyu Peng, Rainer Stiefelhagen",WACV,2023,Papers,Localization,,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Trans4Map_Revisiting_Holistic_Birds-Eye-View_Mapping_From_Egocentric_Images_to_Allocentric_WACV_2023_paper.pdf,,https://github.com/jamycheung/Trans4Map,,,
"18/07/2023, 18:23:47",mirco.pl.93@gmail.com,Balanced Spherical Grid for Egocentric View Synthesis,"Changwoon Choi, Sang Min Kim, Young Min Kim",CVPR,2023,Papers,NeRF,,,https://openaccess.thecvf.com/content/CVPR2023/html/Choi_Balanced_Spherical_Grid_for_Egocentric_View_Synthesis_CVPR_2023_paper.html,https://changwoon.info/publications/EgoNeRF,,,,
"18/07/2023, 18:27:44",mirco.pl.93@gmail.com,AssemblyHands,"Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin",CVPR,2023,Papers,Datasets,,"AssemblyHands, a large-scale benchmark dataset with accurate 3D hand pose annotations, to facilitate the study of egocentric activities with challenging handobject interactions. ",https://openaccess.thecvf.com/content/CVPR2023/papers/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.pdf,https://assemblyhands.github.io/,,,,
"18/07/2023, 18:31:51",mirco.pl.93@gmail.com,Scene-aware Egocentric 3D Human Pose Estimation," Jian Wang, Diogo Luvizon, Weipeng Xu, Lingjie Liu, Kripasindhu Sarkar, Christian Theobalt",CVPR,2023,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Scene-Aware_Egocentric_3D_Human_Pose_Estimation_CVPR_2023_paper.pdf,https://github.com/yt4766269/SceneEgo,,,,
"18/07/2023, 18:35:40",mirco.pl.93@gmail.com,MMG-Ego4D: Multimodal Generalization in Egocentric Action Recognition,"Xinyu Gong, Sreyas Mohan, Naina Dhingra, Jean-Charles Bazin, Yilei Li, Zhangyang Wang, Rakesh Ranjan",CVPR,2023,Papers,Action/Activity Recognition,Domain Generalization,,https://openaccess.thecvf.com/content/CVPR2023/papers/Gong_MMG-Ego4D_Multimodal_Generalization_in_Egocentric_Action_Recognition_CVPR_2023_paper.pdf,,,,,
"18/07/2023, 18:38:18",mirco.pl.93@gmail.com,Egocentric Video Task Translation,"Zihui Xue, Yale Song, Kristen Grauman, Lorenzo Torresani",CVPR,2023,Papers,Multiple Egocentric Tasks,,,https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_Egocentric_Video_Task_Translation_CVPR_2023_paper.pdf,https://vision.cs.utexas.edu/projects/egot2/,,,,
"18/07/2023, 19:50:42",mirco.pl.93@gmail.com,Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition From Egocentric RGB Videos,"Yilin Wen, Hao Pan, Lei Yang, Jia Pan, Taku Komura, Wenping Wang",CVPR,2023,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_Hierarchical_Temporal_Transformer_for_3D_Hand_Pose_Estimation_and_Action_CVPR_2023_paper.pdf,,https://github.com/fylwen/HTT,,,
"18/07/2023, 19:55:25",mirco.pl.93@gmail.com,Egocentric Audio-Visual Object Localization,"Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu",CVPR,2023,Papers,Multi-Modalities,Audio-Visual,,https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Egocentric_Audio-Visual_Object_Localization_CVPR_2023_paper.pdf,,https://github.com/WikiChao/Ego-AV-Loc,,,
"18/07/2023, 19:57:29",mirco.pl.93@gmail.com,EpicSoundingObject,"Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu",CVPR,2023,Papers,Datasets,,Epic Sounding Object dataset with sounding object annotations to benchmark the localization performance in egocentric videos,https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Egocentric_Audio-Visual_Object_Localization_CVPR_2023_paper.pdf,https://github.com/WikiChao/Ego-AV-Loc,https://github.com/WikiChao/Ego-AV-Loc,,,
"18/07/2023, 19:59:37",mirco.pl.93@gmail.com,N-EPIC-Kitchens,"Chiara Plizzari, Mirco Planamente, Gabriele Goletto, Marco Cannici, Emanuele Gusso, Matteo Matteucci, Barbara Caputo",CVPR,2022,Papers,Datasets,,"N-EPIC-Kitchens, the first event-based camera extension of the large-scale EPIC-Kitchens dataset",https://openaccess.thecvf.com/content/CVPR2022/html/Plizzari_E2GOMOTION_Motion_Augmented_Event_Stream_for_Egocentric_Action_Recognition_CVPR_2022_paper.html,https://github.com/EgocentricVision/N-EPIC-Kitchens,,,,
"18/07/2023, 20:02:01",mirco.pl.93@gmail.com,Egocentric Auditory Attention Localization in Conversations,"Fiona Ryan, Hao Jiang, Abhinav Shukla, James M. Rehg, Vamsi Krishna Ithapu",CVPR,2022,Papers,Social Interactions,,,https://openaccess.thecvf.com/content/CVPR2023/papers/Ryan_Egocentric_Auditory_Attention_Localization_in_Conversations_CVPR_2023_paper.pdf,https://fkryan.github.io/saal,,,,
"18/07/2023, 20:22:03",mirco.pl.93@gmail.com,Where is my Wallet? Modeling Object Proposal Sets for Egocentric Visual Query Localization,"Mengmeng Xu, Yanghao Li, Cheng-Yang Fu, Bernard Ghanem, Tao Xiang, Juan-Manuel Pérez-Rúa",CVPR,2023,Papers,Retrieval,,,https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Where_Is_My_Wallet_Modeling_Object_Proposal_Sets_for_Egocentric_CVPR_2023_paper.pdf,,https: //github.com/facebookresearch/vq2d_cvpr,,,
"18/07/2023, 20:25:07",mirco.pl.93@gmail.com,Tracking Multiple Deformable Objects in Egocentric Videos,"Mingzhen Huang, Xiaoxing Li, Jun Hu, Honghong Peng, Siwei Lyu",CVPR,2023,Papers,Tracking,,,https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Tracking_Multiple_Deformable_Objects_in_Egocentric_Videos_CVPR_2023_paper.pdf,https://mingzhenhuang.com/projects/detracker.html,,,,
"19/07/2023, 02:30:15",mirco.pl.93@gmail.com,Bringing Online Egocentric Action Recognition into the wild,"Gabriele Goletto, Mirco Planamente, Barbara Caputo, Giuseppe Averta",RA-L,2023,Papers,Applications,,,https://arxiv.org/abs/2211.03004,,https://github.com/EgocentricVision/EgoWild,,,
"19/07/2023, 02:37:12",mirco.pl.93@gmail.com,Test-time adaptation for egocentric action recognition,"Mirco Plananamente, Chiara Plizzari, Barbara Caputo",ICIAP,2022,Papers,Action/Activity Recognition,Test Time Training (Adaptation),,https://link.springer.com/chapter/10.1007/978-3-031-06433-3_18,,https://github.com/EgocentricVision/RNA-TTA,,,
"19/07/2023, 02:59:43",mirco.pl.93@gmail.com,Use Your Head: Improving Long-Tail Video Recognition,"Toby Perrett, Saptarshi Sinha, Tilo Burghardt, Majid Mirmehdi, Dima Damen",CVPR,2023,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/abs/2304.01143,https://tobyperrett.github.io/lmr/,,,,
"19/07/2023, 03:02:53",mirco.pl.93@gmail.com,How Can Objects Help Action Recognition?,"Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid",CVPR,2023,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.html,,https://github.com/google-research/scenic,,,
"19/07/2023, 03:07:01",mirco.pl.93@gmail.com,The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction,"Alexandros Stergiou, Dima Damen",CVPR,2023,Papers,Action Anticipation,Short-Term Action Anticipation,,https://openaccess.thecvf.com/content/CVPR2023/papers/Stergiou_The_Wisdom_of_Crowds_Temporal_Progressive_Attention_for_Early_Action_CVPR_2023_paper.pdf,https://alexandrosstergiou.github.io/project_pages/TemPr/index.html,,,,
"19/07/2023, 03:10:24",mirco.pl.93@gmail.com,Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations,"Sagnik Majumder, Hao Jiang, Pierre Moulon, Ethan Henderson, Paul Calamia, Kristen Grauman, Vamsi Krishna Ithapu",CVPR,2023,Papers,Localization,,,https://openaccess.thecvf.com/content/CVPR2023/papers/Majumder_Chat2Map_Efficient_Scene_Mapping_From_Multi-Ego_Conversations_CVPR_2023_paper.pdf,https://vision.cs.utexas.edu/projects/chat2map/,,,,
"19/07/2023, 03:12:43",mirco.pl.93@gmail.com,NaQ: Leveraging Narrations As Queries To Supervise Episodic Memory,"Santhosh Kumar Ramakrishnan, Ziad Al-Halah, Kristen Grauman",CVPR,2023,Papers,Video-Language,,,https://openaccess.thecvf.com/content/CVPR2023/papers/Ramakrishnan_NaQ_Leveraging_Narrations_As_Queries_To_Supervise_Episodic_Memory_CVPR_2023_paper.pdf,https://vision.cs.utexas.edu/projects/naq/,,,,
"19/07/2023, 03:14:23",mirco.pl.93@gmail.com,HierVL: Learning Hierarchical Video-Language Embeddings,"Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, Kristen Grauman",CVPR,2023,Papers,Video-Language,,,https://openaccess.thecvf.com/content/CVPR2023/papers/Ashutosh_HierVL_Learning_Hierarchical_Video-Language_Embeddings_CVPR_2023_paper.pdf,https://vision.cs.utexas.edu/projects/hiervl/,,,,
"19/07/2023, 03:24:24",mirco.pl.93@gmail.com,Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment,"Zihui Xue, Kristen Grauman",UnderReview,2023,Papers,From Third-Person to First-Person,,,https://arxiv.org/abs/2306.05526,https://vision.cs.utexas.edu/projects/AlignEgoExo/,,,,
"19/07/2023, 03:28:56",mirco.pl.93@gmail.com,VOST,"Pavel Tokmakov, Jie Li, Adrien Gaidon",CVPR,2023,Papers,Datasets,,"Video Object Segmentation under Transformations (VOST). It consists of more than 700 high-resolution videos, captured in diverse environments, which are 20 seconds long on average and densely labeled with instance masks",https://openaccess.thecvf.com/content/CVPR2023/html/Tokmakov_Breaking_the_Object_in_Video_Object_Segmentation_CVPR_2023_paper.html,https://www.vostdataset.org/,,,,
"19/07/2023, 16:21:01",mirco.pl.93@gmail.com,ProjectAria, , ,2023,Devices,,,, ,https://about.meta.com/it/realitylabs/projectaria/,,,,
"19/07/2023, 17:35:45",mirco.pl.93@gmail.com,MECCANO,"F. Ragusa, A. Furnari, G. M. Farinella", ,2023,Challenges,,,Multimodal Action Recognition (RGB-Depth-Gaze), ,https://iplab.dmi.unict.it/MECCANO/challenge.html,,,,
"19/07/2023, 19:19:18",mirco.pl.93@gmail.com,Aria Digital Twin,"Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, Carl Yuheng Ren", ,2023,Papers,Datasets,,"Aria Digital Twin (ADT) - an egocentric dataset captured using Aria glasses with extensive object, environment, and human level ground truth. This ADT release contains 200 sequences of real-world activities conducted by Aria wearers. Very challenging research problems such as 3D object detection and tracking, scene reconstruction and understanding, sim-to-real learning, human pose prediction - while also inspiring new machine perception tasks for augmented reality (AR) applications",https://arxiv.org/pdf/2306.06362.pdf,https://www.projectaria.com/datasets/adt/,https://github.com/facebookresearch/projectaria_tools,,,
"22/07/2023, 00:03:00",mirco.pl.93@gmail.com,Affordances from Human Videos as a Versatile Representation for Robotics," Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, Deepak Pathak",CVPR,2023,Papers,Human to Robot,,,https://openaccess.thecvf.com/content/CVPR2023/html/Bahl_Affordances_From_Human_Videos_as_a_Versatile_Representation_for_Robotics_CVPR_2023_paper.html,https://vision-robotics-bridge.github.io/,,,,
"22/07/2023, 03:00:23",mirco.pl.93@gmail.com,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction,"Razvan-George Pasca, Alexey Gavryushin, Yen-Ling Kuo, Luc Van Gool, Otmar Hilliges, Xi Wang", ,2023,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2301.09209,https://eth-ait.github.io/transfusion-proj/,,,,
"28/07/2023, 14:02:33",mirco.pl.93@gmail.com,Deep-learning Based Egocentric Action Anticipation: A Survey,"Richard Wardle, Sareh Rowlands",Machine Vision and Applications,2023,Surveys,,,,https://www.researchsquare.com/article/rs-3156532/v1,,,,,
"12/09/2023, 12:02:22",mirco.pl.93@gmail.com,Streaming egocentric action anticipation: An evaluation scheme and approach,"Antonino Furnari, Giovanni Maria Farinella",CVIU,2023,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/pdf/2306.16682.pdf,,,,,
"12/09/2023, 12:15:45",mirco.pl.93@gmail.com,Single-Stage Visual Query Localization in Egocentric Videos,"Hanwen Jiang, Santhosh Kumar Ramakrishnan, Kristen Grauman", ,2023,Papers,Retrieval,,,https://arxiv.org/abs/2306.09324,https://hwjiang1510.github.io/VQLoC/,,,,
"12/09/2023, 12:30:52",mirco.pl.93@gmail.com,VS-TransGRU: A Novel Transformer-GRU-based Framework Enhanced by Visual-Semantic Fusion for Egocentric Action Anticipation,"Congqi Cao, Ze Sun, Qinyi Lv, Lingtong Min, Yanning Zhang", ,2023,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2307.03918,,,,,
"12/09/2023, 12:43:45",mirco.pl.93@gmail.com,EgoAdapt: A multi-stream evaluation study of adaptation to real-world egocentric user video,"Matthias De Lange, Hamid Eghbalzadeh, Reuben Tan, Michael Iuzzolino, Franziska Meier, Karl Ridgeway", ,2023,Papers,Applications,,,https://arxiv.org/abs/2307.05784,,,,,
"12/09/2023, 15:05:42",mirco.pl.93@gmail.com,EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone,"Shraman Pramanick, Yale Song, Sayan Nag, Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou, Rama Chellappa, Pengchuan Zhang",ICCV,2023,Papers,Video-Language,,,https://arxiv.org/abs/2307.05463,https://shramanpramanick.github.io/EgoVLPv2/,,,,
"12/09/2023, 15:18:03",mirco.pl.93@gmail.com,WEAR,"Marius Bock, Hilde Kuehne, Kristof Van Laerhoven, Michael Moeller", ,2023,Papers,Datasets,,The dataset comprises data from 18 participants performing a total of 18 different workout activities with untrimmed inertial (acceleration) and camera (egocentric video) data recorded at 10 different outside locations.,https://arxiv.org/abs/2304.05088,https://mariusbock.github.io/wear/,,,,
"12/09/2023, 15:25:04",mirco.pl.93@gmail.com,Free-Form Composition Networks for Egocentric Action Recognition,"Haoran Wang, Qinghua Cheng, Baosheng Yu, Yibing Zhan, Dapeng Tao, Liang Ding, Haibin Ling", ,2023,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/abs/2307.06527,,,,,
"12/09/2023, 15:28:46",mirco.pl.93@gmail.com,Multimodal Distillation for Egocentric Action Recognition,"Gorjan Radevski, Dusan Grujicic, Marie-Francine Moens, Matthew Blaschko, Tinne Tuytelaars",ICCV,2023,Papers,Multi-Modalities,Audio-Visual,,https://arxiv.org/abs/2307.07483,,https://github.com/gorjanradevski/multimodal-distillation,,,
"12/09/2023, 15:31:34",mirco.pl.93@gmail.com,Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting,"Wentao Bao, Lele Chen, Libing Zeng, Zhong Li, Yi Xu, Junsong Yuan, Yu Kong",ICCV,2023,Papers,Action Anticipation,Trajectory prediction,,https://arxiv.org/abs/2307.08243,,,,,
"12/09/2023, 15:42:15",mirco.pl.93@gmail.com,POV-Surgery,"Rui Wang, Sophokles Ktistakis, Siwei Zhang, Mirko Meboldt, Quentin Lohmeyer",MICCAI,2023,Papers,Datasets,,"POV-Surgery, a large-scale, synthetic, egocentric dataset focusing on pose estimation for hands with different surgical gloves and three orthopedic surgical instruments, namely scalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329 frames, featuring high-resolution RGB-D video streams with activity annotations, accurate 3D and 2D annotations for hand-object pose, and 2D hand-object segmentation masks.",https://arxiv.org/abs/2307.10387,https://batfacewayne.github.io/POV_Surgery_io/,,,,
"12/09/2023, 15:48:05",mirco.pl.93@gmail.com,Multiscale Video Pretraining for Long-Term Activity Forecasting,"Reuben Tan, Matthias De Lange, Michael Iuzzolino, Bryan A. Plummer, Kate Saenko, Karl Ridgeway, Lorenzo Torresani", ,2023,Papers,Action Anticipation,Long-Term Action Anticipation,,https://arxiv.org/abs/2307.12854,,,,,
"12/09/2023, 15:55:13",mirco.pl.93@gmail.com,AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?,"Qi Zhao, Ce Zhang, Shijie Wang, Changcheng Fu, Nakul Agarwal, Kwonjoon Lee, Chen Sun", ,2023,Papers,Action Anticipation,Long-Term Action Anticipation,,https://arxiv.org/abs/2307.16368,https://brown-palm.github.io/AntGPT/,,,,
"12/09/2023, 16:07:42",mirco.pl.93@gmail.com,Quasi-Online Detection of Take and Release Actions from Egocentric Videos,"Rosario Scavo, Francesco Ragusa, Giovanni Maria Farinella, Antonino Furnari",ICIAP,2023,Papers,Temporal Segmentation (Action Detection),,,https://link.springer.com/chapter/10.1007/978-3-031-43153-1_2,,https://github.com/fpv-iplab/Quasi-Online-Detection-Take-Release,,,
"12/09/2023, 16:14:39",mirco.pl.93@gmail.com,Learning Interaction Regions and Motion Trajectories Simultaneously From Egocentric Demonstration Videos,"Jianjia Xin, Lichun Wang, Kai Xu, Chao Yang, Baocai Yin, ",RA-L,2023,Papers,Human to Robot,,,https://ieeexplore.ieee.org/abstract/document/10202190/authors#authors,,,,,
"12/09/2023, 16:50:45",mirco.pl.93@gmail.com,Domain-Guided Spatio-Temporal Self-Attention for Egocentric 3D Pose Estimation,"Jinman Park,  Kimathi Kaai,  Saad Hossain,  Norikatsu Sumi,  Sirisha Rambhatla,  Paul Fieguth",KDD,2023,Papers,User Data from an Egocentric Point of View,,,https://dl.acm.org/doi/abs/10.1145/3580305.3599312,,https://github.com/jmpark0808/Ego-STAN,,,
"12/09/2023, 16:59:07",mirco.pl.93@gmail.com,InterTracker: Discovering and Tracking General Objects Interacting with Hands in the Wild,"Yanyan Shao, Qi Ye, Wenhan Luo, Kaihao Zhang, Jiming Chen",IROS,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2308.03061,,,,,
"12/09/2023, 17:10:19",mirco.pl.93@gmail.com,Object Goal Navigation with Recursive Implicit Maps,"hizhe Chen, Thomas Chabal, Ivan Laptev, Cordelia Schmid",IROS,2023,Papers,Localization,,,https://arxiv.org/pdf/2308.05602.pdf,https://www.di.ens.fr/willow/research/onav_rim/,,,,
"12/09/2023, 17:12:35",mirco.pl.93@gmail.com,An Outlook into the Future of Egocentric Vision,"Chiara Plizzari, Gabriele Goletto, Antonino Furnari, Siddhant Bansal, Francesco Ragusa, Giovanni Maria Farinella, Dima Damen, Tatiana Tommasi", ,2023,Surveys,,,,https://arxiv.org/abs/2308.07123,,,,,
"12/09/2023, 17:17:02",mirco.pl.93@gmail.com,ARGO1M,"Chiara Plizzari, Toby Perrett, Barbara Caputo, Dima Damen",ICCV,2023,Papers,Datasets,," Action Recognition Generalisation dataset (ARGO1M) from videos and narrations from Ego4D. ARGO1M is the first to test action generalisation across both scenario and location shifts, and is the largest domain generalisation dataset across images and video.",https://arxiv.org/abs/2306.08713,https://chiaraplizz.github.io/what-can-a-cook/,,,,
"12/09/2023, 17:21:43",mirco.pl.93@gmail.com,EPIC Fields,"Vadim Tschernezki, Ahmad Darkhalil, Zhifan Zhu, David Fouhey, Iro Laina, Diane Larlus, Dima Damen, Andrea Vedaldi", ,2023,Papers,Datasets,,"EPIC Fields, an augmentation of EPIC-KITCHENS with 3D camera information. Like other datasets for neural rendering, EPIC Fields removes the complex and expensive step of reconstructing cameras using photogrammetry, and allows researchers to focus on modelling problems.",https://arxiv.org/abs/2306.08731,https://epic-kitchens.github.io/epic-fields/,,,,
"13/09/2023, 20:06:34",mirco.pl.93@gmail.com,Dynamic Context Removal: A General Training Strategy for Robust Models on Video Action Predictive Tasks,"Xinyu Xu, Yong-Lu Li, Cewu Lu",IJCV,2023,Papers,Action Anticipation,Short-Term Action Anticipation,,https://link.springer.com/article/10.1007/s11263-023-01850-6,,,,,
"13/09/2023, 20:10:06",mirco.pl.93@gmail.com, Improved Deep Learning-Based Efficientpose Algorithm for Egocentric Marker-Less Tool and Hand Pose Estimation in Manual Assembly,"Zihan Niu, Yi Xia, Jun Zhang, Bing Wang, Peng Chen",ICIC,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://link.springer.com/chapter/10.1007/978-981-99-4761-4_25,,,,,
"13/09/2023, 20:14:16",mirco.pl.93@gmail.com,Leveraging Next-Active Objects for Context-Aware Anticipation in Egocentric Videos,"Sanket Thakur, Cigdem Beyan, Pietro Morerio, Vittorio Murino, Alessio Del Bue",WACV,2024,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/pdf/2308.08303.pdf,,,,,
"13/09/2023, 21:00:02",mirco.pl.93@gmail.com,Helping Hands: An Object-Aware Ego-Centric Video Recognition Model,"Chuhan Zhang, Ankush Gupta, Andrew Zisserman",ICCV,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2308.07918,,,,,
"14/09/2023, 00:32:07",mirco.pl.93@gmail.com,Memory-and-Anticipation Transformer for Online Action Understanding,"Jiahao Wang, Guo Chen, Yifei Huang, Limin Wang, Tong Lu",ICCV,2023,Papers,Temporal Segmentation (Action Detection),,,https://arxiv.org/abs/2308.07893,,https://github.com/Echo0125/Memory-and-Anticipation-Transformer,,,
"14/09/2023, 00:45:55",mirco.pl.93@gmail.com,Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition,"Qitong Wang, Long Zhao, Liangzhe Yuan, Ting Liu, Xi Peng",ICCV,2023,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/abs/2308.11489,,https://github.com/wqtwjt1996/SUM-L,,,
"14/09/2023, 00:55:38",mirco.pl.93@gmail.com,Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images,"Tze Ho Elden Tse, Franziska Mueller, Zhengyang Shen, Danhang Tang, Thabo Beeler, Mingsong Dou, Yinda Zhang, Sasa Petrovic, Hyung Jin Chang, Jonathan Taylor, Bardia Doosti",ICCV,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2308.11015,,,,,
"14/09/2023, 01:11:50",mirco.pl.93@gmail.com,Opening the Vocabulary of Egocentric Actions,"Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao", ,2023,Papers,Action/Activity Recognition,Zero-Shot Learning,,https://arxiv.org/abs/2308.11488,https://dibschat.github.io/openvocab-egoAR/,,,,
"14/09/2023, 01:30:04",mirco.pl.93@gmail.com,EventTransAct: A video transformer-based framework for Event-camera based action recognition,"Tristan de Blegiers, Ishan Rajendrakumar Dave, Adeel Yousaf, Mubarak Shah",IROS,2023,Papers,Multi-Modalities,Event,,https://arxiv.org/abs/2308.13711,https://tristandb8.github.io/EventTransAct_webpage/,,,,
"14/09/2023, 01:51:39",mirco.pl.93@gmail.com,An Optimized Pipeline for Image-Based Localization in Museums from Egocentric Images,"Nicola Messina, Fabrizio Falchi, Antonino Furnari, Claudio Gennaro, Giovanni Maria Farinella ",ICIAP,2023,Papers,Localization,,,https://link.springer.com/chapter/10.1007/978-3-031-43148-7_43,,,,,
"14/09/2023, 01:54:01",mirco.pl.93@gmail.com,HERO: A Multi-modal Approach on Mobile Devices for Visual-Aware Conversational Assistance in Industrial Domains,"Claudia Bonanno, Francesco Ragusa, Antonino Furnari, Giovanni Maria Farinella ",ICIAP,2023,Papers,Video-Language,,,https://link.springer.com/chapter/10.1007/978-3-031-43148-7_36,,,,,
"14/09/2023, 02:00:20",mirco.pl.93@gmail.com,EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding,"Yue Xu, Yong-Lu Li, Zhemin Huang, Michael Xu Liu, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang",ICCV,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2309.02423,https://mvig-rhos.com/ego_pca,,,,
"14/09/2023, 02:12:38",mirco.pl.93@gmail.com,Multi-label affordance mapping from egocentric vision,"Lorenzo Mur-Labadia, Jose J. Guerrero, Ruben Martinez-Cantin",ICCV,2023,Papers,Activity-context,,,https://arxiv.org/abs/2309.02120,,,,,
"20/12/2023, 17:32:14",mirco.pl.93@gmail.com,Enhancing Next Active Object-based Egocentric Action Anticipation with Guided Attention,"Sanket Thakur, Cigdem Beyan, Pietro Morerio, Vittorio Murino, Alessio Del Bue",ICIP,2023,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2305.12953,https://sanketsans.github.io/guided-attention-egocentric.html,,,,
"20/12/2023, 17:34:40",mirco.pl.93@gmail.com,Guided Attention for Next Active Object @ EGO4D STA Challenge,"Sanket Thakur, Cigdem Beyan, Pietro Morerio, Vittorio Murino, Alessio Del Bue",CVPRW,2023,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2305.16066,,,,,
"20/12/2023, 17:53:06",mirco.pl.93@gmail.com," Hands, Objects, Action! Egocentric 2D Hand-Based Action Recognition","Wiktor Mucha, Martin Kampel",ICVS,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://link.springer.com/chapter/10.1007/978-3-031-44137-0_3,,,,,
"20/12/2023, 18:06:42",mirco.pl.93@gmail.com,InCrowdFormer: On-Ground Pedestrian World Model From Egocentric Views,"Mai Nishimura, Shohei Nobuhara, Ko Nishino", ,2023,Papers,Localization,,,https://arxiv.org/abs/2303.09534,,,,,
"20/12/2023, 18:24:38",mirco.pl.93@gmail.com,EgoObjects,"Chenchen Zhu, Fanyi Xiao, Andres Alvarado, Yasmine Babaei, Jiabo Hu, Hichem El-Mohri, Sean Culatana, Roshan Sumbaly, Zhicheng Yan",ICCV,2023,Papers,Datasets,,"EgoObjects, a large-scale egocentric dataset for fine-grained object understanding. contains over 9K videos collected by 250 participants from 50+ countries using 4 wearable devices, and over 650K object annotations from 368 object categories.",https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_EgoObjects_A_Large-Scale_Egocentric_Dataset_for_Fine-Grained_Object_Understanding_ICCV_2023_paper.html,,https://github.com/facebookresearch/EgoObjects,,,
"20/12/2023, 18:54:00",mirco.pl.93@gmail.com,Ego3DPose: Capturing 3D Cues from Binocular Egocentric Views,"Taeho Kang, Kyungjin Lee, Jinrui Zhang, Youngki Lee",SIGGRAPH,2023,Papers,User Data from an Egocentric Point of View,,,https://dl.acm.org/doi/abs/10.1145/3610548.3618147?casa_token=sfD7LG7W-icAAAAA:SLe0a5OVLpchi3l0_lyDcYx_ecufq5NwMauQsBAFuAC96k1WeMTKlycq6-agw9imu1Brqf35-K0B,,,,,
"20/12/2023, 20:04:34",mirco.pl.93@gmail.com,EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries,"Jinjie Mai, Abdullah Hamdi, Silvio Giancola, Chen Zhao, Bernard Ghanem",ICCV,2023,Papers,Localization,,,https://openaccess.thecvf.com/content/ICCV2023/html/Mai_EgoLoc_Revisiting_3D_Object_Localization_from_Egocentric_Videos_with_Visual_ICCV_2023_paper.html,,https://github.com/Wayne-Mai/EgoLoc,,,
"20/12/2023, 20:18:43",mirco.pl.93@gmail.com,Ego-Only: Egocentric Action Detection without Exocentric Transferring,"Huiyu Wang, Mitesh Kumar Singh, Lorenzo Torresani",ICCV,2023,Papers,Temporal Segmentation (Action Detection),,,https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Ego-Only_Egocentric_Action_Detection_without_Exocentric_Transferring_ICCV_2023_paper.html,,,,,
"20/12/2023, 21:37:32",mirco.pl.93@gmail.com,Self-Supervised Object Detection from Egocentric Videos,"Peri Akiva, Jing Huang, Kevin J Liang, Rama Kovvuri, Xingyu Chen, Matt Feiszli, Kristin Dana, Tal Hassner",ICCV,2023,Papers,Other EGO-Context,,,https://openaccess.thecvf.com/content/ICCV2023/html/Akiva_Self-Supervised_Object_Detection_from_Egocentric_Videos_ICCV_2023_paper.html,,,,,
"20/12/2023, 21:46:58",mirco.pl.93@gmail.com,HoloAssist,"Xin Wang, Taein Kwon, Mahdi Rad, Bowen Pan, Ishani Chakraborty, Sean Andrist, Dan Bohus, Ashley Feniello, Bugra Tekin, Felipe Vieira Frujeri, Neel Joshi, Marc Pollefeys",ICCV,2023,Papers,Datasets,,"HoloAssist: a large-scale egocentric human interaction dataset that spans 166 hours of data captured by 350 unique instructor-performer pairs, wearing mixed-reality headsets during collaborative tasks.",https://openaccess.thecvf.com/content/ICCV2023/html/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.html,https://holoassist.github.io/,,,,
"20/12/2023, 22:01:00",mirco.pl.93@gmail.com,Egocentric RGB+Depth Action Recognition in Industry-Like Settings,"Jyoti Kini, Sarah Fleischer, Ishan Dave, Mubarak Shah", ,2023,Papers,Multi-Modalities,Depth,,https://arxiv.org/abs/2309.13962,,,,,
"20/12/2023, 22:10:21",mirco.pl.93@gmail.com,Audio Visual Speaker Localization from EgoCentric Views,"Jinzheng Zhao, Yong Xu, Xinyuan Qian, Wenwu Wang", ,2023,Papers,Multi-Modalities,Audio-Visual,,https://arxiv.org/abs/2309.16308,,https://github.com/KawhiZhao/Egocentric-Audio-Visual-Speaker-Localization,,,
"20/12/2023, 22:21:46",mirco.pl.93@gmail.com,Learning Temporal Sentence Grounding From Narrated EgoVideos,"Kevin Flanagan, Dima Damen, Michael Wray",BMVC,2023,Papers,Retrieval,,,https://arxiv.org/abs/2310.17395,,https://github.com/keflanagan/CliMer,,,
"20/12/2023, 22:23:23",mirco.pl.93@gmail.com,A Survey on Deep Learning Techniques for Action Anticipation,"Zeyun Zhong, Manuel Martin, Michael Voit, Juergen Gall, Jürgen Beyerer", ,2023,Surveys,,,,https://arxiv.org/abs/2309.17257,,,,,
"20/12/2023, 22:37:33",mirco.pl.93@gmail.com,Action Scene Graphs for Long-Form Understanding of Egocentric Videos,"Ivan Rodin, Antonino Furnari, Kyle Min, Subarna Tripathi, Giovanni Maria Farinella", ,2023,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2312.03391,,https://github.com/fpv-iplab/easg,,,
"21/12/2023, 00:51:10",mirco.pl.93@gmail.com,HiFiHR: Enhancing 3D Hand Reconstruction from a Single Image via High-Fidelity Texture,"Jiayin Zhu, Zhuoran Zhao, Linlin Yang, Angela Yao",DAGM,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2308.13628,,https://github.com/viridityzhu/HiFiHR,,,
"21/12/2023, 01:01:09",mirco.pl.93@gmail.com,CaSAR: Contact-aware Skeletal Action Recognition,"Junan Lin, Zhichao Sun, Enjie Cao, Taein Kwon, Mahdi Rad, Marc Pollefeys", ,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2309.10001,,,,,
"21/12/2023, 01:25:15",mirco.pl.93@gmail.com,EGOFALLS,Xueyi Wang, ,2023,Papers,Datasets,,"The dataset comprises 10,948 video samples from 14 subjects, focusing on falls among the elderly. Extracting multimodal descriptors from egocentric camera videos.",https://arxiv.org/abs/2309.04579,,,,,
"21/12/2023, 01:40:20",mirco.pl.93@gmail.com,VidChapters-7M,"Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, Cordelia Schmid",NeurIPS,2023,Papers,Datasets,,"VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters in total. VidChapters-7M is automatically created from videos online in a scalable manner by scraping user-annotated chapters and hence without any additional manual annotation.",https://arxiv.org/abs/2309.13952,https://antoyang.github.io/vidchapters.html,,,,
"21/12/2023, 02:07:46",mirco.pl.93@gmail.com,Behavioural pattern discovery from collections of egocentric photo-streams,"Martin Menchon, Estefania Talavera, Jose M Massa, Petia Radeva",Pervasive and Mobile Computing,2023,Papers,Video summarization,,,https://arxiv.org/pdf/2008.09561.pdf,,,,,
"21/12/2023, 12:34:47",mirco.pl.93@gmail.com,COPILOT: Human-Environment Collision Prediction and Localization from Egocentric Videos,"Boxiao Pan, Bokui Shen, Davis Rempe, Despoina Paschalidou, Kaichun Mo, Yanchao Yang, Leonidas J. Guibas",ICCV,2023,Papers,Other EGO-Context,,,https://openaccess.thecvf.com/content/ICCV2023/papers/Pan_COPILOT_Human-Environment_Collision_Prediction_and_Localization_from_Egocentric_Videos_ICCV_2023_paper.pdf,https://sites.google.com/stanford.edu/copilot,,,,
"21/12/2023, 13:39:14",mirco.pl.93@gmail.com,ENIGMA-51,"Francesco Ragusa, Rosario Leonardi, Michele Mazzamuto, Claudia Bonanno, Rosario Scavo, Antonino Furnari, Giovanni Maria Farinella",WACV,2023,Papers,Datasets,,"ENIGMA-51 is a new egocentric dataset acquired in an industrial scenario by 19 subjects who followed instructions to complete the repair of electrical boards using industrial tools (e.g., electric screwdriver) and equipments (e.g., oscilloscope). The 51 egocentric video sequences are densely annotated with a rich set of labels that enable the systematic study of human behavior in the industrial domain.",https://arxiv.org/abs/2309.14809,https://iplab.dmi.unict.it/ENIGMA-51/#paper,,,,
"21/12/2023, 18:37:21",mirco.pl.93@gmail.com,Learning Video Representations from Large Language Models,"Yue Zhao, Ishan Misra, Philipp Krähenbühl, Rohit Girdhar",CVPR,2023,Papers,Video-Language,,,https://arxiv.org/abs/2212.04501,https://facebookresearch.github.io/LaViLa/,https://github.com/facebookresearch/LaViLa,,,
"21/12/2023, 18:39:14",mirco.pl.93@gmail.com,Training a Large Video Model on a Single Machine in a Day,"Yue Zhao, Philipp Krähenbühl", ,2023,Papers,Applications,,,https://arxiv.org/abs/2309.16669,,https://github.com/zhaoyue-zephyrus/AVION,,,
"21/12/2023, 18:53:56",mirco.pl.93@gmail.com,In the Eye of Transformer: Global–Local Correlation for Egocentric Gaze Estimation and Beyond,"Bolin Lai, Miao Liu, Fiona Ryan, James M. Rehg",IJCV,2023,Papers,Gaze,,,https://link.springer.com/article/10.1007/s11263-023-01879-7,https://bolinlai.github.io/GLC-EgoGazeEst/,,,,
"21/12/2023, 19:02:21",mirco.pl.93@gmail.com,Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge,"Te-Lin Wu, Yu Zhou, Nanyun Peng",EMNLP,2023,Papers,Video-Language,,,https://arxiv.org/abs/2310.15066,,,,,
"21/12/2023, 19:12:55",mirco.pl.93@gmail.com,IndustReal,"Tim J. Schoonbeek, Tim Houben, Hans Onvlee, Peter H.N. de With, Fons van der Sommen",WACV,2024,Papers,Datasets,,"The IndustReal dataset contains 84 videos, demonstrating how 27 participants perform maintenance and assembly procedures on a construction-toy assembly set",https://arxiv.org/abs/2310.17323,,https://github.com/TimSchoonbeek/IndustReal,,,
"22/12/2023, 00:29:32",mirco.pl.93@gmail.com,Slowfast Diversity-aware Prototype Learning for Egocentric Action Recognition,"Guangzhao Dai, Xiangbo Shu, Rui Yan, Peng Huang,  Jinhui Tang",MM,2023,Papers,Action/Activity Recognition,Action Recognition,,https://dl.acm.org/doi/abs/10.1145/3581783.3612144,,,,,
"22/12/2023, 00:39:37",mirco.pl.93@gmail.com,POV: Prompt-Oriented View-Agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World,"Boshen Xu, Sipeng Zheng, Qin Jin",MM,2023,Papers,From Third-Person to First-Person,,,https://dl.acm.org/doi/abs/10.1145/3581783.3612484,,,,,
"22/12/2023, 00:48:12",mirco.pl.93@gmail.com,Object-centric Video Representation for Long-term Action Anticipation,"Ce Zhang, Changcheng Fu, Shijie Wang, Nakul Agarwal, Kwonjoon Lee, Chiho Choi, Chen Sun",WACV,2024,Papers,Action Anticipation,Long-Term Action Anticipation,,https://arxiv.org/abs/2311.00180,,https://github.com/brown-palm/ObjectPrompt,,,
"22/12/2023, 00:50:26",mirco.pl.93@gmail.com,Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective,"Pengfei Wei, Lingdong Kong, Xinghua Qu, Yi Ren, Zhiqiang Xu, Jing Jiang, Xiang Yin",NeurIPS,2023,Papers,Action/Activity Recognition,Usupervised Domain Adaptation,,https://arxiv.org/abs/2208.07365,,https://github.com/ldkong1205/TranSVAE,,,
"22/12/2023, 11:44:25",mirco.pl.93@gmail.com,Functional Hand Type Prior for 3D Hand Pose Estimation and Action Recognition from Egocentric View Monocular Videos,"Wonseok Roh, Seung Hyun Lee, Won Jeong Ryoo, Jakyung Lee, Gyeongrok Oh, Sooyeon Hwang, Hyung-gun Chi, Sangpil Kim",BMVC,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://papers.bmvc2023.org/0193.pdf,,,,,
"22/12/2023, 11:46:09",mirco.pl.93@gmail.com,DiffAnt: Diffusion Models for Action Anticipation,"Zeyun Zhong, Chengzhi Wu, Manuel Martin, Michael Voit, Juergen Gall, Jürgen Beyerer", ,2023,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2311.15991,,,,,
"22/12/2023, 12:31:08",mirco.pl.93@gmail.com,Exo2EgoDVC,"Takehiko Ohkawa, Takuma Yagi, Taichi Nishimura, Ryosuke Furuta, Atsushi Hashimoto, Yoshitaka Ushiku, Yoichi Sato", ,2023,Papers,Datasets,,"EgoYC2, a novel egocentric dataset, adapts procedural captions from YouCook2 to cooking videos re-recorded with head-mounted cameras. Unique in its weakly-paired approach, it aligns caption content with exocentric videos, distinguishing itself from other datasets focused on action labels.",https://arxiv.org/abs/2311.16444,,,,,
"22/12/2023, 12:34:38",mirco.pl.93@gmail.com,Egocentric Whole-Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement,"Jian Wang, Zhe Cao, Diogo Luvizon, Lingjie Liu, Kripasindhu Sarkar, Danhang Tang, Thabo Beeler, Christian Theobalt",CVPR,2024,Papers,User Data from an Egocentric Point of View,,,https://arxiv.org/abs/2311.16495,https://people.mpi-inf.mpg.de/~jianwang/projects/egowholemocap/,,,,
"22/12/2023, 12:36:32",mirco.pl.93@gmail.com,EgoWholeBody,"Jian Wang, Zhe Cao, Diogo Luvizon, Lingjie Liu, Kripasindhu Sarkar, Danhang Tang, Thabo Beeler, Christian Theobalt", ,2023,Papers,Datasets,,"EgoWholeBody, a large synthetic dataset, comprising 840,000 high-quality egocentric images captured across a diverse range of whole-body motion sequences. Quantitative and qualitative evaluations demonstrate the effectiveness of our method in producing high-quality whole-body motion estimates from a single egocentric camera.",https://arxiv.org/abs/2311.16495,https://people.mpi-inf.mpg.de/~jianwang/projects/egowholemocap/,,,,
"22/12/2023, 13:52:14",mirco.pl.93@gmail.com,Centre Stage: Centricity-based Audio-Visual Temporal Action Detection,"Hanyuan Wang, Majid Mirmehdi, Dima Damen, Toby Perrett",WBMVC,2023,Papers,Multi-Modalities,Audio-Visual,,https://arxiv.org/abs/2311.16446,,https://github.com/hanielwang/Audio-Visual-TAD,,,
"22/12/2023, 14:00:18",mirco.pl.93@gmail.com,Object-based (yet Class-agnostic) Video Domain Adaptation,"Dantong Niu, Amir Bar, Roei Herzig, Trevor Darrell, Anna Rohrbach", ,2023,Papers,Action/Activity Recognition,Usupervised Domain Adaptation,,https://arxiv.org/abs/2311.17942,,,,,
"22/12/2023, 14:20:21",mirco.pl.93@gmail.com,"United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos","Siddhant Bansal, Chetan Arora, C.V. Jawahar",WACV,2024,Papers,Applications,,,https://arxiv.org/abs/2311.03550,,,,,
"22/12/2023, 17:17:59",mirco.pl.93@gmail.com,Ego-Exo4D,"Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin J Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas Bertasius, David Crandall, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, Michael Wray",CVPR,2024,Papers,Datasets,,"Ego-Exo4D, a vast multimodal multiview video dataset capturing skilled human activities in both egocentric and exocentric perspectives (e.g., sports, music, dance). With 800+ participants in 13 cities, it offers 1,422 hours of combined footage, featuring diverse activities in 131 natural scene contexts, ranging from 1 to 42 minutes per video.",https://arxiv.org/abs/2311.18259,https://ego-exo4d-data.org/,,,,
"22/12/2023, 17:22:28",mirco.pl.93@gmail.com, SHARE ON Towards Egocentric Compositional Action Anticipation with Adaptive Semantic Debiasing,"  Tianyu Zhang  ,  Weiqing Min, Tao Liu, Shuqiang Jiang, Yong Rui",TOMM,2023,Papers,Action Anticipation,Short-Term Action Anticipation,,https://dl.acm.org/doi/pdf/10.1145/3633333,,,,,
"22/12/2023, 17:29:42",mirco.pl.93@gmail.com,MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians,"Chandradeep Pokhariya, Ishaan N Shah, Angela Xing, Zekun Li, Kefan Chen, Avinash Sharma, Srinath Sridhar", ,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2312.02137,https://ivl.cs.brown.edu/research/manus.html,,,,
"22/12/2023, 23:51:59",mirco.pl.93@gmail.com,Learning from One Continuous Video Stream,"João Carreira, Michael King, Viorica Pătrăucean, Dilara Gokay, Cătălin Ionescu, Yi Yang, Daniel Zoran, Joseph Heyward, Carl Doersch, Yusuf Aytar, Dima Damen, Andrew Zisserman", ,2023,Papers,Applications,,,https://arxiv.org/abs/2312.00598,,,,,
"23/12/2023, 00:20:07",mirco.pl.93@gmail.com,Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark,"Rosario Leonardi, Antonino Furnari, Francesco Ragusa, Giovanni Maria Farinella", ,2023,Papers,Segmentation,,,https://arxiv.org/pdf/2312.02672.pdf,https://iplab.dmi.unict.it/HOI-Synth/,,,,
"23/12/2023, 00:22:53",mirco.pl.93@gmail.com,Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs,"Camillo Quattrocchi, Antonino Furnari, Daniele Di Mauro, Mario Valerio Giuffrida, Giovanni Maria Farinella", ,2023,Papers,Temporal Segmentation (Action Detection),,,https://arxiv.org/abs/2312.02638,,https://github.com/fpv-iplab/synchronization-is-all-you-need,,,
"23/12/2023, 00:34:06",mirco.pl.93@gmail.com,IT3DEgo,"Yunhan Zhao, Haoyu Ma, Shu Kong, Charless Fowlkes", ,2023,Papers,Datasets,,"IT3DEgo dataset: Addresses 3D instance tracking using egocentric sensors (AR/VR). Recorded in diverse indoor scenes with HoloLens2, it comprises 50 recordings (5+ minutes each). Evaluates tracking performance in 3D coordinates, leveraging camera pose and allocentric representation.",https://arxiv.org/pdf/2312.04117.pdf,,https://github.com/IT3DEgo/IT3DEgo/,,,
"23/12/2023, 11:25:10",mirco.pl.93@gmail.com,LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning,"Bolin Lai, Xiaoliang Dai, Lawrence Chen, Guan Pang, James M. Rehg, Miao Liu", ,2023,Papers,Video-Language,,,https://arxiv.org/abs/2312.03849,,,,,
"23/12/2023, 11:36:09",mirco.pl.93@gmail.com,Early Action Recognition with Action Prototypes,"Guglielmo Camporese, Alessandro Bergamo, Xunyu Lin, Joseph Tighe, Davide Modolo", ,2023,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2312.06598,,,,,
"23/12/2023, 11:39:10",mirco.pl.93@gmail.com,3D Hand Pose Estimation in Egocentric Images in the Wild,"Aditya Prakash, Ruisen Tu, Matthew Chang, Saurabh Gupta", ,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2312.06583,https://ap229997.github.io/projects/hands/,,,,
"23/12/2023, 11:44:37",mirco.pl.93@gmail.com,Grounded Question-Answering in Long Egocentric Videos,"Shangzhe Di, Weidi Xie", ,2023,Papers,Video-Language,,,https://arxiv.org/abs/2312.06505,,,,,
"23/12/2023, 11:52:54",mirco.pl.93@gmail.com,GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos,"Tomáš Souček, Dima Damen, Michael Wray, Ivan Laptev, Josef Sivic", ,2023,Papers,Video-Language,,,https://arxiv.org/abs/2312.07322,https://soczech.github.io/genhowto/,,,,
"23/12/2023, 12:05:00",mirco.pl.93@gmail.com,Semantic-Disentangled Transformer With Noun-Verb Embedding for Compositional Action Recognition,"Peng Huang,  Rui Yan,  Xiangbo Shu, Zhewei Tu,  Guangzhao Dai, Jinhui Tang",TIP,2023,Papers,Action/Activity Recognition,Action Recognition,,https://ieeexplore.ieee.org/abstract/document/10363109,,,,,
"23/12/2023, 22:50:16",mirco.pl.93@gmail.com,Egocentric Action Recognition by Capturing Hand-Object Contact and Object State,"Tsukasa Shiota, Motohiro Takagi, Kaori Kumagai, Hitoshi Seshimo, Yushi Aono",WACV,2024,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com/content/WACV2024/papers/Shiota_Egocentric_Action_Recognition_by_Capturing_Hand-Object_Contact_and_Object_State_WACV_2024_paper.pdf,,,,,
"23/12/2023, 22:59:18",mirco.pl.93@gmail.com,SEMA: Semantic Attention for Capturing Long-Range Dependencies in Egocentric Lifelogs,"Pravin Nagar, K.N Ajay Shastry, Jayesh Chaudhari, Chetan Arora",WACV,2024,Papers,Video summarization,,,https://openaccess.thecvf.com/content/WACV2024/papers/Nagar_SEMA_Semantic_Attention_for_Capturing_Long-Range_Dependencies_in_Egocentric_Lifelogs_WACV_2024_paper.pdf,,https://github.com/Pravin74/Semantic_attention/,,,
"23/12/2023, 23:01:31",mirco.pl.93@gmail.com,Interaction Region Visual Transformer for Egocentric Action Anticipation,"Debaditya Roy, Ramanathan Rajendiran, Basura Fernando",WACV,2024,Papers,Action Anticipation,Region prediction,,https://openaccess.thecvf.com/content/WACV2024/papers/Roy_Interaction_Region_Visual_Transformer_for_Egocentric_Action_Anticipation_WACV_2024_paper.pdf,,https://github.com/LAHAproject/InAViT,,,
"25/02/2024, 16:23:16",mirco.pl.93@gmail.com,Get a Grip: Reconstructing Hand-Object Stable Grasps in Egocentric Videos,"Zhifan Zhu, Dima Damen", ,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2312.15719,https://zhifanzhu.github.io/getagrip/,,,,
"25/02/2024, 16:36:29",mirco.pl.93@gmail.com,Retrieval-Augmented Egocentric Video Captioning,"Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie",CVPR,2024,Papers,Retrieval,,,https://arxiv.org/abs/2401.00789v2,,,,,
"25/02/2024, 16:50:46",mirco.pl.93@gmail.com,3D Human Pose Perception from Egocentric Stereo Videos,"Hiroyasu Akada, Jian Wang, Vladislav Golyanik, Christian Theobalt", ,2023,Papers,User Data from an Egocentric Point of View,,,https://arxiv.org/abs/2401.00889,https://4dqv.mpi-inf.mpg.de/UnrealEgo2/,,,,
"25/02/2024, 16:55:24",mirco.pl.93@gmail.com,MACS: Mass Conditioned 3D Hand and Object Motion Synthesis,"Soshi Shimada, Franziska Mueller, Jan Bednarik, Bardia Doosti, Bernd Bickel, Danhang Tang, Vladislav Golyanik, Jonathan Taylor, Christian Theobalt, Thabo Beeler", ,2023,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/pdf/2312.14929.pdf,https://vcai.mpi-inf.mpg.de/projects/MACS/,,,,
"25/02/2024, 17:03:26",mirco.pl.93@gmail.com,EgoGen: An Egocentric Synthetic Data Generator,"Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan Zhang, Marc Pollefeys, Siyu Tang",CVPR,2024,Papers,Other EGO-Context,,,https://arxiv.org/abs/2401.08739,https://ego-gen.github.io/,,,,
"25/02/2024, 17:06:40",mirco.pl.93@gmail.com,GPT4Ego: Unleashing the Potential of Pre-trained Models for Zero-Shot Egocentric Action Recognition,"Guangzhao Dai, Xiangbo Shu, Wenhao Wu", ,2024,Papers,Action/Activity Recognition,Zero-Shot Learning,,https://arxiv.org/abs/2401.10039,,,,,
"25/02/2024, 17:10:45",mirco.pl.93@gmail.com,Exploring Missing Modality in Multimodal Egocentric Datasets,"Merey Ramazanova, Alejandro Pardo, Humam Alwassel, Bernard Ghanem", ,2024,Papers,Action/Activity Recognition,Action Recognition,,https://arxiv.org/abs/2401.11470,,,,,
"25/02/2024, 17:21:09",mirco.pl.93@gmail.com,Integrating Egocentric and Robotic Vision for Object Identification Using Siamese Networks and Superquadric Estimations in Partial Occlusion Scenarios,"Elisabeth Menendez, Santiago Martínez, Fernando Díaz-de-María, Carlos Balaguer",Intelligent Human-Robot Interaction,2024,Papers,Human to Robot,,,https://www.mdpi.com/2313-7673/9/2/100,,,,,
"25/02/2024, 18:11:13",mirco.pl.93@gmail.com,Are you Struggling? Dataset and Baselines for Struggle Determination in Assembly Videos,"Shijia Feng, Michael Wray, Brian Sullivan, Casimir Ludwig, Iain Gilchrist, Walterio Mayol-Cuevas", ,2024,Papers,Applications,,,https://arxiv.org/abs/2402.11057,,,,,
"25/02/2024, 18:14:24",mirco.pl.93@gmail.com,Real-time 3D Semantic Scene Perception for Egocentric Robots with Binocular Vision,"K. Nguyen, T. Dang, M. Huber", ,2024,Papers,Human to Robot,,,https://arxiv.org/pdf/2402.11872.pdf,,https://github.com/mkhangg/semantic_scene_perception,,,
"07/04/2024, 19:14:11",mirco.pl.93@gmail.com,Relative Norm Alignment for Tackling Domain Shift in Deep Multi-modal Classification,"Mirco Planamente, Chiara Plizzari, Simone Alberto Peirone, Barbara Caputo, Andrea Bottino ",IJCV,2024,Papers,Action/Activity Recognition,Usupervised Domain Adaptation,,https://link.springer.com/article/10.1007/s11263-024-01998-9,,,,,
"07/04/2024, 19:17:59",mirco.pl.93@gmail.com,A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives,A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives,CVPR,2024,Papers,Multiple Egocentric Tasks,,,https://arxiv.org/abs/2403.03037v1,https://sapeirone.github.io/EgoPack/,,,,
"07/04/2024, 19:35:37",mirco.pl.93@gmail.com,Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation,"Ruicong Liu, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato",CVPR,2024,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2403.04381,,https://github.com/ut-vision/S2DHand,,,
"07/04/2024, 19:40:27",mirco.pl.93@gmail.com,EgoExoLearn,"Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, Baoqi Pei, Hongjie Zhang, Lu Dong, Yali Wang, Limin Wang, Yu Qiao",CVPR,2024,Papers,Datasets,,"EgoExoLearn, a large-scale dataset that emulates the human demonstration following process, in which individuals record egocentric videos as they execute tasks guided by demonstration videos. EgoExoLearn contains egocentric and demonstration video data spanning 120 hours captured in daily life scenarios and specialized laboratories.",https://arxiv.org/abs/2403.16182,,https://github.com/OpenGVLab/EgoExoLearn,,,
"07/04/2024, 19:53:51",mirco.pl.93@gmail.com,X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization,"Anna Kukleva, Fadime Sener, Edoardo Remelli, Bugra Tekin, Eric Sauser, Bernt Schiele, Shugao Ma",CVPR,2024,Papers,Action/Activity Recognition,Domain Generalization,,https://arxiv.org/abs/2403.19811,,https://github.com/annusha/xmic,,,
"07/04/2024, 20:18:21",mirco.pl.93@gmail.com,Multimodal Score Fusion with Sparse Low Rank Bilinear Pooling for Egocentric Hand Action Recognition,Kankana Roy,TOMM,2024,Papers,Multi-Modalities,Depth,,https://dl.acm.org/doi/10.1145/3656044,,,,,
"07/04/2024, 20:32:08",mirco.pl.93@gmail.com,BioVL,"Taichi Nishimura, Kojiro Sakoda, Atsushi Hashimoto, Yoshitaka Ushiku, Natsuko Tanaka, Fumihito Ono, Hirotaka Kameko, Shinsuke Mori",WICCV,2021,Papers,Datasets,,"A novel biochemical video-andlanguage (BioVL) dataset, which consists of experimental
videos, corresponding protocols, and annotations of alignment between events in the video and instructions in the protocol.  16 videos from four protocols with a total length of 1.6 hours.",https://openaccess.thecvf.com/content/ICCV2021W/CLVL/papers/Nishimura_Egocentric_Biochemical_Video-and-Language_Dataset_ICCVW_2021_paper.pdf,,,,,
"07/04/2024, 20:34:55",mirco.pl.93@gmail.com,BioVL-QR,"Taichi Nishimura, Koki Yamamoto, Yuto Haneji, Keiya Kajimura, Chihiro Nishiwaki, Eriko Daikoku, Natsuko Okuda, Fumihito Ono, Hirotaka Kameko, Shinsuke Mori", ,2024,Papers,Datasets,,"A biochemical vision-and-language dataset, which consists of 24 egocentric experiment videos, corresponding protocols, and video-and-language alignments. This study focuses on Micro QR Codes to detect objects automatically. From our preliminary study, we found that detecting objects only using Micro QR Codes is still difficult because the researchers manipulate objects, causing blur and occlusion frequently.",https://arxiv.org/abs/2404.03161v1,,,,,
"07/04/2024, 20:55:05",mirco.pl.93@gmail.com,PREGO: online mistake detection in PRocedural EGOcentric videos,"Alessandro Flaborea, Guido Maria D'Amely di Melendugno, Leonardo Plini, Luca Scofano, Edoardo De Matteis, Antonino Furnari, Giovanni Maria Farinella, Fabio Galasso",CVPR,2024,Papers,Applications,,,https://arxiv.org/abs/2404.01933,,https://github.com/aleflabo/PREGO,,,
"07/04/2024, 21:03:45",mirco.pl.93@gmail.com,Continual Egocentric Activity Recognition with Foreseeable-Generalized Visual-IMU Representations,"Chiyuan He, Shaoxu Cheng, Zihuan Qiu, Linfeng Xu, Fanman Meng, Qingbo Wu, Hongliang Li",IEEE Sensors Journal ,2024,Papers,Multi-Modalities,IMU,,https://ieeexplore.ieee.org/abstract/document/10462907/authors#authors,,,,,
"07/04/2024, 21:08:54",mirco.pl.93@gmail.com,Refining Action Boundaries for One-stage Detection,"Hanyuan Wang, Majid Mirmehdi, Dima Damen, Toby Perrett",AVSS,2024,Papers,Temporal Segmentation (Action Detection),,,https://ieeexplore.ieee.org/abstract/document/9959554?casa_token=AT_Tl3i91NUAAAAA:h_YK0QbnF65WRQL_Au9y82_FPV7SPbQnc_Duw3CTZ3MtMgrvSfH7Z3Vn4eMF-4Cmpoh38kPo,,,,,
"07/04/2024, 21:19:07",mirco.pl.93@gmail.com,Multi-Task Learning of Object States and State-Modifying Actions from Web Videos,"Tomáš Souček, Jean-Baptiste Alayrac, Antoine Miech, Ivan Laptev, Josef Sivic",TPAMI,2023,Papers,Multiple Egocentric Tasks,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10420504,,https://github.com/soCzech/MultiTaskObjectStates?tab=readme-ov-file,,,
"07/04/2024, 21:26:20",mirco.pl.93@gmail.com,Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos,"Mi Luo, Zihui Xue, Alex Dimakis, Kristen Grauman", ,2024,Papers,From Third-Person to First-Person,,,https://arxiv.org/abs/2403.06351,,,,,
"16/05/2024, 19:31:26",mirco.pl.93@gmail.com,Detours for Navigating Instructional Videos,"Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, Kristen Grauman",CVPR,2024,Papers,Video-Language,,,https://arxiv.org/abs/2401.01823,https://vision.cs.utexas.edu/projects/detours/,,,,
"16/05/2024, 19:40:16",mirco.pl.93@gmail.com,Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects,"Zicong Fan, Takehiko Ohkawa, Linlin Yang, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao", ,2024,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2403.16428,,,,,
"16/05/2024, 19:45:54",mirco.pl.93@gmail.com,ARCTIC,"Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael J. Black, Otmar Hilliges",CVPR,2023,Papers,Datasets,,"A dataset with 2.1 million video frames shows two hands skillfully manipulating objects. It includes precise 3D models of the hands and objects, as well as detailed, dynamic contact information. The dataset features two-handed actions with objects like scissors and laptops, capturing the changing hand positions and object states over time.",https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.pdf,https://arctic.is.tue.mpg.de/,,,,
"17/05/2024, 22:24:03",mirco.pl.93@gmail.com,EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams,"Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik",CVPR,2024,Papers,Multi-Modalities,Event,,https://arxiv.org/abs/2404.08640,https://4dqv.mpi-inf.mpg.de/EventEgo3D/,,,,
"17/05/2024, 22:27:47",mirco.pl.93@gmail.com,SimpleEgo: Predicting Probabilistic Body Pose from Egocentric Cameras,"Hanz Cuevas-Velasquez, Charlie Hewitt, Sadegh Aliakbarian, Tadas Baltrušaitis",3DV,2024,Papers,User Data from an Egocentric Point of View,,,https://arxiv.org/abs/2401.14785,https://microsoft.github.io/SimpleEgo/,,,,
"17/05/2024, 22:35:48",mirco.pl.93@gmail.com,SLVP: Self-Supervised Language-Video Pre-Training for Referring Video Object Segmentation,"Jie Mei, AJ Piergiovanni, Jenq-Neng Hwang, Wei Li",WACVW,2024,Papers,Video-Language,,,https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Mei_SLVP_Self-Supervised_Language-Video_Pre-Training_for_Referring_Video_Object_Segmentation_WACVW_2024_paper.html,,,,,
"17/05/2024, 22:38:54",mirco.pl.93@gmail.com,ActionMixer: Temporal action detection with Optimal Action Segment Assignment and mixers,"Jianhua Yang, Ke Wang, Lijun Zhao, Zhiqiang Jiang, Ruifeng Li",Expert Systems with Applications,2024,Papers,Temporal Segmentation (Action Detection),,,https://www.sciencedirect.com/science/article/abs/pii/S0957417423018328,,,,,
"17/05/2024, 22:44:48",mirco.pl.93@gmail.com,IKEA Ego 3D Dataset,"Yizhak Ben-Shabat, Jonathan Paul, Eviatar Segev, Oren Shrout, Stephen Gould",WACV,2024,Papers,Datasets,,A novel dataset for ego-view 3D point cloud action recognition. The dataset consists of approximately 493k frames and 56 classes of intricate furniture assembly actions of four different furniture types.,https://openaccess.thecvf.com/content/WACV2024/html/Ben-Shabat_IKEA_Ego_3D_Dataset_Understanding_Furniture_Assembly_Actions_From_Ego-View_WACV_2024_paper.html,https://sitzikbs.github.io/IKEAEgo3D.github.io/,,,,
"17/05/2024, 22:56:39",mirco.pl.93@gmail.com,On the Efficacy of Text-Based Input Modalities for Action Anticipation,"Apoorva Beedu, Karan Samel, Irfan Essa", ,2024,Papers,Action Anticipation,Short-Term Action Anticipation,,https://arxiv.org/abs/2401.12972,,,,,
"17/05/2024, 23:03:20",mirco.pl.93@gmail.com,Context in Human Action Through Motion Complementarity,"Eadom Dessalene, Michael Maynord, Cornelia Fermüller, Yiannis Aloimonos",WACV,2024,Papers,Action/Activity Recognition,Action Recognition,,https://openaccess.thecvf.com/content/WACV2024/html/Dessalene_Context_in_Human_Action_Through_Motion_Complementarity_WACV_2024_paper.html,,,,,
"18/05/2024, 17:47:09",mirco.pl.93@gmail.com,EvIs-Kitchen,"Yuzhe Hao, Asako Kanezaki, Ikuro Sato, Rei Kawakami, Koichi Shinoda",IEEE Sensors Journal,2024,Papers,Datasets,,"The EvIs-Kitchen dataset is the first VIdeo-Sensor-Sensor (V-S-S) interaction-focused dataset for ego-HAR tasks, capturing sequences of everyday kitchen activities. This dataset uses two inertial sensors on both wrists to better capture subject-object interactions. ",https://ieeexplore.ieee.org/abstract/document/10387162,,,,,
"18/05/2024, 17:52:53",mirco.pl.93@gmail.com,CaptainCook4D,"Rohith Peddi, Shivvrat Arya, Bharath Challa, Likhitha Pallapothula, Akshay Vyas, Jikai Wang, Qifan Zhang, Vasundhara Komaragiri, Eric Ragan, Nicholas Ruozzi, Yu Xiang, Vibhav Gogate",ICMLW,2023,Papers,Datasets,,"CaptainCook4D, comprising 384 recordings (94.5 hours) of people performing recipes in real kitchen environments. This dataset consists of two distinct types of activity: one in which participants adhere to the provided recipe instructions and another in which they deviate and induce errors. We provide 5.3K step annotations and 10K fine-grained action annotations and benchmark the dataset for the following tasks: supervised error recognition, multistep localization, and procedure learning",https://arxiv.org/abs/2312.14556,https://captaincook4d.github.io/captain-cook/,,,,
"18/05/2024, 18:29:48",mirco.pl.93@gmail.com,Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos,"Junyi Ma, Jingyi Xu, Xieyuanli Chen, Hesheng Wang", ,2024,Papers,Action Anticipation,Trajectory prediction,,https://arxiv.org/abs/2405.04370,,,,,
"18/05/2024, 18:42:59",mirco.pl.93@gmail.com,EMAG: Ego-motion Aware and Generalizable 2D Hand Forecasting from Egocentric Videos,"Masashi Hatano, Ryo Hachiuma, Hideo Saito", ,2024,Papers,Action Anticipation,Trajectory prediction,,https://masashi-hatano.github.io/assets/pdf/emag.pdf,,,,,
"18/05/2024, 18:57:30",mirco.pl.93@gmail.com,TEXT2TASTE: A Versatile Egocentric Vision System for Intelligent Reading Assistance Using Large Language Model,"Wiktor Mucha, Florin Cuconasu, Naome A. Etori, Valia Kalokyri, Giovanni Trappolini",ICCHP,2024,Papers,Asssitive Egocentric Vision,,,https://arxiv.org/abs/2404.09254,,,,,
"18/05/2024, 19:11:48",mirco.pl.93@gmail.com,Sparse multi-view hand-object reconstruction for unseen environments,"Yik Lung Pang, Changjae Oh, Andrea Cavallaro",CVPRW,2024,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2405.01353,,,,,
"18/05/2024, 19:15:28",mirco.pl.93@gmail.com,Learning Object States from Actions via Large Language Models,"Masatoshi Tateno, Takuma Yagi, Ryosuke Furuta, Yoichi Sato", ,2024,Papers,Video-Language,,,https://arxiv.org/abs/2405.01090,,,,,
"18/05/2024, 19:30:13",mirco.pl.93@gmail.com,Text-driven Affordance Learning from Egocentric Vision,"Tomoya Yoshida, Shuhei Kurita, Taichi Nishimura, Shinsuke Mori", ,2024,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2404.02523,,,,,
"18/05/2024, 19:41:13",mirco.pl.93@gmail.com,Step Differences in Instructional Video,"Tushar Nagarajan, Lorenzo Torresani", ,2024,Papers,Video-Language,,,https://arxiv.org/abs/2404.16222,,,,,
"18/05/2024, 19:50:43",mirco.pl.93@gmail.com,Combating Missing Modalities in Egocentric Videos at Test Time,"Merey Ramazanova, Alejandro Pardo, Bernard Ghanem, Motasem Alfarra", ,2024,Papers,Action/Activity Recognition,Test Time Training (Adaptation),,https://arxiv.org/abs/2404.15161,,,,,
"18/05/2024, 19:56:59",mirco.pl.93@gmail.com,OAKINK2,"Xinyu Zhan, Lixin Yang, Yifei Zhao, Kangrui Mao, Hanlin Xu, Zenan Lin, Kailin Li, Cewu Lu",CVPR,2024,Papers,Datasets,," A dataset of bimanual object manipulation tasks for complex daily activities.  OAKINK2 introduces three level of abstraction to organize the manipulation tasks: Affordance, Primitive Task, and Complex Task. OAKINK2 dataset provides multi-view image streams and precise pose annotations for the human body, hands and various interacting objects. This extensive collection supports applications such as interaction reconstruction and motion synthesis.",https://arxiv.org/abs/2403.19417,https://oakink.net/v2/,,,,
"26/05/2024, 20:45:11",mirco.pl.93@gmail.com,"Spatial Cognition from Egocentric Video: Out of Sight, Not Out of Mind","Chiara Plizzari, Shubham Goel, Toby Perrett, Jacob Chalk, Angjoo Kanazawa, Dima Damen", ,2024,Papers,Tracking,,,https://arxiv.org/abs/2404.05072,https://dimadamen.github.io/OSNOM/,,,,
"30/05/2024, 19:59:32",mirco.pl.93@gmail.com,EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views,"Yuhang Yang, Wei Zhai, Chengfeng Wang, Chengjun Yu, Yang Cao, Zheng-Jun Zha", ,2024,Papers,Activity-context,,,https://arxiv.org/abs/2405.13659,,,,,
"30/05/2024, 20:02:49",mirco.pl.93@gmail.com,Rank2Reward: Learning Shaped Reward Functions from Passive Video,"Daniel Yang, Davin Tjia, Jacob Berg, Dima Damen, Pulkit Agrawal, Abhishek Gupta",ICRA,2024,Papers,Human to Robot,,,https://arxiv.org/pdf/2404.14735,https://rank2reward.github.io/,,,,
"30/05/2024, 20:06:51",mirco.pl.93@gmail.com,HOI-Ref,"Siddhant Bansal, Michael Wray, Dima Damen", ,2024,Papers,Datasets,,"It consists of 3.9M question-answer pairs for training and evaluating VLMs. HOI-QA includes questions relating to locating hands, objects, and critically their interactions (e.g. referring to the object being manipulated by the hand). ",https://arxiv.org/abs/2404.09933,https://sid2697.github.io/hoi-ref/,,,,
"30/05/2024, 20:14:56",mirco.pl.93@gmail.com,TIM: A Time Interval Machine for Audio-Visual Action Recognition,"Jacob Chalk, Jaesung Huh, Evangelos Kazakos, Andrew Zisserman, Dima Damen",CVPR,2024,Papers,Multi-Modalities,Audio-Visual,,https://arxiv.org/abs/2404.05559,https://jacobchalk.github.io/TIM-Project/,,,,
"02/06/2024, 18:05:38",mirco.pl.93@gmail.com,EgoNCE++: Do Egocentric Video-Language Models Really Understand Hand-Object Interactions?,"Boshen Xu, Ziheng Wang, Yang Du, Sipeng Zheng, Zhinan Song, Qin Jin", ,2024,Papers,Video-Language,,,https://arxiv.org/abs/2405.17719,,https://github.com/xuboshen/EgoNCEpp,,,
"02/06/2024, 18:21:19",mirco.pl.93@gmail.com,HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data,"Mengqi Zhang, Yang Fu, Zheng Ding, Sifei Liu, Zhuowen Tu, Xiaolong Wang", ,2024,Papers,Diffusion models,,,https://arxiv.org/abs/2403.12011,https://mq-zhang1.github.io/HOIDiffusion/,,,,
"26/06/2024, 08:11:13",mirco.pl.93@gmail.com,Instance Tracking in 3D Scenes from Egocentric Videos,"Yunhan Zhao, Haoyu Ma, Shu Kong, Charless Fowlkes",CVPR,2024,Papers,Tracking,,,https://arxiv.org/abs/2312.04117,,https://github.com/IT3DEgo/IT3DEgo/,,,
"08/07/2024, 07:55:21",mirco.pl.93@gmail.com,HOT3D,"Prithviraj Banerjee, Sindi Shkodrani, Pierre Moulon, Shreyas Hampali, Fan Zhang, Jade Fountain, Edward Miller, Selen Basol, Richard Newcombe, Robert Wang, Jakob Julian Engel, Tomas Hodan", ,2024,Papers,Datasets,,"HOT3D is benchmark dataset for egocentric vision-based understanding of 3D hand-object interactions. The dataset offers over 833 minutes (more than 3.7M images) of multi-view RGB/monochrome image streams showing 19 subjects interacting with 33 diverse rigid objects, multi-modal signals such as eye gaze or scene point clouds, as well as comprehensive ground truth annotations including 3D poses of objects, hands, and cameras, and 3D models of hands and objects.",https://arxiv.org/abs/2406.09598,https://www.projectaria.com/datasets/hot3d/,https://github.com/TimSchoonbeek/IndustReal,,,
"01/09/2024, 19:18:40",mirco.pl.93@gmail.com,ADL4D,"Marsil Zakour, Partha Pratim Nath, Ludwig Lohmer, Emre Faik Gökçe, Martin Piccolrovazzi, Constantin Patsch, Yuankai Wu, Rahul Chaudhari, Eckehard Steinbach", ,2024,Papers,Datasets,,"ADL4D dataset offers a novel perspective on human-object interactions, providing video sequences of everyday activities involving multiple people and objects interacting simultaneously.",https://arxiv.org/abs/2402.17758,,,,,
"01/09/2024, 19:28:46",mirco.pl.93@gmail.com,Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting,"Taeho Kang, Youngki Lee",CVPR,2024,Papers,User Data from an Egocentric Point of View,,,https://arxiv.org/abs/2402.18330,,https://github.com/tho-kn/EgoTAP,,,
"01/09/2024, 19:33:28",mirco.pl.93@gmail.com,HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields,"Haozhe Qi, Chen Zhao, Mathieu Salzmann, Alexander Mathis",CVPR,2024,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://arxiv.org/abs/2402.17062v1,,https://github.com/amathislab/HOISDF,,,
"01/09/2024, 19:40:44",mirco.pl.93@gmail.com,A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval,"Andreea-Maria Oncescu, João F. Henriques, Andrew Zisserman, Samuel Albanie, A. Sophia Koepke",ICASSP,2024,Papers,Video-Language,,,https://arxiv.org/abs/2402.19106,,,,,
"01/09/2024, 19:53:20",mirco.pl.93@gmail.com,SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos,"Changan Chen, Kumar Ashutosh, Rohit Girdhar, David Harwath, Kristen Grauman",CVPR,2024,Papers,Multi-Modalities,Audio-Visual,,https://arxiv.org/abs/2404.05206,https://vision.cs.utexas.edu/projects/soundingactions/,,,,
"03/11/2024, 18:35:09",mirco.pl.93@gmail.com,Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches and a Head-Mounted Camera,"Jiye Lee, Hanbyul Joo",CVPR,2024,Papers,Applications,,,https://openaccess.thecvf.com/content/CVPR2024/html/Lee_Mocap_Everyone_Everywhere_Lightweight_Motion_Capture_With_Smartwatches_and_a_CVPR_2024_paper,,https://github.com/jiyewise/MocapEvery,,,
"03/11/2024, 18:38:38",mirco.pl.93@gmail.com,EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams,"Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik",CVPR,2024,Papers,Multi-Modalities,Event,,https://openaccess.thecvf.com//content/CVPR2024/html/Millerdurai_EventEgo3D_3D_Human_Motion_Capture_from_Egocentric_Event_Streams_CVPR_2024_paper,https://4dqv.mpi-inf.mpg.de/EventEgo3D/,,,,
"03/11/2024, 18:38:40",mirco.pl.93@gmail.com,EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams,"Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik",CVPR,2024,Papers,User Data from an Egocentric Point of View,Event,,https://openaccess.thecvf.com//content/CVPR2024/html/Millerdurai_EventEgo3D_3D_Human_Motion_Capture_from_Egocentric_Event_Streams_CVPR_2024_paper,https://4dqv.mpi-inf.mpg.de/EventEgo3D/,,,,
"03/11/2024, 18:47:18",mirco.pl.93@gmail.com,Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation,"Ruicong Liu, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato",CVPR,2024,Papers,Action/Activity Recognition,Hand-Object Interactions,,https://openaccess.thecvf.com//content/CVPR2024/html/Liu_Single-to-Dual-View_Adaptation_for_Egocentric_3D_Hand_Pose_Estimation_CVPR_2024_paper,,https://github.com/ut-vision/S2DHand,,,
"03/11/2024, 18:49:59",mirco.pl.93@gmail.com,Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting,"Taeho Kang, Youngki Lee",CVPR,2024,Papers,User Data from an Egocentric Point of View,,,https://openaccess.thecvf.com//content/CVPR2024/html/Kang_Attention-Propagation_Network_for_Egocentric_Heatmap_to_3D_Pose_Lifting_CVPR_2024_paper,,https://github.com/tho-kn/EgoTAP,,,
"03/11/2024, 18:57:17",mirco.pl.93@gmail.com,UnrealEgo2-UnrealEgo-RW,"Hiroyasu Akada, Jian Wang, Vladislav Golyanik, Christian Theobalt",CVPR,2024,Papers,Datasets,,"UnrealEgo2 Dataset: An expanded dataset capturing over 15,200 motions of realistic 3D human models with a glasses-based device, offering 1.25 million stereo views and comprehensive joint annotations. UnrealEgo-RW Dataset: A real-world dataset utilizing a compact mobile device with fisheye cameras, designed for versatile egocentric image capture in various environments.",https://openaccess.thecvf.com//content/CVPR2024/html/Akada_3D_Human_Pose_Perception_from_Egocentric_Stereo_Videos_CVPR_2024_paper,https://4dqv.mpi-inf.mpg.de/UnrealEgo2/,https://unrealego.mpi-inf.mpg.de/,,,
"03/11/2024, 19:10:58",mirco.pl.93@gmail.com,TF2023,"Ziwei Zhao, Yuchen Wang, Chuhua Wang",CVPR,2024,Papers,Datasets,,"A novel dataset featuring synchronized first-person and third-person views, including masks of camera wearers linked to their respective views. It consists of 208,794 training and 87,449 testing image pairs, with no actor overlap between sets. Each scene averages 4.29 actors, focusing on complex interactions like puzzle games, enhancing its value for cross-view matching in egocentric vision.",https://openaccess.thecvf.com//content/CVPR2024/html/Zhao_Fusing_Personal_and_Environmental_Cues_for_Identification_and_Segmentation_of_CVPR_2024_paper,,https://github.com/ziweizhao1993/PEN,,,
"03/11/2024, 19:23:46",mirco.pl.93@gmail.com,VideoLLM-online: Online Video Large Language Model for Streaming Video,"Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, Mike Zheng Shou",CVPR,2024,Papers,Applications,,,https://openaccess.thecvf.com//content/CVPR2024/html/Chen_VideoLLM-online_Online_Video_Large_Language_Model_for_Streaming_Video_CVPR_2024_paper,,,https://showlab.github.io/videollm-online/,,
"03/11/2024, 19:33:14",mirco.pl.93@gmail.com,Learning to Segment Referred Objects from Narrated Egocentric Videos,"Yuhan Shen, Huiyu Wang, Xitong Yang, Matt Feiszli, Ehsan Elhamifar, Lorenzo Torresani, Effrosyni Mavroudi;",CVPR,2024,Papers,Segmentation,,,https://openaccess.thecvf.com//content/CVPR2024/html/Shen_Learning_to_Segment_Referred_Objects_from_Narrated_Egocentric_Videos_CVPR_2024_paper,,,,,
"03/11/2024, 23:10:55",mirco.pl.93@gmail.com,Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,"Razvan-George Pasca, Alexey Gavryushin, Muhammad Hamza, Yen-Ling Kuo, Kaichun Mo, Luc Van Gool, Otmar Hilliges, Xi Wang",CVPR,2024,Papers,Action Anticipation,Short-Term Action Anticipation,,https://openaccess.thecvf.com//content/CVPR2024/html/Pasca_Summarize_the_Past_to_Predict_the_Future_Natural_Language_Descriptions_CVPR_2024_paper,,,,,
"03/11/2024, 23:15:32",mirco.pl.93@gmail.com,Grounded Question-Answering in Long Egocentric Videos,"Shangzhe Di, Weidi Xie;",CVPR,2024,Papers,Video-Language,,,https://openaccess.thecvf.com//content/CVPR2024/html/Di_Grounded_Question-Answering_in_Long_Egocentric_Videos_CVPR_2024_paper,,https://github.com/Becomebright/GroundVQA,,,
"03/11/2024, 23:23:32",mirco.pl.93@gmail.com,FACT: Frame-Action Cross-Attention Temporal,"Zijia Lu, Ehsan Elhamifar",CVPR,2024,Papers,Temporal Segmentation (Action Detection),,,https://openaccess.thecvf.com//content/CVPR2024/html/Lu_FACT_Frame-Action_Cross-Attention_Temporal_Modeling_for_Efficient_Action_Segmentation_CVPR_2024_paper,,https://github.com/ZijiaLewisLu/CVPR2024-FACT,,,
"03/11/2024, 23:29:35",mirco.pl.93@gmail.com,Error Detection in Egocentric Procedural Task Videos,"Shih-Po Lee, Zijia Lu, Zekun Zhang, Minh Hoai, Ehsan Elhamifar",CVPR,2024,Papers,Applications,,,https://openaccess.thecvf.com//content/CVPR2024/html/Lee_Error_Detection_in_Egocentric_Procedural_Task_Videos_CVPR_2024_paper,,https://github.com/robert80203/EgoPER_official,,,
"04/11/2024, 01:37:11",mirco.pl.93@gmail.com,EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models,"Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, Yang Liu",CVPR,2024,Papers,Multiple Egocentric Tasks,,,https://openaccess.thecvf.com//content/CVPR2024/html/Cheng_EgoThink_Evaluating_First-Person_Perspective_Thinking_Capability_of_Vision-Language_Models_CVPR_2024_paper,https://adacheng.github.io/EgoThink/,https://github.com/AdaCheng/EgoThink,,,
"04/11/2024, 01:43:10",mirco.pl.93@gmail.com,LoCoNet: Long-Short Context Network for Active Speaker Detection,"Xizi Wang, Feng Cheng, Gedas Bertasius",CVPR,2024,Papers,Social Interactions,,,https://openaccess.thecvf.com//content/CVPR2024/html/Wang_LoCoNet_Long-Short_Context_Network_for_Active_Speaker_Detection_CVPR_2024_paper,,https://github.com/SJTUwxz/LoCoNet_ASD,,,
"04/11/2024, 01:53:10",mirco.pl.93@gmail.com,Video ReCap: Recursive Captioning of Hour-Long Videos,"Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius",CVPR,2024,Papers,Video-Language,,,https://openaccess.thecvf.com//content/CVPR2024/html/Islam_Video_ReCap_Recursive_Captioning_of_Hour-Long_Videos_CVPR_2024_paper,https://sites.google.com/view/vidrecap,,,,
"04/11/2024, 08:36:37",mirco.pl.93@gmail.com,Can't Make an Omelette Without Breaking Some Eggs: Plausible Action Anticipation Using Large Video-Language Models,"Himangi Mittal, Nakul Agarwal, Shao-Yuan Lo, Kwonjoon Lee",CVPR,2024,Papers,Action Anticipation,Short-Term Action Anticipation,,https://openaccess.thecvf.com//content/CVPR2024/html/Mittal_Cant_Make_an_Omelette_Without_Breaking_Some_Eggs_Plausible_Action_CVPR_2024_paper,,,,,
"04/11/2024, 08:41:12",mirco.pl.93@gmail.com,Active Object Detection with Knowledge Aggregation and Distillation from Large Models,"Dejie Yang, Yang Liu",CVPR,2024,Papers,Other EGO-Context,,,https://openaccess.thecvf.com//content/CVPR2024/html/Yang_Active_Object_Detection_with_Knowledge_Aggregation_and_Distillation_from_Large_CVPR_2024_paper,,https://github.com/idejie/KAD,,,
"04/11/2024, 08:45:29",mirco.pl.93@gmail.com,https://openaccess.thecvf.com//content/CVPR2024/html/Shen_Progress-Aware_Online_Action_Segmentation_for_Egocentric_Procedural_Task_Videos_CVPR_2024_paper,"Yuhan Shen, Ehsan Elhamifar",CVPR,2024,Papers,Temporal Segmentation (Action Detection),,,https://openaccess.thecvf.com//content/CVPR2024/html/Shen_Progress-Aware_Online_Action_Segmentation_for_Egocentric_Procedural_Task_Videos_CVPR_2024_paper,,https://github.com/Yuhan-Shen/ProTAS,,,
"04/11/2024, 08:56:12",mirco.pl.93@gmail.com,The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective,"Wenqi Jia, Miao Liu, Hao Jiang, Ishwarya Ananthabhotla, James M. Rehg, Vamsi Krishna Ithapu, Ruohan Gao",CVPR,2024,Papers,From Third-Person to First-Person,,,https://openaccess.thecvf.com//content/CVPR2024/html/Jia_The_Audio-Visual_Conversational_Graph_From_an_Egocentric-Exocentric_Perspective_CVPR_2024_paper,https://vjwq.github.io/AV-CONV/,,,,
"04/11/2024, 08:56:30",mirco.pl.93@gmail.com,The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective,"Wenqi Jia, Miao Liu, Hao Jiang, Ishwarya Ananthabhotla, James M. Rehg, Vamsi Krishna Ithapu, Ruohan Gao",CVPR,2024,Papers,Multi-Modalities,Audio-Visual,,https://openaccess.thecvf.com//content/CVPR2024/html/Jia_The_Audio-Visual_Conversational_Graph_From_an_Egocentric-Exocentric_Perspective_CVPR_2024_paper,https://vjwq.github.io/AV-CONV/,,,,
"04/11/2024, 22:28:58",mirco.pl.93@gmail.com,Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos,"Sagnik Majumder, Ziad Al-Halah, Kristen Grauman",CVPR,2024,Papers,Social Interactions,,,https://openaccess.thecvf.com//content/CVPR2024/html/Majumder_Learning_Spatial_Features_from_Audio-Visual_Correspondence_in_Egocentric_Videos_CVPR_2024_paper,https://vision.cs.utexas.edu/projects/ego_av_corr/,,,,
"04/11/2024, 22:28:59",mirco.pl.93@gmail.com,Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos,"Sagnik Majumder, Ziad Al-Halah, Kristen Grauman",CVPR,2024,Papers,Multi-Modalities,Audio-Visual,,https://openaccess.thecvf.com//content/CVPR2024/html/Majumder_Learning_Spatial_Features_from_Audio-Visual_Correspondence_in_Egocentric_Videos_CVPR_2024_paper,https://vision.cs.utexas.edu/projects/ego_av_corr/,,,,
"04/11/2024, 22:34:50",mirco.pl.93@gmail.com,SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos,"Changan Chen, Kumar Ashutosh, Rohit Girdhar, David Harwath, Kristen Grauman",CVPR,2024,Papers,Multi-Modalities,Audio-Visual,,https://openaccess.thecvf.com//content/CVPR2024/html/Chen_SoundingActions_Learning_How_Actions_Sound_from_Narrated_Egocentric_Videos_CVPR_2024_paper,,,,,
"04/11/2024, 22:43:00",mirco.pl.93@gmail.com,TACO,"Yun Liu, Haolin Yang, Xu Si, Ling Liu, Zipeng Li, Yuxiang Zhang, Yebin Liu, Li Yi",CVPR,2024,Papers,Datasets,,"A large-scale dataset of real-world bimanual tool-object interactions, featuring 131 tool-action-object triplets across 2.5K motion sequences and 5.2M frames with egocentric and 3rd-person views. TACO enables benchmarks in action recognition, hand-object motion forecasting, and grasp synthesis, advancing generalization research in human-object interactions.",https://openaccess.thecvf.com//content/CVPR2024/html/Liu_TACO_Benchmarking_Generalizable_Bimanual_Tool-ACtion-Object_Understanding_CVPR_2024_paper,https://taco2024.github.io/,,,,