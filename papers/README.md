# [Egocentric Vision](https://egocentricvision.github.io/EgocentricVision/)
better view here -> https://egocentricvision.github.io/EgocentricVision/

- [Surveys](#surveys) 
- [Papers](#papers) 
- [Datasets](#datasets) 
- [Challenges](#challenges) 
- [Devices](#devices)




## Surveys

- [Deep-learning Based Egocentric Action Anticipation: A Survey](https://www.researchsquare.com/article/rs-3156532/v1) - Richard Wardle, Sareh Rowlands, Machine Vision and Applications 2023.

- [An Outlook into the Future of Egocentric Vision](https://arxiv.org/abs/2308.07123) - Chiara Plizzari, Gabriele Goletto, Antonino Furnari, Siddhant Bansal, Francesco Ragusa, Giovanni Maria Farinella, Dima Damen, Tatiana Tommasi,  2023.

- [A Survey on Deep Learning Techniques for Action Anticipation](https://arxiv.org/abs/2309.17257) - Zeyun Zhong, Manuel Martin, Michael Voit, Juergen Gall, Jürgen Beyerer,  2023.

- [Egocentric Vision-based Action Recognition: A survey](https://www.sciencedirect.com/science/article/pii/S0925231221017586) - Adrián Núñez-Marcos, Gorka Azkune, Ignacio Arganda-Carreras, Neurocomputing 2021.

- [Predicting the future from first person (egocentric) vision: A survey](https://arxiv.org/abs/2107.13411) - Ivan Rodin, Antonino Furnari, Dimitrios Mavroedis, Giovanni Maria Farinella, CVIU 2021.

- [Analysis of the hands in egocentric vision: A survey](https://arxiv.org/abs/1912.10867) - Andrea Bandini, José Zariffa, TPAMI 2020.

- [A survey of activity recognition in egocentric lifelogging datasets](https://ieeexplore.ieee.org/abstract/document/7934659) - El Asnaoui Khalid, Aksasse Hamid, Aksasse Brahim, Ouanan Mohammed, WITS 2017.

- [Summarization of Egocentric Videos: A Comprehensive Survey](https://ieeexplore.ieee.org/abstract/document/7750564) - Ana Garcia del Molino, Cheston Tan, Joo-Hwee Lim, Ah-Hwee Tan, THMS 2017.

- [Recognition of Activities of Daily Living with Egocentric Vision: A Review](https://www.mdpi.com/1424-8220/16/1/72) - Thi-Hoa-Cuc Nguyen, Jean-Christophe Nebel, Francisco Florez-Revuelta, Sensors 2016.

- [The Evolution of First Person Vision Methods: A Survey](https://arxiv.org/abs/1409.1484) - Alejandro Betancourt, Pietro Morerio, Carlo S. Regazzoni, Matthias Rauterberg, TCSVT 2015.


## Papers

### Action / Activity Recognition
#### Action Recognition
- [Context in Human Action Through Motion Complementarity](https://openaccess.thecvf.com/content/WACV2024/html/Dessalene_Context_in_Human_Action_Through_Motion_Complementarity_WACV_2024_paper.html) - Eadom Dessalene, Michael Maynord, Cornelia Fermüller, Yiannis Aloimonos, WACV 2024.

- [Exploring Missing Modality in Multimodal Egocentric Datasets](https://arxiv.org/abs/2401.11470) - Merey Ramazanova, Alejandro Pardo, Humam Alwassel, Bernard Ghanem,  2024.

- [Semantic-Disentangled Transformer With Noun-Verb Embedding for Compositional Action Recognition](https://ieeexplore.ieee.org/abstract/document/10363109) - Peng Huang,  Rui Yan,  Xiangbo Shu, Zhewei Tu,  Guangzhao Dai, Jinhui Tang, TIP 2023.

- [Slowfast Diversity-aware Prototype Learning for Egocentric Action Recognition](https://dl.acm.org/doi/abs/10.1145/3581783.3612144) - Guangzhao Dai, Xiangbo Shu, Rui Yan, Peng Huang,  Jinhui Tang, MM 2023.

- [Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition](https://arxiv.org/abs/2308.11489) - Qitong Wang, Long Zhao, Liangzhe Yuan, Ting Liu, Xi Peng, ICCV 2023. [[code]](https://github.com/wqtwjt1996/SUM-L)

- [Use Your Head: Improving Long-Tail Video Recognition](https://arxiv.org/abs/2304.01143) - Toby Perrett, Saptarshi Sinha, Tilo Burghardt, Majid Mirmehdi, Dima Damen, CVPR 2023. [[project page]](https://tobyperrett.github.io/lmr/)

- [How Can Objects Help Action Recognition?](https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.html) - Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid, CVPR 2023. [[code]](https://github.com/google-research/scenic)

- [Free-Form Composition Networks for Egocentric Action Recognition](https://arxiv.org/abs/2307.06527) - Haoran Wang, Qinghua Cheng, Baosheng Yu, Yibing Zhan, Dapeng Tao, Liang Ding, Haibin Ling,  2023.

- [Integrating Human Gaze Into Attention for Egocentric Activity Recognition](https://openaccess.thecvf.com/content/WACV2021/html/Min_Integrating_Human_Gaze_Into_Attention_for_Egocentric_Activity_Recognition_WACV_2021_paper.html) - Kyle Min, Jason J. Corso, WACV 2021.

- [Learning to Recognize Actions on Objects in Egocentric Video with Attention Dictionaries](https://ieeexplore.ieee.org/abstract/document/9353268) - Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz, T-PAMI 2021.

- [Interactive Prototype Learning for Egocentric Action Recognition](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Interactive_Prototype_Learning_for_Egocentric_Action_Recognition_ICCV_2021_paper.html) - Xiaohan Wang, Linchao Zhu, Heng Wang, Yi Yang, ICCV 2021.

- [Slow-Fast Auditory Streams For Audio Recognition](https://arxiv.org/abs/2103.03516) - Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen, ICASSP 2021.

- [ACTION-Net: Multipath Excitation for Action Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_ACTION-Net_Multipath_Excitation_for_Action_Recognition_CVPR_2021_paper.pdf) - Zhengwei Wang, Qi She, Aljosa Smolic, CVPR 2021. [[code]](https://github.com/V-Sense/ACTION-Net)

- [Stacked Temporal Attention: Improving First-person Action Recognition by Emphasizing Discriminative Clips](https://arxiv.org/abs/2112.01038) - Lijin Yang, Yifei Huang, Yusuke Sugano, Yoichi Sato, BMVC 2021.

- [With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition](https://arxiv.org/abs/2111.01024) - Evangelos Kazakos, Jaesung Huh, Arsha Nagrani, Andrew Zisserman, Dima Damen, BMVC 2021.

- [Trear: Transformer-based RGB-D Egocentric Action Recognition](https://ieeexplore.ieee.org/abstract/document/9312201?casa_token=VjrXPrZDuSgAAAAA:ezQgxMoeH7q3fxl8su7zg1yghkp60nbxCwU3FxyZEKWghbUVozmKmS_YE99AYceBr3lxA6Ud) - Xiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, Wanqing Li, TCDS 2020.

- [Self-Supervised Joint Encoding of Motion and Appearance for First Person Action Recognition](https://arxiv.org/pdf/2002.03982.pdf) - Mirco Planamente, Andrea Bottino, Barbara Caputo, ICPR 2020.

- [Gate-Shift Networks for Video Action Recognition](https://openaccess.thecvf.com/content_CVPR_2020/html/Sudhakaran_Gate-Shift_Networks_for_Video_Action_Recognition_CVPR_2020_paper.html) - Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz, CVPR 2020. [[code]](https://github.com/swathikirans/GSM)

- [Learning Spatiotemporal Attention for Egocentric Action Recognition](http://openaccess.thecvf.com/content_ICCVW_2019/papers/EPIC/Lu_Learning_Spatiotemporal_Attention_for_Egocentric_Action_Recognition_ICCVW_2019_paper.pdf) - Minlong Lu, Danping Liao, Ze-Nian Li, WICCV 2019.

- [Multitask Learning to Improve Egocentric Action Recognition](https://arxiv.org/abs/1909.06761) - Georgios Kapidis, Ronald Poppe, Elsbeth van Dam, Lucas Noldus, Remco Veltkamp, WICCV 2019.

- [Seeing and Hearing Egocentric Actions: How Much Can We Learn?](https://arxiv.org/abs/1910.06693) - Alejandro Cartas, Jordi Luque, Petia Radeva, Carlos Segura, Mariella Dimiccoli, WICCV 2019.

- [Deep Attention Network for Egocentric Action Recognition](https://ieeexplore.ieee.org/abstract/document/8653357) - Minlong Lu, Simon Fraser, Ze-Nian Li, Yueming Wang, Gang Pan, TIP 2019.

- [EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf) - Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima, ICCV 2019. [[code]](https://github.com/ekazakos/temporal-binding-network)

- [LSTA: Long Short-Term Attention for Egocentric Action Recognition](https://openaccess.thecvf.com/content_CVPR_2019/papers/Sudhakaran_LSTA_Long_Short-Term_Attention_for_Egocentric_Action_Recognition_CVPR_2019_paper.pdf) - Sudhakaran, Swathikiran and Escalera, Sergio and Lanz, Oswald, CVPR 2019. [[code]](https://github.com/swathikirans/LSTA)

- [Long-Term Feature Banks for Detailed Video Understanding](https://arxiv.org/abs/1812.05038) - Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krähenbühl, Ross Girshick, CVPR 2019.

- [In the eye of beholder: Joint learning of gaze and actions in first person video](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yin_Li_In_the_Eye_ECCV_2018_paper.pdf) - Li, Y., Liu, M., & Rehg, J. M., ECCV 2018.

- [Egocentric Activity Recognition on a Budget](https://openaccess.thecvf.com/content_cvpr_2018/papers/Possas_Egocentric_Activity_Recognition_CVPR_2018_paper.pdf) - Possas, Rafael and Caceres, Sheila Pinto and Ramos, Fabio, CVPR 2018.

- [Attention is All We Need: Nailing Down Object-centric Attention for Egocentric Activity Recognition](https://arxiv.org/abs/1807.11794) - Swathikiran Sudhakaran, Oswald Lanz, BMVC 2018.

- [Trajectory Aligned Features For First Person Action Recognition](http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/JournalPublications/2016/Suriya_2016_Trajectory_Features.pdf) - S. Singh, C. Arora, and C.V. Jawahar, Pattern Recognition 2017.

- [Action recognition in RGB-D egocentric videos](https://ieeexplore.ieee.org/document/8296915) - Yansong Tang, Yi Tian, Jiwen Lu, Jianjiang Feng, Jie Zhou, ICIP 2017.

- [Egocentric Gesture Recognition Using Recurrent 3D Convolutional Neural Networks with Spatiotemporal Transformer Modules](https://openaccess.thecvf.com/content_ICCV_2017/papers/Cao_Egocentric_Gesture_Recognition_ICCV_2017_paper.pdf) - Cao, Congqi and Zhang, Yifan and Wu, Yi and Lu, Hanqing and Cheng, Jian, ICCV 2017.

- [Modeling Sub-Event Dynamics in First-Person Action Recognition](https://openaccess.thecvf.com/content_cvpr_2017/html/Zaki_Modeling_Sub-Event_Dynamics_CVPR_2017_paper.html) - Hasan F. M. Zaki, Faisal Shafait, Ajmal Mian, CVPR 2017.

- [First Person Action Recognition Using Deep Learned Descriptors](https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S12-15.pdf) - S. Singh, C. Arora, and C.V. Jawahar, CVPR 2016. [[code]](https://github.com/suriyasingh/EgoConvNet)

- [Generating Notifications for Missing Actions: Don't forget to turn the lights off!](https://homes.cs.washington.edu/~ali/alarm-iccv.pdf) - Soran, Bilge, Ali Farhadi, and Linda Shapiro, ICCV 2015.

- [Delving into egocentric actions](https://openaccess.thecvf.com/content_cvpr_2015/papers/Li_Delving_Into_Egocentric_2015_CVPR_paper.pdf) - Li, Y., Ye, Z., & Rehg, J. M., CVPR 2015.

- [Pooled Motion Features for First-Person Videos](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ryoo_Pooled_Motion_Features_2015_CVPR_paper.pdf) - Michael S. Ryoo, Brandon Rothrock and Larry H. Matthies, CVPR 2015.

- [First-Person Activity Recognition: What Are They Doing to Me?](http://cvrc.ece.utexas.edu/mryoo/papers/cvpr2013_ryoo.pdf) - M. S. Ryoo and L. Matthies, CVPR 2013.

- [Learning to recognize daily actions using gaze](http://ai.stanford.edu/~alireza/publication/ECCV12.pdf) - Fathi, A., Li, Y., & Rehg, J. M, ECCV 2012.

- [Detecting activities of daily living in first-person camera views](https://www.cs.cmu.edu/~deva/papers/ADL_2012.pdf) - Pirsiavash, H., & Ramanan, D., CVPR 2012.

#### Hand-Object Interactions
- [Egocentric Action Recognition by Capturing Hand-Object Contact and Object State](https://openaccess.thecvf.com/content/WACV2024/papers/Shiota_Egocentric_Action_Recognition_by_Capturing_Hand-Object_Contact_and_Object_State_WACV_2024_paper.pdf) - Tsukasa Shiota, Motohiro Takagi, Kaori Kumagai, Hitoshi Seshimo, Yushi Aono, WACV 2024.

- [Sparse multi-view hand-object reconstruction for unseen environments](https://arxiv.org/abs/2405.01353) - Yik Lung Pang, Changjae Oh, Andrea Cavallaro, CVPRW 2024.

- [Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation](https://arxiv.org/abs/2403.04381) - Ruicong Liu, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato, CVPR 2024. [[code]](https://github.com/ut-vision/S2DHand)

- [HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields](https://arxiv.org/abs/2402.17062v1) - Haozhe Qi, Chen Zhao, Mathieu Salzmann, Alexander Mathis, CVPR 2024. [[code]](https://github.com/amathislab/HOISDF)

- [Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation](https://openaccess.thecvf.com//content/CVPR2024/html/Liu_Single-to-Dual-View_Adaptation_for_Egocentric_3D_Hand_Pose_Estimation_CVPR_2024_paper) - Ruicong Liu, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato, CVPR 2024. [[code]](https://github.com/ut-vision/S2DHand)

- [Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects](https://arxiv.org/abs/2403.16428) - Zicong Fan, Takehiko Ohkawa, Linlin Yang, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao,  2024.

- [Text-driven Affordance Learning from Egocentric Vision](https://arxiv.org/abs/2404.02523) - Tomoya Yoshida, Shuhei Kurita, Taichi Nishimura, Shinsuke Mori,  2024.

- [Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos](https://openaccess.thecvf.com/content/WACV2023/papers/Yu_Fine-Grained_Affordance_Annotation_for_Egocentric_Hand-Object_Interaction_Videos_WACV_2023_paper.pdf) - Zecheng Yu, Yifei Huang, Ryosuke Furuta, Takuma Yagi, Yusuke Goutsu, Yoichi Sato, WACV 2023. [[project page]](https://github.com/zch-yu/epic-affordance-annotation)

- [InterTracker: Discovering and Tracking General Objects Interacting with Hands in the Wild](https://arxiv.org/abs/2308.03061) - Yanyan Shao, Qi Ye, Wenhan Luo, Kaihao Zhang, Jiming Chen, IROS 2023.

- [Hands, Objects, Action! Egocentric 2D Hand-Based Action Recognition](https://link.springer.com/chapter/10.1007/978-3-031-44137-0_3) - Wiktor Mucha, Martin Kampel, ICVS 2023.

- [Improved Deep Learning-Based Efficientpose Algorithm for Egocentric Marker-Less Tool and Hand Pose Estimation in Manual Assembly](https://link.springer.com/chapter/10.1007/978-981-99-4761-4_25) - Zihan Niu, Yi Xia, Jun Zhang, Bing Wang, Peng Chen, ICIC 2023.

- [Helping Hands: An Object-Aware Ego-Centric Video Recognition Model](https://arxiv.org/abs/2308.07918) - Chuhan Zhang, Ankush Gupta, Andrew Zisserman, ICCV 2023.

- [Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images](https://arxiv.org/abs/2308.11015) - Tze Ho Elden Tse, Franziska Mueller, Zhengyang Shen, Danhang Tang, Thabo Beeler, Mingsong Dou, Yinda Zhang, Sasa Petrovic, Hyung Jin Chang, Jonathan Taylor, Bardia Doosti, ICCV 2023.

- [EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding](https://arxiv.org/abs/2309.02423) - Yue Xu, Yong-Lu Li, Zhemin Huang, Michael Xu Liu, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang, ICCV 2023. [[project page]](https://mvig-rhos.com/ego_pca)

- [HiFiHR: Enhancing 3D Hand Reconstruction from a Single Image via High-Fidelity Texture](https://arxiv.org/abs/2308.13628) - Jiayin Zhu, Zhuoran Zhao, Linlin Yang, Angela Yao, DAGM 2023. [[code]](https://github.com/viridityzhu/HiFiHR)

- [Functional Hand Type Prior for 3D Hand Pose Estimation and Action Recognition from Egocentric View Monocular Videos](https://papers.bmvc2023.org/0193.pdf) - Wonseok Roh, Seung Hyun Lee, Won Jeong Ryoo, Jakyung Lee, Gyeongrok Oh, Sooyeon Hwang, Hyung-gun Chi, Sangpil Kim, BMVC 2023.

- [CaSAR: Contact-aware Skeletal Action Recognition](https://arxiv.org/abs/2309.10001) - Junan Lin, Zhichao Sun, Enjie Cao, Taein Kwon, Mahdi Rad, Marc Pollefeys,  2023.

- [MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians](https://arxiv.org/abs/2312.02137) - Chandradeep Pokhariya, Ishaan N Shah, Angela Xing, Zekun Li, Kefan Chen, Avinash Sharma, Srinath Sridhar,  2023. [[project page]](https://ivl.cs.brown.edu/research/manus.html)

- [3D Hand Pose Estimation in Egocentric Images in the Wild](https://arxiv.org/abs/2312.06583) - Aditya Prakash, Ruisen Tu, Matthew Chang, Saurabh Gupta,  2023. [[project page]](https://ap229997.github.io/projects/hands/)

- [Get a Grip: Reconstructing Hand-Object Stable Grasps in Egocentric Videos](https://arxiv.org/abs/2312.15719) - Zhifan Zhu, Dima Damen,  2023. [[project page]](https://zhifanzhu.github.io/getagrip/)

- [MACS: Mass Conditioned 3D Hand and Object Motion Synthesis](https://arxiv.org/pdf/2312.14929.pdf) - Soshi Shimada, Franziska Mueller, Jan Bednarik, Bardia Doosti, Bernd Bickel, Danhang Tang, Vladislav Golyanik, Jonathan Taylor, Christian Theobalt, Thabo Beeler,  2023. [[project page]](https://vcai.mpi-inf.mpg.de/projects/MACS/)

- [Egocentric Human-Object Interaction Detection Exploiting Synthetic Data](https://arxiv.org/abs/2204.07061) - Rosario Leonardi, Francesco Ragusa, Antonino Furnari, Giovanni Maria Farinella, ICIAP 2022.

- [SOS! Self-supervised Learning Over Sets Of Handled Objects In Egocentric Action Recognition](https://arxiv.org/abs/2204.04796) - Victor Escorcia, Ricardo Guerrero, Xiatian Zhu, Brais Martinez, ECCV 2022.

- [Is First Person Vision Challenging for Object Tracking?](https://arxiv.org/abs/2011.12263) - Matteo Dunnhofer, Antonino Furnari, Giovanni Maria Farinella, Christian Micheloni, WICCV 2021.

- [Real Time Egocentric Object Segmentation: THU-READ Labeling and Benchmarking Results](https://arxiv.org/abs/2106.04957) - E. Gonzalez-Sosa, G. Robledo, D. Gonzalez-Morin, P. Perez-Garcia, A. Villegas, WCVPR 2021.

- [The MECCANO Dataset: Understanding Human-Object Interactions from Egocentric Videos in an Industrial-like Domain](https://arxiv.org/abs/2010.05654) - Francesco Ragusa, Antonino Furnari, Salvatore Livatino, Giovanni Maria Farinella, WACV 2021.

- [Learning Visual Affordance Grounding from Demonstration Videos](https://arxiv.org/abs/2108.05675) - Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao,  2021.

- [Domain and View-point Agnostic Hand Action Recognition](https://arxiv.org/abs/2103.02303) - Alberto Sabater, Iñigo Alonso, Luis Montesano, Ana C. Murillo,  2021.

- [Understanding Egocentric Hand-Object Interactions from Hand Estimation](https://arxiv.org/abs/2109.14657) - Yao Lu, Walterio W. Mayol-Cuevas,  2021.

- [Egocentric Hand-object Interaction Detection and Application](https://arxiv.org/abs/2109.14734) - Yao Lu, Walterio W. Mayol-Cuevas,  2021.

- [Hand-Priming in Object Localization for Assistive Egocentric Vision](https://openaccess.thecvf.com/content_WACV_2020/papers/Lee_Hand-Priming_in_Object_Localization_for_Assistive_Egocentric_Vision_WACV_2020_paper.pdf) - Lee, Kyungjun and Shrivastava, Abhinav and Kacorri, Hernisa, WACV 2020.

- [Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460681.pdf) - Miao Liu, Siyu Tang, Yin Li, James M. Rehg, ECCV 2020.

- [Understanding Human Hands in Contact at Internet Scale](https://openaccess.thecvf.com/content_CVPR_2020/html/Shan_Understanding_Human_Hands_in_Contact_at_Internet_Scale_CVPR_2020_paper.html) - Dandan Shan, Jiaqi Geng, Michelle Shu, David F. Fouhey, CVPR 2020.

- [Generalizing Hand Segmentation in Egocentric Videos with Uncertainty-Guided Model Adaptation](https://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_Generalizing_Hand_Segmentation_in_Egocentric_Videos_With_Uncertainty-Guided_Model_Adaptation_CVPR_2020_paper.pdf) - Minjie Cai and Feng Lu and Yoichi Sato, CVPR 2020. [[code]](https://github.com/cai-mj/UMA)

- [Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild](https://openaccess.thecvf.com/content_CVPR_2020/papers/Kulon_Weakly-Supervised_Mesh-Convolutional_Hand_Reconstruction_in_the_Wild_CVPR_2020_paper.pdf) - Dominik Kulon, Riza Alp Güler, Iasonas Kokkinos, Michael Bronstein, Stefanos Zafeiriou, CVPR 2020.

- [Learning joint reconstruction of hands and manipulated objects](https://openaccess.thecvf.com/content_CVPR_2019/papers/Hasson_Learning_Joint_Reconstruction_of_Hands_and_Manipulated_Objects_CVPR_2019_paper.pdf) - Yana Hasson, Gül Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J. Black, Ivan Laptev, Cordelia Schmid, CVPR 2020.

- [H+O: Unified Egocentric Recognition of 3D Hand-Object Poses and Interactions](https://openaccess.thecvf.com/content_CVPR_2019/papers/Tekin_HO_Unified_Egocentric_Recognition_of_3D_Hand-Object_Poses_and_Interactions_CVPR_2019_paper.pdf) - Tekin, Bugra and Bogo, Federica and Pollefeys, Marc, CVPR 2019. [[video]](https://youtu.be/ko6kNZ9DuAk?t=3240)

- [Understanding Hand-Object Manipulation with Grasp Types and Object Attributes](http://www.cs.cmu.edu/~kkitani/pdf/CKY-RSS16.pdf) - Minjie Cai and Kris M. Kitani and Yoichi Sato, Robotics: Science and Systems 2018.

- [From Lifestyle VLOGs to Everyday Interaction](https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0733.pdf) - David F. Fouhey and Weicheng Kuo and Alexei A. Efros and Jitendra Malik, CVPR 2018.

- [Analysis of Hand Segmentation in the Wild](https://arxiv.org/pdf/1803.03317) - Aisha Urooj, Ali Borj, CVPR 2018.

- [First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations](https://openaccess.thecvf.com/content_cvpr_2018/papers/Garcia-Hernando_First-Person_Hand_Action_CVPR_2018_paper.pdf) - Garcia-Hernando, Guillermo and Yuan, Shanxin and Baek, Seungryul and Kim, Tae-Kyun, CVPR 2018. [[code]](https://github.com/guiggh/hand_pose_action)

- [Jointly Recognizing Object Fluents and Tasks in Egocentric Videos](https://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Jointly_Recognizing_Object_ICCV_2017_paper.pdf) - Liu, Yang and Wei, Ping and Zhu, Song-Chun, ICCV 2017.

- [Egocentric Gesture Recognition Using Recurrent 3D Convolutional Neural Networks with Spatiotemporal Transformer Modules](https://openaccess.thecvf.com/content_ICCV_2017/papers/Cao_Egocentric_Gesture_Recognition_ICCV_2017_paper.pdf) - Cao, Congqi and Zhang, Yifan and Wu, Yi and Lu, Hanqing and Cheng, Jian, ICCV 2017.

- [First Person Action-Object Detection with EgoNet](https://arxiv.org/abs/1603.04908) - Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi,  2017.

- [Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions](http://homes.sice.indiana.edu/sbambach/papers/iccv-egohands.pdf) - Bambach, S., Lee, S., Crandall, D. J., & Yu, C., ICCV 2015.

- [Understanding Everyday Hands in Action From RGB-D Images](https://openaccess.thecvf.com/content_iccv_2015/html/Rogez_Understanding_Everyday_Hands_ICCV_2015_paper.html) - Gregory Rogez, James S. Supancic III, Deva Ramanan, ICCV 2015.

- [3D Hand Pose Detection in Egocentric RGB-D Images](https://link.springer.com/chapter/10.1007/978-3-319-16178-5_25) - Grégory Rogez, Maryam Khademi, J. S. Supančič III, J. M. M. Montiel, Deva Ramanan, WECCV 2014.

- [Detecting Snap Points in Egocentric Video with a Web Photo Prior](https://www.cs.utexas.edu/~grauman/papers/bo-eccv2014.pdf) - Bo Xiong and Kristen Grauman, ECCV 2014. [[code]](http://vision.cs.utexas.edu/projects/ego_snappoints/#code)

- [You-Do, I-Learn: Discovering Task Relevant Objects and their Modes of Interaction from Multi-User Egocentric Video](http://www.bmva.org/bmvc/2014/files/paper059.pdf) - Dima Damen, Teesid Leelasawassuk, Osian Haines, Andrew Calway, and Walterio Mayol-Cuevas, BMVC 2014.

- [Pixel-level hand detection in ego-centric videos](https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Pixel-Level_Hand_Detection_2013_CVPR_paper.pdf) - Li, Cheng, Kris M. Kitani, CVPR 2013. [[code]](https://github.com/irllabs/handtrack) [[video]](https://youtu.be/N756YmLpZyY)

- [Learning to recognize objects in egocentric activities](https://homes.cs.washington.edu/~xren/publication/fathi_cvpr11_egocentric_objects.pdf) - Fathi, A., Ren, X., & Rehg, J. M., CVPR 2011.

- [Context-based vision system for place and object recognition](https://www.cs.ubc.ca/~murphyk/Papers/iccv03.pdf) - Torralba, A., Murphy, K. P., Freeman, W. T., & Rubin, M. A., ICCV 2003.

#### Usupervised Domain Adaptation
- [Relative Norm Alignment for Tackling Domain Shift in Deep Multi-modal Classification](https://link.springer.com/article/10.1007/s11263-024-01998-9) - Mirco Planamente, Chiara Plizzari, Simone Alberto Peirone, Barbara Caputo, Andrea Bottino, IJCV 2024.

- [Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective](https://arxiv.org/abs/2208.07365) - Pengfei Wei, Lingdong Kong, Xinghua Qu, Yi Ren, Zhiqiang Xu, Jing Jiang, Xiang Yin, NeurIPS 2023. [[code]](https://github.com/ldkong1205/TranSVAE)

- [Object-based (yet Class-agnostic) Video Domain Adaptation](https://arxiv.org/abs/2311.17942) - Dantong Niu, Amir Bar, Roei Herzig, Trevor Darrell, Anna Rohrbach,  2023.

- [Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition](https://arxiv.org/abs/2110.10101) - Mirco Planamente, Chiara Plizzari, Emanuele Alberti, Barbara Caputo, WACV 2022.

- [Interact before Align: Leveraging Cross-Modal Knowledge for Domain Adaptive Action Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Interact_Before_Align_Leveraging_Cross-Modal_Knowledge_for_Domain_Adaptive_Action_CVPR_2022_paper.pdf) - Lijin Yang, Yifei Huang, Yusuke Sugano, Yoichi Sato, CVPR 2022.

- [Contrast and Mix: Temporal Contrastive Video Domain Adaptation with Background Mixing](https://openreview.net/forum?id=a1wQOh27zcy) - Aadarsh Sahoo, Rutav Shah, Rameswar Panda, Kate Saenko, Abir Das, NIPS 2021.

- [Differentiated Learning for Multi-Modal Domain Adaptation](https://dl.acm.org/doi/pdf/10.1145/3474085.3475660?casa_token=wOh7PYXIrGoAAAAA:WBP-sajm70r9KKNqNcwM7RIMW9D_re7MC56V10yq3_GCh4JafS_JegifZJ8--87l5TEcucuGaTYM) - Jianming Lv, Kaijie Liu, Shengfeng He, MM 2021.

- [Learning Cross-modal Contrastive Features for Video Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Learning_Cross-Modal_Contrastive_Features_for_Video_Domain_Adaptation_ICCV_2021_paper.pdf) - Donghyun Kim, Yi-Hsuan Tsai, Bingbing Zhuang, Xiang Yu, Stan Sclaroff, Kate Saenko, Manmohan Chandraker, ICCV 2021.

- [Spatio-temporal Contrastive Domain Adaptation for Action Recognition](https://openaccess.thecvf.com/content/CVPR2021/html/Song_Spatio-temporal_Contrastive_Domain_Adaptation_for_Action_Recognition_CVPR_2021_paper.html) - Xiaolin Song, Sicheng Zhao, Jingyu Yang, Huanjing Yue, Pengfei Xu, Runbo Hu, Hua Chai, CVPR 2021.

- [Domain Adaptation in Multi-View Embedding for Cross-Modal Video Retrieval](https://arxiv.org/abs/2110.12812) - Jonathan Munro, Michael Wray, Diane Larlus, Gabriela Csurka, Dima Damen,  2021.

- [Multi-Modal Domain Adaptation for Fine-Grained Action Recognition](https://openaccess.thecvf.com/content_CVPR_2020/html/Munro_Multi-Modal_Domain_Adaptation_for_Fine-Grained_Action_Recognition_CVPR_2020_paper.html) - Jonathan Munro, Dima Damen, CVPR 2020.

#### Domain Generalization
- [X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization](https://arxiv.org/abs/2403.19811) - Anna Kukleva, Fadime Sener, Edoardo Remelli, Bugra Tekin, Eric Sauser, Bernt Schiele, Shugao Ma, CVPR 2024. [[code]](https://github.com/annusha/xmic)

- [MMG-Ego4D: Multimodal Generalization in Egocentric Action Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Gong_MMG-Ego4D_Multimodal_Generalization_in_Egocentric_Action_Recognition_CVPR_2023_paper.pdf) - Xinyu Gong, Sreyas Mohan, Naina Dhingra, Jean-Charles Bazin, Yilei Li, Zhangyang Wang, Rakesh Ranjan, CVPR 2023.

- [Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition](https://arxiv.org/abs/2110.10101) - Mirco Planamente, Chiara Plizzari, Emanuele Alberti, Barbara Caputo, WACV 2022.

#### Source Free Domain Adaptation
#### Test Time Training (Adaptation)
- [Combating Missing Modalities in Egocentric Videos at Test Time](https://arxiv.org/abs/2404.15161) - Merey Ramazanova, Alejandro Pardo, Bernard Ghanem, Motasem Alfarra,  2024.

- [Test-time adaptation for egocentric action recognition](https://link.springer.com/chapter/10.1007/978-3-031-06433-3_18) - Mirco Plananamente, Chiara Plizzari, Barbara Caputo, ICIAP 2022. [[code]](https://github.com/EgocentricVision/RNA-TTA)

#### Zero-Shot Learning
- [GPT4Ego: Unleashing the Potential of Pre-trained Models for Zero-Shot Egocentric Action Recognition](https://arxiv.org/abs/2401.10039) - Guangzhao Dai, Xiangbo Shu, Wenhao Wu,  2024.

- [Opening the Vocabulary of Egocentric Actions](https://arxiv.org/abs/2308.11488) - Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao,  2023. [[project page]](https://dibschat.github.io/openvocab-egoAR/)

### Action Anticipation
#### Short-Term Action Anticipation
- [Leveraging Next-Active Objects for Context-Aware Anticipation in Egocentric Videos](https://arxiv.org/pdf/2308.08303.pdf) - Sanket Thakur, Cigdem Beyan, Pietro Morerio, Vittorio Murino, Alessio Del Bue, WACV 2024.

- [Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation](https://openaccess.thecvf.com//content/CVPR2024/html/Pasca_Summarize_the_Past_to_Predict_the_Future_Natural_Language_Descriptions_CVPR_2024_paper) - Razvan-George Pasca, Alexey Gavryushin, Muhammad Hamza, Yen-Ling Kuo, Kaichun Mo, Luc Van Gool, Otmar Hilliges, Xi Wang, CVPR 2024.

- [Can't Make an Omelette Without Breaking Some Eggs: Plausible Action Anticipation Using Large Video-Language Models](https://openaccess.thecvf.com//content/CVPR2024/html/Mittal_Cant_Make_an_Omelette_Without_Breaking_Some_Eggs_Plausible_Action_CVPR_2024_paper) - Himangi Mittal, Nakul Agarwal, Shao-Yuan Lo, Kwonjoon Lee, CVPR 2024.

- [On the Efficacy of Text-Based Input Modalities for Action Anticipation](https://arxiv.org/abs/2401.12972) - Apoorva Beedu, Karan Samel, Irfan Essa,  2024.

- [SHARE ON Towards Egocentric Compositional Action Anticipation with Adaptive Semantic Debiasing](https://dl.acm.org/doi/pdf/10.1145/3633333) - Tianyu Zhang  ,  Weiqing Min, Tao Liu, Shuqiang Jiang, Yong Rui, TOMM 2023.

- [Dynamic Context Removal: A General Training Strategy for Robust Models on Video Action Predictive Tasks](https://link.springer.com/article/10.1007/s11263-023-01850-6) - Xinyu Xu, Yong-Lu Li, Cewu Lu, IJCV 2023.

- [Enhancing Next Active Object-based Egocentric Action Anticipation with Guided Attention](https://arxiv.org/abs/2305.12953) - Sanket Thakur, Cigdem Beyan, Pietro Morerio, Vittorio Murino, Alessio Del Bue, ICIP 2023. [[project page]](https://sanketsans.github.io/guided-attention-egocentric.html)

- [Guided Attention for Next Active Object @ EGO4D STA Challenge](https://arxiv.org/abs/2305.16066) - Sanket Thakur, Cigdem Beyan, Pietro Morerio, Vittorio Murino, Alessio Del Bue, CVPRW 2023.

- [The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Stergiou_The_Wisdom_of_Crowds_Temporal_Progressive_Attention_for_Early_Action_CVPR_2023_paper.pdf) - Alexandros Stergiou, Dima Damen, CVPR 2023. [[project page]](https://alexandrosstergiou.github.io/project_pages/TemPr/index.html)

- [Streaming egocentric action anticipation: An evaluation scheme and approach](https://arxiv.org/pdf/2306.16682.pdf) - Antonino Furnari, Giovanni Maria Farinella, CVIU 2023.

- [Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction](https://arxiv.org/abs/2301.09209) - Razvan-George Pasca, Alexey Gavryushin, Yen-Ling Kuo, Luc Van Gool, Otmar Hilliges, Xi Wang,  2023. [[project page]](https://eth-ait.github.io/transfusion-proj/)

- [VS-TransGRU: A Novel Transformer-GRU-based Framework Enhanced by Visual-Semantic Fusion for Egocentric Action Anticipation](https://arxiv.org/abs/2307.03918) - Congqi Cao, Ze Sun, Qinyi Lv, Lingtong Min, Yanning Zhang,  2023.

- [Action Scene Graphs for Long-Form Understanding of Egocentric Videos](https://arxiv.org/abs/2312.03391) - Ivan Rodin, Antonino Furnari, Kyle Min, Subarna Tripathi, Giovanni Maria Farinella,  2023. [[code]](https://github.com/fpv-iplab/easg)

- [DiffAnt: Diffusion Models for Action Anticipation](https://arxiv.org/abs/2311.15991) - Zeyun Zhong, Chengzhi Wu, Manuel Martin, Michael Voit, Juergen Gall, Jürgen Beyerer,  2023.

- [Early Action Recognition with Action Prototypes](https://arxiv.org/abs/2312.06598) - Guglielmo Camporese, Alessandro Bergamo, Xunyu Lin, Joseph Tighe, Davide Modolo,  2023.

- [A Hybrid Egocentric Activity Anticipation Framework via Memory-Augmented Recurrent and One-Shot Representation Forecasting](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_Hybrid_Egocentric_Activity_Anticipation_Framework_via_Memory-Augmented_Recurrent_and_CVPR_2022_paper.pdf) - Tianshan Liu, Kin-Man Lam, CVPR 2022.

- [Towards Streaming Egocentric Action Anticipation](https://arxiv.org/abs/2110.05386) - Antonino Furnari, Giovanni Maria Farinella, arXiv 2021.

- [Action Anticipation Using Pairwise Human-Object Interactions and Transformers](https://ieeexplore.ieee.org/abstract/document/9546623?casa_token=S642phYzKBsAAAAA:N3U0R4Hj7qiWztApTVHFJitkK8zFux5RTqTzjE6fRaT8luL4gVW-l-Fzoqd-K4u0x0bHvGgpjPI) - Debaditya Roy; Basura Fernando, TIP 2021.

- [Self-Regulated Learning for Egocentric Video Activity Anticipation](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34) - Zhaobo Qi; Shuhui Wang; Chi Su; Li Su; Qingming Huang; Qi Tian, T-PAMI 2021.

- [What If We Could Not See? Counterfactual Analysis for Egocentric Action Anticipation](ijcai.org/proceedings/2021/182) - T Zhang, W Min, J Yang, T Liu, S Jiang, Y Rui, IJCAI 2021.

- [What If We Could Not See? Counterfactual Analysis for Egocentric Action Anticipation](https://www.ijcai.org/proceedings/2021/0182.pdf) - Tianyu Zhang, Weiqing Min, Jiahao Yang, Tao Liu, Shuqiang Jiang, Yong Rui, IJCAI 2021.

- [Knowledge Distillation for Human Action Anticipation](https://ieeexplore.ieee.org/abstract/document/9506693?casa_token=BecLRxlT_acAAAAA:AT3srfvaOxi3pcMZ5jNhPziC75sADNRNhHLRXYAUWAe9yo5pUNlKIJvDw1a4Geg087evZvEkEA) - Vinh Tran Stony Brook University, Stony Brook NY, Yang Wang,  Zekun Zhang, Minh Hoai, ICIP 2021.

- [Anticipative Video Transformer](https://arxiv.org/abs/2106.02036) - Rohit Girdhar, Kristen Grauman, ICCV 2021.

- [Multi-Modal Temporal Convolutional Network for Anticipating Actions in Egocentric Videos](https://openaccess.thecvf.com/content/CVPR2021W/Precognition/html/Zatsarynna_Multi-Modal_Temporal_Convolutional_Network_for_Anticipating_Actions_in_Egocentric_Videos_CVPRW_2021_paper.html) - Olga Zatsarynna, Yazan Abu Farha, Juergen Gall, CVPRW 2021.

- [Anticipating Human Actions by Correlating Past With the Future With Jaccard Similarity Measures](https://openaccess.thecvf.com/content/CVPR2021/html/Fernando_Anticipating_Human_Actions_by_Correlating_Past_With_the_Future_With_CVPR_2021_paper.html) - Basura Fernando, Samitha Herath, CVPR 2021.

- [Higher Order Recurrent Space-Time Transformer for Video Action Prediction](https://arxiv.org/abs/2104.08665) - Tsung-Ming Tai, Giuseppe Fiameni, Cheng-Kuang Lee, Oswald Lanz, ArXiv 2021.

- [Multimodal Global Relation Knowledge Distillation for Egocentric Action Anticipation](https://dl.acm.org/doi/abs/10.1145/3474085.3475327?casa_token=S_eM0ZcL9G8AAAAA:n9-Xa3-WzAD5NGx9h9WA7ZnBFW5Xzv-QYu-wWYtUaqpYAagALI37qL1rc3WWawgiNf_0VrtOWX0) - Y Huang, X Yang, C Xu, ACM 2021.

- [Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video](https://arxiv.org/abs/2005.02190) - Antonino Furnari, Giovanni Maria Farinella, T-PAMI 2020.

- [Knowledge Distillation for Action Anticipation via Label Smoothing](https://arxiv.org/abs/2004.07711) - Guglielmo Camporese, Pasquale Coscia, Antonino Furnari, Giovanni Maria Farinella, Lamberto Ballan, ICPR 2020.

- [Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460681.pdf) - Miao Liu, Siyu Tang, Yin Li, James M. Rehg, ECCV 2020.

- [An Egocentric Action Anticipation Framework via Fusing Intuition and Analysis](https://dl.acm.org/doi/10.1145/3394171.3413964) - Tianyu Zhang, Weiqing Min, Ying Zhu, Yong Rui, Shuqiang Jiang, ACM 2020.

- [What Would You Expect? Anticipating Egocentric Actions with Rolling-Unrolling LSTMs and Modality Attention](https://arxiv.org/pdf/1905.09035) - Antonino Furnari, Giovanni Maria Farinella, ICCV 2019. [[code]](https://github.com/fpv-iplab/rulstm)

- [Zero-Shot Anticipation for Instructional Activities](https://ieeexplore.ieee.org/document/9008304) - Fadime Sener, Angela Yao, ICCV 2019.

- [Leveraging the Present to Anticipate the Future in Videos](https://arxiv.org/abs/2004.07711) - Antoine Miech, Ivan Laptev, Josef Sivic, Heng Wang, Lorenzo Torresani, Du Tran, CVPRW 2019.

#### Long-Term Action Anticipation
- [Object-centric Video Representation for Long-term Action Anticipation](https://arxiv.org/abs/2311.00180) - Ce Zhang, Changcheng Fu, Shijie Wang, Nakul Agarwal, Kwonjoon Lee, Chiho Choi, Chen Sun, WACV 2024. [[code]](https://github.com/brown-palm/ObjectPrompt)

- [Intention-Conditioned Long-Term Human Egocentric Action Anticipation](https://openaccess.thecvf.com/content/WACV2023/papers/Mascaro_Intention-Conditioned_Long-Term_Human_Egocentric_Action_Anticipation_WACV_2023_paper.pdf) - Esteve Valls Mascaro´, Hyemin Ahn, Dongheui Lee, WACV 2023.

- [Multiscale Video Pretraining for Long-Term Activity Forecasting](https://arxiv.org/abs/2307.12854) - Reuben Tan, Matthias De Lange, Michael Iuzzolino, Bryan A. Plummer, Kate Saenko, Karl Ridgeway, Lorenzo Torresani,  2023.

- [AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?](https://arxiv.org/abs/2307.16368) - Qi Zhao, Ce Zhang, Shijie Wang, Changcheng Fu, Nakul Agarwal, Kwonjoon Lee, Chen Sun,  2023. [[project page]](https://brown-palm.github.io/AntGPT/)

- [Rethinking Learning Approaches for Long-Term Action Anticipation](https://arxiv.org/abs/2210.11566) - Megha Nawhal, Akash Abdu Jyothi, Greg Mori, ECCV 2022. [[project page]](https://meghanawhal.github.io/projects/anticipatr.html)

- [Learning to Anticipate Egocentric Actions by Imagination](https://arxiv.org/pdf/2101.04924.pdf) - Yu Wu, Linchao Zhu, Xiaohan Wang, Yi Yang, Fei Wu, TIP 2021.

- [On Diverse Asynchronous Activity Anticipation](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123740766.pdf) - He Zhao and Richard P. Wildes, ECCV 2020.

- [Time-Conditioned Action Anticipation in One Shot](https://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Time-Conditioned_Action_Anticipation_in_One_Shot_CVPR_2019_paper.html) - Qiuhong Ke, Mario Fritz, Bernt Schiele, CVPR 2019.

- [When Will You Do What? - Anticipating Temporal Occurrences of Activities](https://openaccess.thecvf.com/content_cvpr_2018/html/Abu_Farha_When_Will_You_CVPR_2018_paper.html) - Anticipating Temporal Occurrences of Activities](https://openaccess.thecvf.com/content_cvpr_2018/html/Abu_Farha_When_Will_You_CVPR_2018_paper.html) - Yazan Abu Farha, Alexander Richard, Juergen Gall, CVPR 2018.

- [Joint Prediction of Activity Labels and Starting Times in Untrimmed Videos](https://openaccess.thecvf.com/content_ICCV_2017/papers/Mahmud_Joint_Prediction_of_ICCV_2017_paper.pdf) - Tahmida Mahmud, Mahmudul Hasan, Amit K. Roy-Chowdhury, ICCV 2017.

- [First-Person Activity Forecasting with Online Inverse Reinforcement Learning](https://arxiv.org/pdf/1612.07796) - Nicholas Rhinehart, Kris M. Kitani, ICCV 2017. [[video]](https://youtu.be/rvVoW3iuq-s)

#### Future Gaze Prediction
- [Unsupervised gaze prediction in egocentric videos by energy-based surprise modeling](http://arxiv.org/abs/2001.11580) - Aakur, S.N., Bagavathi, A., ArXiv 2020.

- [Digging Deeper into Egocentric Gaze Prediction](https://arxiv.org/pdf/1904.06090) - Hamed R. Tavakoli and Esa Rahtu and Juho Kannala and Ali Borji, WACV 2019.

- [Predicting Gaze in Egocentric Video by Learning Task-dependent Attention Transition](https://arxiv.org/pdf/1803.09125) - Huang, Y., Cai, M., Li, Z., & Sato, Y., ECCV 2018. [[code]](https://github.com/hyf015/egocentric-gaze-prediction)

- [Deep future gaze: Gaze anticipation on egocentric videos using adversarial networks](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Deep_Future_Gaze_CVPR_2017_paper.pdf) - Zhang, M., Teck Ma, K., Hwee Lim, J., Zhao, Q., & Feng, J., CVPR 2017. [[code]](https://github.com/Mengmi/deepfuturegaze_gan)

- [Learning to predict gaze in egocentric video](http://ai.stanford.edu/~alireza/publication/Li-Fathi-Rehg-ICCV13.pdf) - Li, Yin, Alireza Fathi, and James M. Rehg, ICCV 2013.

#### Trajectory prediction
- [Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos](https://arxiv.org/abs/2405.04370) - Junyi Ma, Jingyi Xu, Xieyuanli Chen, Hesheng Wang,  2024.

- [EMAG: Ego-motion Aware and Generalizable 2D Hand Forecasting from Egocentric Videos](https://masashi-hatano.github.io/assets/pdf/emag.pdf) - Masashi Hatano, Ryo Hachiuma, Hideo Saito,  2024.

- [Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting](https://arxiv.org/abs/2307.08243) - Wentao Bao, Lele Chen, Libing Zeng, Zhong Li, Yi Xu, Junsong Yuan, Yu Kong, ICCV 2023.

- [Forecasting Action through Contact Representations from First Person Video](https://ieeexplore.ieee.org/abstract/document/9340014?casa_token=PUk2a8mN4CoAAAAA:ICkziPRIBtlxgzsyJm9ZVxUIzGnEq0phTHLOP8G8TxFlTIp159calFp8jZOdUCnxeWTknFjlB0w) - Eadom Dessalene; Chinmaya Devaraj; Michael Maynord; Cornelia Fermuller; Yiannis Aloimonos, T-PAMI 2021.

- [Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460681.pdf) - Miao Liu, Siyu Tang, Yin Li, James M. Rehg, ECCV 2020.

- [How Can I See My Future? FvTraj: Using First-person View for Pedestrian Trajectory Prediction](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520562.pdf) - Huikun Bi, Ruisi Zhang, Tianlu Mao, Zhigang Deng, Zhaoqi Wang, ECCV 2020. [[video]](https://youtu.be/HcsyH7zMHAw)

- [Multimodal Future Localization and Emergence Prediction for Objects in Egocentric View With a Reachability Prior](https://openaccess.thecvf.com/content_CVPR_2020/papers/Makansi_Multimodal_Future_Localization_and_Emergence_Prediction_for_Objects_in_Egocentric_CVPR_2020_paper.pdf) - Makansi, Osama and Cicek, Ozgun and Buchicchio, Kevin and Brox, Thomas, CVPR 2020. [[code]](https://github.com/lmb-freiburg/FLN-EPN-RPN)

- [Understanding Human Hands in Contact at Internet Scale](https://openaccess.thecvf.com/content_CVPR_2020/html/Shan_Understanding_Human_Hands_in_Contact_at_Internet_Scale_CVPR_2020_paper.html) - Dandan Shan, Jiaqi Geng, Michelle Shu, David F. Fouhey, CVPR 2020.

- [Future Person Localization in First-Person Videos](https://ieeexplore.ieee.org/document/8578890) - Takuma Yagi; Karttikeya Mangalam; Ryo Yonetani; Yoichi Sato, CVPR 2018.

- [Egocentric Future Localization](https://openaccess.thecvf.com/content_cvpr_2016/papers/Park_Egocentric_Future_Localization_CVPR_2016_paper.pdf) - Park, Hyun Soo and Hwang, Jyh-Jing and Niu, Yedong and Shi, Jianbo, CVPR 2016.

- [Going deeper into first-person activity recognition](http://www.cs.cmu.edu/~kkitani/pdf/MFK-CVPR2016.pdf) - Ma, M., Fan, H., & Kitani, K. M., CVPR 2016.

#### Region prediction
- [Interaction Region Visual Transformer for Egocentric Action Anticipation](https://openaccess.thecvf.com/content/WACV2024/papers/Roy_Interaction_Region_Visual_Transformer_for_Egocentric_Action_Anticipation_WACV_2024_paper.pdf) - Debaditya Roy, Ramanathan Rajendiran, Basura Fernando, WACV 2024. [[code]](https://github.com/LAHAproject/InAViT)

- [Joint Hand Motion and Interaction Hotspots Prediction From Egocentric Videos](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Joint_Hand_Motion_and_Interaction_Hotspots_Prediction_From_Egocentric_Videos_CVPR_2022_paper.pdf) - Shaowei Liu, Subarna Tripathi, Somdeb Majumdar, Xiaolong Wang, CVPR 2022. [[code]](https://github.com/stevenlsw/hoi-forecast) [[project page]](https://stevenlsw.github.io/hoi-forecast/)

- [EGO-TOPO: Environment Affordances from Egocentric Video](https://openaccess.thecvf.com/content_CVPR_2020/papers/Nagarajan_Ego-Topo_Environment_Affordances_From_Egocentric_Video_CVPR_2020_paper.pdf) - Nagarajan, Tushar and Li, Yanghao and Feichtenhofer, Christoph and Grauman, Kristen, CVPR 2020.

- [Forecasting human object interaction: Joint prediction of motor attention and egocentric activity](http://arxiv.org/abs/1911.10967) - Liu, M., Tang, S., Li, Y., Rehg, J., arXiv 2019.

- [Forecasting Hands and Objects in Future Frames](https://openaccess.thecvf.com/content_eccv_2018_workshops/w15/html/Fan_Forecasting_Hands_and_Objects_in_Future_Frames_ECCVW_2018_paper.html) - Chenyou Fan, Jangwon Lee, Michael S. Ryoo, ECCVW 2018.

- [Next-active-object prediction from egocentric videos](https://www.sciencedirect.com/science/article/abs/pii/S1047320317301967) - Antonino Furnari, Sebastiano Battiato, Kristen Grauman, Giovanni Maria Farinella, JVCIR 2017.

- [Unsupervised Learning of Important Objects From First-Person Videos](https://openaccess.thecvf.com/content_iccv_2017/html/Bertasius_Unsupervised_Learning_of_ICCV_2017_paper.html) - Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi, ICCV 2017.

- [First Person Action-Object Detection with EgoNet](https://arxiv.org/abs/1603.04908) - G Bertasius, HS Park, SX Yu, J Shi, arXiv 2016.

### Multi-Modalities
#### Audio-Visual
- [TIM: A Time Interval Machine for Audio-Visual Action Recognition](https://arxiv.org/abs/2404.05559) - Jacob Chalk, Jaesung Huh, Evangelos Kazakos, Andrew Zisserman, Dima Damen, CVPR 2024. [[project page]](https://jacobchalk.github.io/TIM-Project/)

- [SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos](https://arxiv.org/abs/2404.05206) - Changan Chen, Kumar Ashutosh, Rohit Girdhar, David Harwath, Kristen Grauman, CVPR 2024. [[project page]](https://vision.cs.utexas.edu/projects/soundingactions/)

- [The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective](https://openaccess.thecvf.com//content/CVPR2024/html/Jia_The_Audio-Visual_Conversational_Graph_From_an_Egocentric-Exocentric_Perspective_CVPR_2024_paper) - Wenqi Jia, Miao Liu, Hao Jiang, Ishwarya Ananthabhotla, James M. Rehg, Vamsi Krishna Ithapu, Ruohan Gao, CVPR 2024. [[project page]](https://vjwq.github.io/AV-CONV/)

- [Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Majumder_Learning_Spatial_Features_from_Audio-Visual_Correspondence_in_Egocentric_Videos_CVPR_2024_paper) - Sagnik Majumder, Ziad Al-Halah, Kristen Grauman, CVPR 2024. [[project page]](https://vision.cs.utexas.edu/projects/ego_av_corr/)

- [SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Chen_SoundingActions_Learning_How_Actions_Sound_from_Narrated_Egocentric_Videos_CVPR_2024_paper) - Changan Chen, Kumar Ashutosh, Rohit Girdhar, David Harwath, Kristen Grauman, CVPR 2024.

- [Centre Stage: Centricity-based Audio-Visual Temporal Action Detection](https://arxiv.org/abs/2311.16446) - Hanyuan Wang, Majid Mirmehdi, Dima Damen, Toby Perrett, WBMVC 2023. [[code]](https://github.com/hanielwang/Audio-Visual-TAD)

- [Multimodal Distillation for Egocentric Action Recognition](https://arxiv.org/abs/2307.07483) - Gorjan Radevski, Dusan Grujicic, Marie-Francine Moens, Matthew Blaschko, Tinne Tuytelaars, ICCV 2023. [[code]](https://github.com/gorjanradevski/multimodal-distillation)

- [Egocentric Audio-Visual Object Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Egocentric_Audio-Visual_Object_Localization_CVPR_2023_paper.pdf) - Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu, CVPR 2023. [[code]](https://github.com/WikiChao/Ego-AV-Loc)

- [Audio Visual Speaker Localization from EgoCentric Views](https://arxiv.org/abs/2309.16308) - Jinzheng Zhao, Yong Xu, Xinyuan Qian, Wenwu Wang,  2023. [[code]](https://github.com/KawhiZhao/Egocentric-Audio-Visual-Speaker-Localization)

- [Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition](https://arxiv.org/abs/2110.10101) - Mirco Planamente, Chiara Plizzari, Emanuele Alberti, Barbara Caputo, WACV 2022.

- [Attention Bottlenecks for Multimodal Fusion](https://arxiv.org/abs/2107.00135) - Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, Chen Sun, NIPS 2021.

- [Slow-Fast Auditory Streams For Audio Recognition](https://arxiv.org/abs/2103.03516) - Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen, ICASSP 2021.

- [With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition](https://arxiv.org/abs/2111.01024) - Evangelos Kazakos, Jaesung Huh, Arsha Nagrani, Andrew Zisserman, Dima Damen, BMVC 2021.

- [Multi-modal Egocentric Activity Recognition using Audio-Visual Features](https://arxiv.org/pdf/1807.00612.pdf) - Mehmet Ali Arabacı, Fatih Özkan, Elif Surer, Peter Jančovič, Alptekin Temizel, MTA 2020.

- [Seeing and Hearing Egocentric Actions: How Much Can We Learn?](https://arxiv.org/abs/1910.06693) - Alejandro Cartas, Jordi Luque, Petia Radeva, Carlos Segura, Mariella Dimiccoli, WICCV 2019.

- [EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf) - Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima, ICCV 2019.

#### Depth
- [Multimodal Score Fusion with Sparse Low Rank Bilinear Pooling for Egocentric Hand Action Recognition](https://dl.acm.org/doi/10.1145/3656044) - Kankana Roy, TOMM 2024.

- [Egocentric RGB+Depth Action Recognition in Industry-Like Settings](https://arxiv.org/abs/2309.13962) - Jyoti Kini, Sarah Fleischer, Ishan Dave, Mubarak Shah,  2023.

- [Egocentric Scene Understanding via Multimodal Spatial Rectifier](https://openaccess.thecvf.com/content/CVPR2022/papers/Do_Egocentric_Scene_Understanding_via_Multimodal_Spatial_Rectifier_CVPR_2022_paper.pdf) - Tien Do, Khiem Vuong, Hyun Soo Park, CVPR 2022. [[code]](https://github.com/tien-d/EgoDepthNormal)

- [Moving SLAM: Fully Unsupervised Deep Learning in Non-Rigid Scenes](https://ieeexplore.ieee.org/abstract/document/9636075?casa_token=UYaHkHj4TJcAAAAA:NketkffyE1V5tUn7UeG_ko9rQGzgiwgSPrliBqW2uhuihLNYtSJa2vU-ufaRzWxRvWZzCMLqGw) - Dan Xu, Andrea Vedaldi, João F. Henriques, IROS 2021.

- [Trear: Transformer-based RGB-D Egocentric Action Recognition](https://ieeexplore.ieee.org/abstract/document/9312201?casa_token=VjrXPrZDuSgAAAAA:ezQgxMoeH7q3fxl8su7zg1yghkp60nbxCwU3FxyZEKWghbUVozmKmS_YE99AYceBr3lxA6Ud) - Xiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, Wanqing Li, TCDS 2020.

- [Multi-stream Deep Neural Networks for RGB-D Egocentric Action Recognition](http://www.cs.toronto.edu/~zianwang/MDNN/TCSVT18_MDNN.pdf) - Yansong Tang, Zian Wang, Jiwen Lu, Jianjiang Feng, Jie Zhou, TCSVT 2018.

- [First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations](https://openaccess.thecvf.com/content_cvpr_2018/papers/Garcia-Hernando_First-Person_Hand_Action_CVPR_2018_paper.pdf) - Garcia-Hernando, Guillermo and Yuan, Shanxin and Baek, Seungryul and Kim, Tae-Kyun, CVPR 2018. [[code]](https://github.com/guiggh/hand_pose_action)

- [Action recognition in RGB-D egocentric videos](https://ieeexplore.ieee.org/document/8296915) - Yansong Tang, Yi Tian, Jiwen Lu, Jianjiang Feng, Jie Zhou, ICIP 2017.

- [Scene Semantic Reconstruction from Egocentric RGB-D-Thermal Videos](https://ieeexplore.ieee.org/abstract/document/8374614) - Rachel Luo, Ozan Sener, Silvio Savarese, 3DV 2017.

- [3D Hand Pose Detection in Egocentric RGB-D Images](https://link.springer.com/chapter/10.1007/978-3-319-16178-5_25) - Grégory Rogez, Maryam Khademi, J. S. Supančič III, J. M. M. Montiel, Deva Ramanan, WECCV 2014.

#### Thermal
- [Scene Semantic Reconstruction from Egocentric RGB-D-Thermal Videos](https://ieeexplore.ieee.org/abstract/document/8374614) - Rachel Luo, Ozan Sener, Silvio Savarese, 3DV 2017.

#### Event
- [EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams](https://arxiv.org/abs/2404.08640) - Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik, CVPR 2024. [[project page]](https://4dqv.mpi-inf.mpg.de/EventEgo3D/)

- [EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams](https://openaccess.thecvf.com//content/CVPR2024/html/Millerdurai_EventEgo3D_3D_Human_Motion_Capture_from_Egocentric_Event_Streams_CVPR_2024_paper) - Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik, CVPR 2024. [[project page]](https://4dqv.mpi-inf.mpg.de/EventEgo3D/)

- [EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams](https://openaccess.thecvf.com//content/CVPR2024/html/Millerdurai_EventEgo3D_3D_Human_Motion_Capture_from_Egocentric_Event_Streams_CVPR_2024_paper) - Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik, CVPR 2024. [[project page]](https://4dqv.mpi-inf.mpg.de/EventEgo3D/)

- [EventTransAct: A video transformer-based framework for Event-camera based action recognition](https://arxiv.org/abs/2308.13711) - Tristan de Blegiers, Ishan Rajendrakumar Dave, Adeel Yousaf, Mubarak Shah, IROS 2023. [[project page]](https://tristandb8.github.io/EventTransAct_webpage/)

- [E(GO)^2MOTION: Motion Augmented Event Stream for Egocentric Action Recognition](https://arxiv.org/abs/2112.03596) - Chiara Plizzari, Mirco Planamente, Gabriele Goletto, Marco Cannici, Emanuele Gusso, Matteo Matteucci, Barbara Caputo, CVPR 2022.

#### IMU
- [Continual Egocentric Activity Recognition with Foreseeable-Generalized Visual-IMU Representations](https://ieeexplore.ieee.org/abstract/document/10462907/authors#authors) - Chiyuan He, Shaoxu Cheng, Zihuan Qiu, Linfeng Xu, Fanman Meng, Qingbo Wu, Hongliang Li, IEEE Sensors Journal 2024.

- [How You Move Your Head Tells What You Do: Self-supervised Video Representation Learning with Egocentric Cameras and IMU Sensors](https://arxiv.org/abs/2110.01680) - Satoshi Tsutsui, Ruta Desai, Karl Ridgeway, WICCV 2021.

### Temporal Segmentation (Action Detection)
- [ActionMixer: Temporal action detection with Optimal Action Segment Assignment and mixers](https://www.sciencedirect.com/science/article/abs/pii/S0957417423018328) - Jianhua Yang, Ke Wang, Lijun Zhao, Zhiqiang Jiang, Ruifeng Li, Expert Systems with Applications 2024.

- [FACT: Frame-Action Cross-Attention Temporal](https://openaccess.thecvf.com//content/CVPR2024/html/Lu_FACT_Frame-Action_Cross-Attention_Temporal_Modeling_for_Efficient_Action_Segmentation_CVPR_2024_paper) - Zijia Lu, Ehsan Elhamifar, CVPR 2024. [[code]](https://github.com/ZijiaLewisLu/CVPR2024-FACT)

- [https://openaccess.thecvf.com//content/CVPR2024/html/Shen_Progress-Aware_Online_Action_Segmentation_for_Egocentric_Procedural_Task_Videos_CVPR_2024_paper](https://openaccess.thecvf.com//content/CVPR2024/html/Shen_Progress-Aware_Online_Action_Segmentation_for_Egocentric_Procedural_Task_Videos_CVPR_2024_paper) - Yuhan Shen, Ehsan Elhamifar, CVPR 2024. [[code]](https://github.com/Yuhan-Shen/ProTAS)

- [Refining Action Boundaries for One-stage Detection](https://ieeexplore.ieee.org/abstract/document/9959554?casa_token=AT_Tl3i91NUAAAAA:h_YK0QbnF65WRQL_Au9y82_FPV7SPbQnc_Duw3CTZ3MtMgrvSfH7Z3Vn4eMF-4Cmpoh38kPo) - Hanyuan Wang, Majid Mirmehdi, Dima Damen, Toby Perrett, AVSS 2024.

- [Quasi-Online Detection of Take and Release Actions from Egocentric Videos](https://link.springer.com/chapter/10.1007/978-3-031-43153-1_2) - Rosario Scavo, Francesco Ragusa, Giovanni Maria Farinella, Antonino Furnari, ICIAP 2023. [[code]](https://github.com/fpv-iplab/Quasi-Online-Detection-Take-Release)

- [Memory-and-Anticipation Transformer for Online Action Understanding](https://arxiv.org/abs/2308.07893) - Jiahao Wang, Guo Chen, Yifei Huang, Limin Wang, Tong Lu, ICCV 2023. [[code]](https://github.com/Echo0125/Memory-and-Anticipation-Transformer)

- [Ego-Only: Egocentric Action Detection without Exocentric Transferring](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Ego-Only_Egocentric_Action_Detection_without_Exocentric_Transferring_ICCV_2023_paper.html) - Huiyu Wang, Mitesh Kumar Singh, Lorenzo Torresani, ICCV 2023.

- [Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs](https://arxiv.org/abs/2312.02638) - Camillo Quattrocchi, Antonino Furnari, Daniele Di Mauro, Mario Valerio Giuffrida, Giovanni Maria Farinella,  2023. [[code]](https://github.com/fpv-iplab/synchronization-is-all-you-need)

- [My View is the Best View: Procedure Learning from Egocentric Videos](https://arxiv.org/abs/2207.10883) - Siddhant Bansal, Chetan Arora, C.V. Jawahar, ECCV 2022.

- [Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Bridge-Prompt_Towards_Ordinal_Action_Understanding_in_Instructional_Videos_CVPR_2022_paper.pdf) - Muheng Li, Lei Chen, Yueqi Duan, Zhilan Hu, Jianjiang Feng, Jie Zhou, Jiwen Lu, CVPR 2022. [[code]](https://github.com/ttlmh/Bridge-Prompt)

- [Temporal Action Segmentation from Timestamp Supervision](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Temporal_Action_Segmentation_From_Timestamp_Supervision_CVPR_2021_paper.pdf) - Zhe Li, Yazan Abu Farha, Jurgen Gall, CVPR 2021.

- [UnweaveNet: Unweaving Activity Stories](https://arxiv.org/pdf/2112.10194.pdf) - Will Price, Carl Vondrick, Dima Damen,  2021.

- [Personal-Location-Based Temporal Segmentation of Egocentric Video for Lifelogging Applications](https://iplab.dmi.unict.it/PersonalLocationSegmentation/downloads/furnari2018personal.pdf) - A. Furnari, G. M. Farinella, S. Battiato, Journal of Visual Communication and Image Representation 2017.

- [Temporal segmentation and activity classification from first-person sensing](https://ieeexplore.ieee.org/document/5204354) - Spriggs, Ekaterina H., Fernando De La Torre, and Martial Hebert, Computer Vision and Pattern Recognition Workshops, WCVPR 2009.

### Retrieval
- [Retrieval-Augmented Egocentric Video Captioning](https://arxiv.org/abs/2401.00789v2) - Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie, CVPR 2024.

- [Where is my Wallet? Modeling Object Proposal Sets for Egocentric Visual Query Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Where_Is_My_Wallet_Modeling_Object_Proposal_Sets_for_Egocentric_CVPR_2023_paper.pdf) - Mengmeng Xu, Yanghao Li, Cheng-Yang Fu, Bernard Ghanem, Tao Xiang, Juan-Manuel Pérez-Rúa, CVPR 2023. [[code]](https: //github.com/facebookresearch/vq2d_cvpr)

- [Learning Temporal Sentence Grounding From Narrated EgoVideos](https://arxiv.org/abs/2310.17395) - Kevin Flanagan, Dima Damen, Michael Wray, BMVC 2023. [[code]](https://github.com/keflanagan/CliMer)

- [Single-Stage Visual Query Localization in Egocentric Videos](https://arxiv.org/abs/2306.09324) - Hanwen Jiang, Santhosh Kumar Ramakrishnan, Kristen Grauman,  2023. [[project page]](https://hwjiang1510.github.io/VQLoC/)

- [On Semantic Similarity in Video Retrieval](https://openaccess.thecvf.com/content/CVPR2021/papers/Wray_On_Semantic_Similarity_in_Video_Retrieval_CVPR_2021_paper.pdf) - Michael Wray, Hazel Doughty, Dima Damen, CVPR 2021.

- [Domain Adaptation in Multi-View Embedding for Cross-Modal Video Retrieval](https://arxiv.org/abs/2110.12812) - Jonathan Munro, Michael Wray, Diane Larlus, Gabriela Csurka, Dima Damen,  2021.

- [Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings](https://openaccess.thecvf.com/content_ICCV_2019/papers/Wray_Fine-Grained_Action_Retrieval_Through_Multiple_Parts-of-Speech_Embeddings_ICCV_2019_paper.pdf) - Michael Wray, Diane Larlus, Gabriela Csurka, Dima Damen, ICCV 2019.

### Segmentation
- [Learning to Segment Referred Objects from Narrated Egocentric Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Shen_Learning_to_Segment_Referred_Objects_from_Narrated_Egocentric_Videos_CVPR_2024_paper) - Yuhan Shen, Huiyu Wang, Xitong Yang, Matt Feiszli, Ehsan Elhamifar, Lorenzo Torresani, Effrosyni Mavroudi;, CVPR 2024.

- [Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark](https://arxiv.org/pdf/2312.02672.pdf) - Rosario Leonardi, Antonino Furnari, Francesco Ragusa, Giovanni Maria Farinella,  2023. [[project page]](https://iplab.dmi.unict.it/HOI-Synth/)

- [Generative Adversarial Network for Future Hand Segmentation from Egocentric Video](https://arxiv.org/abs/2203.11305) - Wenqi Jia, Miao Liu, James M. Rehg, ECCV 2022. [[project page]](https://vjwq.github.io/EgoGAN/)

### Video-Language
- [SLVP: Self-Supervised Language-Video Pre-Training for Referring Video Object Segmentation](https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Mei_SLVP_Self-Supervised_Language-Video_Pre-Training_for_Referring_Video_Object_Segmentation_WACVW_2024_paper.html) - Jie Mei, AJ Piergiovanni, Jenq-Neng Hwang, Wei Li, WACVW 2024.

- [A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval](https://arxiv.org/abs/2402.19106) - Andreea-Maria Oncescu, João F. Henriques, Andrew Zisserman, Samuel Albanie, A. Sophia Koepke, ICASSP 2024.

- [Detours for Navigating Instructional Videos](https://arxiv.org/abs/2401.01823) - Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, Kristen Grauman, CVPR 2024. [[project page]](https://vision.cs.utexas.edu/projects/detours/)

- [Grounded Question-Answering in Long Egocentric Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Di_Grounded_Question-Answering_in_Long_Egocentric_Videos_CVPR_2024_paper) - Shangzhe Di, Weidi Xie;, CVPR 2024. [[code]](https://github.com/Becomebright/GroundVQA)

- [Video ReCap: Recursive Captioning of Hour-Long Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Islam_Video_ReCap_Recursive_Captioning_of_Hour-Long_Videos_CVPR_2024_paper) - Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius, CVPR 2024. [[project page]](https://sites.google.com/view/vidrecap)

- [Learning Object States from Actions via Large Language Models](https://arxiv.org/abs/2405.01090) - Masatoshi Tateno, Takuma Yagi, Ryosuke Furuta, Yoichi Sato,  2024.

- [Step Differences in Instructional Video](https://arxiv.org/abs/2404.16222) - Tushar Nagarajan, Lorenzo Torresani,  2024.

- [EgoNCE++: Do Egocentric Video-Language Models Really Understand Hand-Object Interactions?](https://arxiv.org/abs/2405.17719) - Boshen Xu, Ziheng Wang, Yang Du, Sipeng Zheng, Zhinan Song, Qin Jin,  2024. [[code]](https://github.com/xuboshen/EgoNCEpp)

- [HERO: A Multi-modal Approach on Mobile Devices for Visual-Aware Conversational Assistance in Industrial Domains](https://link.springer.com/chapter/10.1007/978-3-031-43148-7_36) - Claudia Bonanno, Francesco Ragusa, Antonino Furnari, Giovanni Maria Farinella, ICIAP 2023.

- [EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone](https://arxiv.org/abs/2307.05463) - Shraman Pramanick, Yale Song, Sayan Nag, Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou, Rama Chellappa, Pengchuan Zhang, ICCV 2023. [[project page]](https://shramanpramanick.github.io/EgoVLPv2/)

- [Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge](https://arxiv.org/abs/2310.15066) - Te-Lin Wu, Yu Zhou, Nanyun Peng, EMNLP 2023.

- [NaQ: Leveraging Narrations As Queries To Supervise Episodic Memory](https://openaccess.thecvf.com/content/CVPR2023/papers/Ramakrishnan_NaQ_Leveraging_Narrations_As_Queries_To_Supervise_Episodic_Memory_CVPR_2023_paper.pdf) - Santhosh Kumar Ramakrishnan, Ziad Al-Halah, Kristen Grauman, CVPR 2023. [[project page]](https://vision.cs.utexas.edu/projects/naq/)

- [HierVL: Learning Hierarchical Video-Language Embeddings](https://openaccess.thecvf.com/content/CVPR2023/papers/Ashutosh_HierVL_Learning_Hierarchical_Video-Language_Embeddings_CVPR_2023_paper.pdf) - Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, Kristen Grauman, CVPR 2023. [[project page]](https://vision.cs.utexas.edu/projects/hiervl/)

- [Learning Video Representations from Large Language Models](https://arxiv.org/abs/2212.04501) - Yue Zhao, Ishan Misra, Philipp Krähenbühl, Rohit Girdhar, CVPR 2023. [[code]](https://github.com/facebookresearch/LaViLa) [[project page]](https://facebookresearch.github.io/LaViLa/)

- [LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning](https://arxiv.org/abs/2312.03849) - Bolin Lai, Xiaoliang Dai, Lawrence Chen, Guan Pang, James M. Rehg, Miao Liu,  2023.

- [Grounded Question-Answering in Long Egocentric Videos](https://arxiv.org/abs/2312.06505) - Shangzhe Di, Weidi Xie,  2023.

- [GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos](https://arxiv.org/abs/2312.07322) - Tomáš Souček, Dima Damen, Michael Wray, Ivan Laptev, Josef Sivic,  2023. [[project page]](https://soczech.github.io/genhowto/)

- [EgoTaskQA: Understanding Human Tasks in Egocentric Videos](https://proceedings.neurips.cc/paper_files/paper/2022/hash/161c94a58ca25bafcaf47893e8233deb-Abstract-Datasets_and_Benchmarks.html) - Baoxiong Jia, Ting Lei, Song-Chun Zhu, Siyuan Huang, NeurIPS 2022. [[project page]](https://sites.google.com/view/egotaskqa)

- [Episodic Memory Question Answering](https://arxiv.org/pdf/2205.01652.pdf) - Samyak Datta, Sameer Dharur, Vincent Cartillier, Ruta Desai, Mukul Khanna, Dhruv Batra, CVPR 2022.

- [Egocentric Video-Language Pretraining](https://arxiv.org/pdf/2206.01670.pdf) - Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao, Weijie Kong, Chengfei Cai, Hongfa Wang, Dima Damen, Bernard Ghanem, Wei Liu,  2022.

### Few-Shot Action Recognition
- [Unifying Few- and Zero-Shot Egocentric Action Recognition](https://arxiv.org/abs/2006.11393) - Tyler R. Scott, Michael Shvartsman, Karl Ridgeway, CVPRW 2021.

### Gaze
- [In the Eye of Transformer: Global–Local Correlation for Egocentric Gaze Estimation and Beyond](https://link.springer.com/article/10.1007/s11263-023-01879-7) - Bolin Lai, Miao Liu, Fiona Ryan, James M. Rehg, IJCV 2023. [[project page]](https://bolinlai.github.io/GLC-EgoGazeEst/)

- [1000 Pupil Segmentations in a Second Using Haar Like Features and Statistical Learning](https://openaccess.thecvf.com/content/ICCV2021W/EPIC/html/Fuhl_1000_Pupil_Segmentations_in_a_Second_Using_Haar_Like_Features_ICCVW_2021_paper.html) - Wolfgang Fuhl, Johannes Schneider, Enkelejda Kasneci, WICCV 2021.

### From Third-Person to First-Person
- [The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective](https://openaccess.thecvf.com//content/CVPR2024/html/Jia_The_Audio-Visual_Conversational_Graph_From_an_Egocentric-Exocentric_Perspective_CVPR_2024_paper) - Wenqi Jia, Miao Liu, Hao Jiang, Ishwarya Ananthabhotla, James M. Rehg, Vamsi Krishna Ithapu, Ruohan Gao, CVPR 2024. [[project page]](https://vjwq.github.io/AV-CONV/)

- [Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos](https://arxiv.org/abs/2403.06351) - Mi Luo, Zihui Xue, Alex Dimakis, Kristen Grauman,  2024.

- [Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment](https://arxiv.org/abs/2306.05526) - Zihui Xue, Kristen Grauman, UnderReview 2023. [[project page]](https://vision.cs.utexas.edu/projects/AlignEgoExo/)

- [POV: Prompt-Oriented View-Agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World](https://dl.acm.org/doi/abs/10.1145/3581783.3612484) - Boshen Xu, Sipeng Zheng, Qin Jin, MM 2023.

- [Ego-Exo: Transferring Visual Representations From Third-Person to First-Person Videos](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Ego-Exo_Transferring_Visual_Representations_From_Third-Person_to_First-Person_Videos_CVPR_2021_paper.pdf) - Yanghao Li, Tushar Nagarajan, Bo Xiong, Kristen Grauman, CVPR 2021.

- [Making Third Person Techniques Recognize First-Person Actions in Egocentric Videos](https://ieeexplore.ieee.org/abstract/document/8451249?casa_token=p1k79yrTIkMAAAAA:QHlXMC8Y7qrCEDsdypGNZbh7zeoEPVEs2k6j5a0g1MkvA76Uf6_VDIfCzbiG2bWdU8EoFyagbK4) - Sagar Verma, Pravin Nagar, Divam Gupta, Chetan Arora, ICIP 2018.

- [Actor and Observer: Joint Modeling of First and Third-Person Videos](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sigurdsson_Actor_and_Observer_CVPR_2018_paper.pdf) - Gunnar A. Sigurdsson and Abhinav Gupta and Cordelia Schmid and Ali Farhadi and Karteek Alahari, CVPR 2018. [[code]](https://github.com/gsig/actor-observer)

### NeRF
- [Balanced Spherical Grid for Egocentric View Synthesis](https://openaccess.thecvf.com/content/CVPR2023/html/Choi_Balanced_Spherical_Grid_for_Egocentric_View_Synthesis_CVPR_2023_paper.html) - Changwoon Choi, Sang Min Kim, Young Min Kim, CVPR 2023. [[project page]](https://changwoon.info/publications/EgoNeRF)

### User Data from an Egocentric Point of View
- [Egocentric Whole-Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement](https://arxiv.org/abs/2311.16495) - Jian Wang, Zhe Cao, Diogo Luvizon, Lingjie Liu, Kripasindhu Sarkar, Danhang Tang, Thabo Beeler, Christian Theobalt, CVPR 2024. [[project page]](https://people.mpi-inf.mpg.de/~jianwang/projects/egowholemocap/)

- [Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting](https://arxiv.org/abs/2402.18330) - Taeho Kang, Youngki Lee, CVPR 2024. [[code]](https://github.com/tho-kn/EgoTAP)

- [EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams](https://openaccess.thecvf.com//content/CVPR2024/html/Millerdurai_EventEgo3D_3D_Human_Motion_Capture_from_Egocentric_Event_Streams_CVPR_2024_paper) - Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik, CVPR 2024. [[project page]](https://4dqv.mpi-inf.mpg.de/EventEgo3D/)

- [Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting](https://openaccess.thecvf.com//content/CVPR2024/html/Kang_Attention-Propagation_Network_for_Egocentric_Heatmap_to_3D_Pose_Lifting_CVPR_2024_paper) - Taeho Kang, Youngki Lee, CVPR 2024. [[code]](https://github.com/tho-kn/EgoTAP)

- [SimpleEgo: Predicting Probabilistic Body Pose from Egocentric Cameras](https://arxiv.org/abs/2401.14785) - Hanz Cuevas-Velasquez, Charlie Hewitt, Sadegh Aliakbarian, Tadas Baltrušaitis, 3DV 2024. [[project page]](https://microsoft.github.io/SimpleEgo/)

- [Ego3DPose: Capturing 3D Cues from Binocular Egocentric Views](https://dl.acm.org/doi/abs/10.1145/3610548.3618147?casa_token=sfD7LG7W-icAAAAA:SLe0a5OVLpchi3l0_lyDcYx_ecufq5NwMauQsBAFuAC96k1WeMTKlycq6-agw9imu1Brqf35-K0B) - Taeho Kang, Kyungjin Lee, Jinrui Zhang, Youngki Lee, SIGGRAPH 2023.

- [Domain-Guided Spatio-Temporal Self-Attention for Egocentric 3D Pose Estimation](https://dl.acm.org/doi/abs/10.1145/3580305.3599312) - Jinman Park,  Kimathi Kaai,  Saad Hossain,  Norikatsu Sumi,  Sirisha Rambhatla,  Paul Fieguth, KDD 2023. [[code]](https://github.com/jmpark0808/Ego-STAN)

- [Scene-aware Egocentric 3D Human Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Scene-Aware_Egocentric_3D_Human_Pose_Estimation_CVPR_2023_paper.pdf) - Jian Wang, Diogo Luvizon, Weipeng Xu, Lingjie Liu, Kripasindhu Sarkar, Christian Theobalt, CVPR 2023. [[project page]](https://github.com/yt4766269/SceneEgo)

- [Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition From Egocentric RGB Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_Hierarchical_Temporal_Transformer_for_3D_Hand_Pose_Estimation_and_Action_CVPR_2023_paper.pdf) - Yilin Wen, Hao Pan, Lei Yang, Jia Pan, Taku Komura, Wenping Wang, CVPR 2023. [[code]](https://github.com/fylwen/HTT)

- [3D Human Pose Perception from Egocentric Stereo Videos](https://arxiv.org/abs/2401.00889) - Hiroyasu Akada, Jian Wang, Vladislav Golyanik, Christian Theobalt,  2023. [[project page]](https://4dqv.mpi-inf.mpg.de/UnrealEgo2/)

- [Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual Reality](https://openaccess.thecvf.com/content/CVPR2022/papers/Jourabloo_Robust_Egocentric_Photo-Realistic_Facial_Expression_Transfer_for_Virtual_Reality_CVPR_2022_paper.pdf) - Amin Jourabloo, Fernando De la Torre, Jason Saragih, Shih-En Wei, Stephen Lombardi, Te-Li Wang, Danielle Belko, Autumn Trimble, Hernan Badino, CVPR 2022.

- [Whose Hand Is This? Person Identification From Egocentric Hand Gestures](https://openaccess.thecvf.com/content/WACV2021/html/Tsutsui_Whose_Hand_Is_This_Person_Identification_From_Egocentric_Hand_Gestures_WACV_2021_paper.html) - Satoshi Tsutsui, Yanwei Fu, David J. Crandall, WACV 2021.

- [Dynamics-regulated kinematic policy for egocentric pose estimation](https://proceedings.neurips.cc/paper/2021/hash/d1fe173d08e959397adf34b1d77e88d7-Abstract.html) - Zhengyi Luo, Ryo Hachiuma, Ye Yuan, Kris Kitani, NIPS 2021.

- [Estimating Egocentric 3D Human Pose in Global Space](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Estimating_Egocentric_3D_Human_Pose_in_Global_Space_ICCV_2021_paper.html) - Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, Christian Theobalt, ICCV 2021.

- [Egocentric Pose Estimation From Human Vision Span](https://openaccess.thecvf.com/content/ICCV2021/html/Jiang_Egocentric_Pose_Estimation_From_Human_Vision_Span_ICCV_2021_paper.html) - Hao Jiang, Vamsi Krishna Ithapu, ICCV 2021.

- [EgoRenderer: Rendering Human Avatars From Egocentric Camera Images](https://openaccess.thecvf.com/content/ICCV2021/html/Hu_EgoRenderer_Rendering_Human_Avatars_From_Egocentric_Camera_Images_ICCV_2021_paper.html) - Tao Hu, Kripasindhu Sarkar, Lingjie Liu, Matthias Zwicker, Christian Theobalt, ICCV 2021.

- [Recognizing Camera Wearer from Hand Gestures in Egocentric Videos](https://dl.acm.org/doi/pdf/10.1145/3394171.3413654?casa_token=tlspOQU5qekAAAAA:rM0hbyyg1cvY5KRK16blErILxTO_OJpU9CIr8W9nDxBbdvjJBNxyKJ5GcNWTjrgJwV_H_Me8cFlj) - Daksh Thapar, Aditya Nigam, Chetan Arora, MM 2020.

- [You2Me: Inferring Body Pose in Egocentric Video via First and Second Person Interactions](https://openaccess.thecvf.com/content_CVPR_2020/papers/Ng_You2Me_Inferring_Body_Pose_in_Egocentric_Video_via_First_and_CVPR_2020_paper.pdf) - Ng, Evonne and Xiang, Donglai and Joo, Hanbyul and Grauman, Kristen, CVPR 2020. [[code]](https://github.com/facebookresearch/you2me#)

- [Ego-Pose Estimation and Forecasting as Real-Time PD Control](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yuan_Ego-Pose_Estimation_and_Forecasting_As_Real-Time_PD_Control_ICCV_2019_paper.pdf) - Ye Yuan and Kris Kitani, ICCV 2019. [[code]](https://github.com/Khrylx/EgoPose)

- [xR-EgoPose: Egocentric 3D Human Pose From an HMD Camera](https://openaccess.thecvf.com/content_ICCV_2019/papers/Tome_xR-EgoPose_Egocentric_3D_Human_Pose_From_an_HMD_Camera_ICCV_2019_paper.pdf) - Tome, Denis and Peluse, Patrick and Agapito, Lourdes and Badino, Hernan, ICCV 2019.

- [3D Ego-Pose Estimation via Imitation Learning](https://openaccess.thecvf.com/content_ECCV_2018/html/Ye_Yuan_3D_Ego-Pose_Estimation_ECCV_2018_paper.html) - Ye Yuan, Kris Kitani, ECCV 2018.

### Localization
- [Egocentric Activity Recognition and Localization on a 3D Map](https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Trans4Map_Revisiting_Holistic_Birds-Eye-View_Mapping_From_Egocentric_Images_to_Allocentric_WACV_2023_paper.pdf) - Chang Chen, Jiaming Zhang, Kailun Yang, Kunyu Peng, Rainer Stiefelhagen, WACV 2023. [[code]](https://github.com/jamycheung/Trans4Map)

- [Object Goal Navigation with Recursive Implicit Maps](https://arxiv.org/pdf/2308.05602.pdf) - hizhe Chen, Thomas Chabal, Ivan Laptev, Cordelia Schmid, IROS 2023. [[project page]](https://www.di.ens.fr/willow/research/onav_rim/)

- [An Optimized Pipeline for Image-Based Localization in Museums from Egocentric Images](https://link.springer.com/chapter/10.1007/978-3-031-43148-7_43) - Nicola Messina, Fabrizio Falchi, Antonino Furnari, Claudio Gennaro, Giovanni Maria Farinella, ICIAP 2023.

- [EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries](https://openaccess.thecvf.com/content/ICCV2023/html/Mai_EgoLoc_Revisiting_3D_Object_Localization_from_Egocentric_Videos_with_Visual_ICCV_2023_paper.html) - Jinjie Mai, Abdullah Hamdi, Silvio Giancola, Chen Zhao, Bernard Ghanem, ICCV 2023. [[code]](https://github.com/Wayne-Mai/EgoLoc)

- [Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations](https://openaccess.thecvf.com/content/CVPR2023/papers/Majumder_Chat2Map_Efficient_Scene_Mapping_From_Multi-Ego_Conversations_CVPR_2023_paper.pdf) - Sagnik Majumder, Hao Jiang, Pierre Moulon, Ethan Henderson, Paul Calamia, Kristen Grauman, Vamsi Krishna Ithapu, CVPR 2023. [[project page]](https://vision.cs.utexas.edu/projects/chat2map/)

- [InCrowdFormer: On-Ground Pedestrian World Model From Egocentric Views](https://arxiv.org/abs/2303.09534) - Mai Nishimura, Shohei Nobuhara, Ko Nishino,  2023.

- [Egocentric Indoor Localization From Room Layouts and Image Outer Corners](https://openaccess.thecvf.com/content/ICCV2021W/EPIC/html/Chen_Egocentric_Indoor_Localization_From_Room_Layouts_and_Image_Outer_Corners_ICCVW_2021_paper.html) - Xiaowei Chen, Guoliang Fan, WICCV 2021.

- [Egocentric Activity Recognition and Localization on a 3D Map](https://arxiv.org/abs/2105.09544) - Miao Liu, Lingni Ma, Kiran Somasundaram, Yin Li, Kristen Grauman, James M. Rehg, Chao Li,  2021.

- [Egocentric Shopping Cart Localization](https://iplab.dmi.unict.it/EgocentricShoppingCartLocalization/home/_paper/egocentric%20shopping%20cart%20localization.pdf) - E. Spera, A. Furnari, S. Battiato, G. M. Farinella, ICPR 2018.

- [Recognizing personal locations from egocentric videos](https://ieeexplore.ieee.org/document/7588113) - Furnari, A., Farinella, G. M., & Battiato, S., IEEE Transactions on Human-Machine Systems 2017.

- [Context-based vision system for place and object recognition](https://www.cs.ubc.ca/~murphyk/Papers/iccv03.pdf) - Torralba, A., Murphy, K. P., Freeman, W. T., & Rubin, M. A., ICCV 2003.

### Privacy protection
- [Anonymizing Egocentric Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Thapar_Anonymizing_Egocentric_Videos_ICCV_2021_paper.pdf) - Daksh Thapar, Aditya Nigam, Chetan Arora, ICCV 2021.

- [Mitigating Bystander Privacy Concerns in Egocentric Activity Recognition with Deep Learning and Intentional Image Degradation](http://users.ece.utexas.edu/~ethomaz/papers/j2.pdf) - Dimiccoli, M., Marín, J., & Thomaz, E., Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2018.

- [Privacy-Preserving Human Activity Recognition from Extreme Low Resolution](https://arxiv.org/pdf/1604.03196) - Ryoo, M. S., Rothrock, B., Fleming, C., & Yang, H. J., AAAI 2017.

### Tracking
- [Instance Tracking in 3D Scenes from Egocentric Videos](https://arxiv.org/abs/2312.04117) - Yunhan Zhao, Haoyu Ma, Shu Kong, Charless Fowlkes, CVPR 2024. [[code]](https://github.com/IT3DEgo/IT3DEgo/)

- [Spatial Cognition from Egocentric Video: Out of Sight, Not Out of Mind](https://arxiv.org/abs/2404.05072) - Chiara Plizzari, Shubham Goel, Toby Perrett, Jacob Chalk, Angjoo Kanazawa, Dima Damen,  2024. [[project page]](https://dimadamen.github.io/OSNOM/)

- [Tracking Multiple Deformable Objects in Egocentric Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Tracking_Multiple_Deformable_Objects_in_Egocentric_Videos_CVPR_2023_paper.pdf) - Mingzhen Huang, Xiaoxing Li, Jun Hu, Honghong Peng, Siwei Lyu, CVPR 2023. [[project page]](https://mingzhenhuang.com/projects/detracker.html)

### Social Interactions
- [LoCoNet: Long-Short Context Network for Active Speaker Detection](https://openaccess.thecvf.com//content/CVPR2024/html/Wang_LoCoNet_Long-Short_Context_Network_for_Active_Speaker_Detection_CVPR_2024_paper) - Xizi Wang, Feng Cheng, Gedas Bertasius, CVPR 2024. [[code]](https://github.com/SJTUwxz/LoCoNet_ASD)

- [Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Majumder_Learning_Spatial_Features_from_Audio-Visual_Correspondence_in_Egocentric_Videos_CVPR_2024_paper) - Sagnik Majumder, Ziad Al-Halah, Kristen Grauman, CVPR 2024. [[project page]](https://vision.cs.utexas.edu/projects/ego_av_corr/)

- [Egocentric Deep Multi-Channel Audio-Visual Active Speaker Localization](https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Egocentric_Deep_Multi-Channel_Audio-Visual_Active_Speaker_Localization_CVPR_2022_paper.pdf) - Hao Jiang, Calvin Murdock, Vamsi Krishna Ithapu, CVPR 2022.

- [Egocentric Auditory Attention Localization in Conversations](https://openaccess.thecvf.com/content/CVPR2023/papers/Ryan_Egocentric_Auditory_Attention_Localization_in_Conversations_CVPR_2023_paper.pdf) - Fiona Ryan, Hao Jiang, Abhinav Shukla, James M. Rehg, Vamsi Krishna Ithapu, CVPR 2022. [[project page]](https://fkryan.github.io/saal)

- [EgoCom: A Multi-person Multi-modal Egocentric Communications Dataset](https://ieeexplore.ieee.org/document/9200754) - Curtis G. Northcutt and Shengxin Zha and Steven Lovegrove and Richard Newcombe, PAMI 2020.

- [Deep Dual Relation Modeling for Egocentric Interaction Recognition](https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Deep_Dual_Relation_Modeling_for_Egocentric_Interaction_Recognition_CVPR_2019_paper.pdf) - Li, Haoxin and Cai, Yijun and Zheng, Wei-Shi, CVPR 2019.

- [Recognizing Micro-Actions and Reactions from Paired Egocentric Videos](https://openaccess.thecvf.com/content_cvpr_2016/papers/Yonetani_Recognizing_Micro-Actions_and_CVPR_2016_paper.pdf) - Yonetani, Ryo and Kitani, Kris M. and Sato, Yoichi, CVPR 2016.

- [Social interactions: A first-person perspective](http://www.cs.utexas.edu/~cv-fall2012/slides/jake-expt.pdf) - Fathi, A., Hodgins, J. K., & Rehg, J. M., CVPR 2012.

### Multiple Egocentric Tasks
- [A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives](https://arxiv.org/abs/2403.03037v1) - A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives, CVPR 2024. [[project page]](https://sapeirone.github.io/EgoPack/)

- [EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models](https://openaccess.thecvf.com//content/CVPR2024/html/Cheng_EgoThink_Evaluating_First-Person_Perspective_Thinking_Capability_of_Vision-Language_Models_CVPR_2024_paper) - Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, Yang Liu, CVPR 2024. [[code]](https://github.com/AdaCheng/EgoThink) [[project page]](https://adacheng.github.io/EgoThink/)

- [Multi-Task Learning of Object States and State-Modifying Actions from Web Videos](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10420504) - Tomáš Souček, Jean-Baptiste Alayrac, Antoine Miech, Ivan Laptev, Josef Sivic, TPAMI 2023. [[code]](https://github.com/soCzech/MultiTaskObjectStates?tab=readme-ov-file)

- [Ego4D: Around the World in 3,000 Hours of Egocentric Video](https://openaccess.thecvf.com/content/CVPR2023/html/Gong_MMG-Ego4D_Multimodal_Generalization_in_Egocentric_Action_Recognition_CVPR_2023_paper.html) - Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, Jitendra Malik, CVPR 2023. [[video]](https://drive.google.com/file/d/1oknfQIH9w1rXy6I1j5eUE6Cqh96UwZ4L/view?usp=sharing)

- [Egocentric Video Task Translation](https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_Egocentric_Video_Task_Translation_CVPR_2023_paper.pdf) - Zihui Xue, Yale Song, Kristen Grauman, Lorenzo Torresani, CVPR 2023. [[project page]](https://vision.cs.utexas.edu/projects/egot2/)

### Activity-context
- [EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views](https://arxiv.org/abs/2405.13659) - Yuhang Yang, Wei Zhai, Chengfeng Wang, Chengjun Yu, Yang Cao, Zheng-Jun Zha,  2024.

- [Multi-label affordance mapping from egocentric vision](https://arxiv.org/abs/2309.02120) - Lorenzo Mur-Labadia, Jose J. Guerrero, Ruben Martinez-Cantin, ICCV 2023.

- [Shaping embodied agent behavior with activity-context priors from egocentric video](https://proceedings.neurips.cc/paper/2021/file/f8b7aa3a0d349d9562b424160ad18612-Paper.pdf) - Tushar Nagarajan, Kristen Grauman, NIPS 2021.

- [Learning Visual Affordance Grounding from Demonstration Videos](https://arxiv.org/abs/2108.05675) - Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao,  2021.

- [EGO-TOPO: Environment Affordances from Egocentric Video](https://openaccess.thecvf.com/content_CVPR_2020/html/Nagarajan_Ego-Topo_Environment_Affordances_From_Egocentric_Video_CVPR_2020_paper.html) - Tushar Nagarajan, Yanghao Li, Christoph Feichtenhofer, Kristen Grauman, CVPR 2020.

### Diffusion models
- [HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data](https://arxiv.org/abs/2403.12011) - Mengqi Zhang, Yang Fu, Zheng Ding, Sifei Liu, Zhuowen Tu, Xiaolong Wang,  2024. [[project page]](https://mq-zhang1.github.io/HOIDiffusion/)

### Video summarization
- [SEMA: Semantic Attention for Capturing Long-Range Dependencies in Egocentric Lifelogs](https://openaccess.thecvf.com/content/WACV2024/papers/Nagar_SEMA_Semantic_Attention_for_Capturing_Long-Range_Dependencies_in_Egocentric_Lifelogs_WACV_2024_paper.pdf) - Pravin Nagar, K.N Ajay Shastry, Jayesh Chaudhari, Chetan Arora, WACV 2024. [[code]](https://github.com/Pravin74/Semantic_attention/)

- [Behavioural pattern discovery from collections of egocentric photo-streams](https://arxiv.org/pdf/2008.09561.pdf) - Martin Menchon, Estefania Talavera, Jose M Massa, Petia Radeva, Pervasive and Mobile Computing 2023.

- [Multi-stream dynamic video Summarization](https://openaccess.thecvf.com/content/WACV2022/html/Elfeki_Multi-Stream_Dynamic_Video_Summarization_WACV_2022_paper.html) - Mohamed Elfeki, Liqiang Wang, Ali Borji, WACV 2022.

- [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598) - Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Pete Florence,  2022.

- [Egocentric video summarisation via purpose-oriented frame scoring and selection](https://www.sciencedirect.com/science/article/pii/S0957417421014159) - V. Javier Traver and Dima Damen,  2022.

- [Together Recognizing, Localizing and Summarizing Actions in Egocentric Videos](https://ieeexplore.ieee.org/abstract/document/9399266?casa_token=R8LKJM45-MgAAAAA:Pfjxjt8k7l4SD_iopfL9JYsq2k6ShpZGATkXg-z5B4BTuPQV3A4HqhtZ2VqhVPtiIVbPIi_oaPU) - Abhimanyu Sahu; Ananda S. Chowdhury, TIP 2021.

- [First person video summarization using different graph representations](https://www.sciencedirect.com/science/article/pii/S0167865521001008?casa_token=H7rMpQAduAsAAAAA:Aq6ryy4IihojkZ9Tj3LoYRxT66VO3KmdBIiRTJoDvd_WBNIsHJxhruPTSNrzR6NniRg8iYyk) - Abhimanyu Sahu, Ananda S.Chowdhury, Pattern Recognition Letters 2021.

- [Summarizing egocentric videos using deep features and optimal clustering](https://www.sciencedirect.com/science/article/pii/S0925231220302952) - Abhimanyu Sahu, Ananda S.Chowdhury, Neurocomputing 2020.

- [Text Synopsis Generation for Egocentric Videos](https://ieeexplore.ieee.org/abstract/document/9412111?casa_token=Vf3uXoASupsAAAAA:CHg-misMhCLcZn-CWdUFFBLJ_SGlvsmZrAc-lfujd5yxVQSF0pr13RAYSdmrOTfYaTB0xKTj_Wg) - Aidean Sharghi; Niels da Vitoria Lobo; Mubarak Shah, ICPR 2020.

- [Shot Level Egocentric Video Co-summarization](https://ieeexplore.ieee.org/document/8546119) - Abhimanyu Sahu; Ananda S. Chowdhury, ICPR 2018.

- [Personalized Egocentric Video Summarization of Cultural Tour on User Preferences Input](https://ieeexplore.ieee.org/abstract/document/7931584?casa_token=gsvjlImpjQQAAAAA:F420NFVZd0V3igjGLVv8VpXnD1Ul5SakMxlfwcdAYCwNTsEjPgrLAKMhnUKX2VgOpoJgRm03XzI) - Patrizia Varini; Giuseppe Serra; Rita Cucchiara, IEEE Transactions on Multimedia 2017.

- [Discovering Picturesque Highlights from Egocentric Vacation Videos](https://arxiv.org/abs/1601.04406) - Vinay Bettadapura, Daniel Castro, Irfan Essa, arXiv 2016.

- [Spatial and temporal scoring for egocentric video summarization](https://www.sciencedirect.com/science/article/pii/S0925231216304805?casa_token=2uf2ekbvb7cAAAAA:YxtgDl8G6D-uunhYGOGv_aMgJeWefuO9klkQdMIh-jXz3V4JzEocy_Og3pPbaWMIlG2URM5t) - Zhao Guo, Lianli Gao, Xiantong Zhen, Fuhao Zou, Fumin Shen, Kai Zheng, Neurocomputing 2016.

- [Video Summarization with Long Short-term Memory](https://arxiv.org/abs/1605.08110) - Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman, ECCV 2016.

- [Highlight Detection with Pairwise Deep Ranking for First-Person Video Summarization](https://ieeexplore.ieee.org/document/7780481) - Ting Yao; Tao Mei; Yong Rui, CVPR 2016.

- [Predicting Important Objects for Egocentric Video Summarization](https://link.springer.com/article/10.1007/s11263-014-0794-5?sa_campaign=email/event/articleAuthor/onlineFirst&error=cookies_not_supported&error=cookies_not_supported&code=9f9cd56d-eec9-49eb-bb9f-229724e371da&code=a2d596a3-5527-4ece-addc-1db7b036c200) - Yong Jae Lee & Kristen Grauman, IJCV 2015.

- [Storyline Representation of Egocentric Videos with an Applications to Story-Based Search](https://ieeexplore.ieee.org/document/7410871) - Bo Xiong; Gunhee Kim; Leonid Sigal, ICCV 2015.

- [Gaze-Enabled Egocentric Video Summarization via Constrained Submodular Maximization](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Xu_Gaze-Enabled_Egocentric_Video_2015_CVPR_paper.html) - Jia Xu, Lopamudra Mukherjee, Yin Li, Jamieson Warner, James M. Rehg, Vikas Singh, CVPR 2015.

- [Video Summarization by Learning Submodular Mixtures of Objectives](https://openaccess.thecvf.com/content_cvpr_2015/papers/Gygli_Video_Summarization_by_2015_CVPR_paper.pdf) - Michael Gygli, Helmut Grabner, Luc Van Gool, CVPR 2015.

- [Detecting Snap Points in Egocentric Video with a Web Photo Prior](https://www.cs.utexas.edu/~grauman/papers/bo-eccv2014.pdf) - Bo Xiong and Kristen Grauman, ECCV 2014.

- [Creating Summaries from User Videos](https://gyglim.github.io/me/papers/GygliECCV14_vsum.pdf) - Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool, ECCV 2014.

- [Quasi Real-Time Summarization for Consumer Videos](https://www.cs.cmu.edu/~epxing/papers/2014/Zhao_Xing_cvpr14a.pdf) - Bin Zhao,  Eric P. Xing, CVPR 2014.

- [Story-Driven Summarization for Egocentric Video](https://www.cs.utexas.edu/~grauman/papers/lu-grauman-cvpr2013.pdf) - Zheng Lu and Kristen Grauman, CVPR 2013.

- [Discovering Important People and Objects for Egocentric Video Summarization](http://vision.cs.utexas.edu/projects/egocentric/egocentric_cvpr2012.pdf) - Yong Jae Lee, Joydeep Ghosh, and Kristen Grauman, CVPR 2012.

- [Wearable hand activity recognition for event summarization](https://ieeexplore.ieee.org/document/1550796) - Mayol, W. W., & Murray, D. W., IEEE International Symposium on Wearable Computers, IEEE International Symposium on Wearable Computers 2005.

### Applications
- [United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos](https://arxiv.org/abs/2311.03550) - Siddhant Bansal, Chetan Arora, C.V. Jawahar, WACV 2024.

- [PREGO: online mistake detection in PRocedural EGOcentric videos](https://arxiv.org/abs/2404.01933) - Alessandro Flaborea, Guido Maria D'Amely di Melendugno, Leonardo Plini, Luca Scofano, Edoardo De Matteis, Antonino Furnari, Giovanni Maria Farinella, Fabio Galasso, CVPR 2024. [[code]](https://github.com/aleflabo/PREGO)

- [Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches and a Head-Mounted Camera](https://openaccess.thecvf.com/content/CVPR2024/html/Lee_Mocap_Everyone_Everywhere_Lightweight_Motion_Capture_With_Smartwatches_and_a_CVPR_2024_paper) - Jiye Lee, Hanbyul Joo, CVPR 2024. [[code]](https://github.com/jiyewise/MocapEvery)

- [VideoLLM-online: Online Video Large Language Model for Streaming Video](https://openaccess.thecvf.com//content/CVPR2024/html/Chen_VideoLLM-online_Online_Video_Large_Language_Model_for_Streaming_Video_CVPR_2024_paper) - Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, Mike Zheng Shou, CVPR 2024. [[video]](https://showlab.github.io/videollm-online/)

- [Error Detection in Egocentric Procedural Task Videos](https://openaccess.thecvf.com//content/CVPR2024/html/Lee_Error_Detection_in_Egocentric_Procedural_Task_Videos_CVPR_2024_paper) - Shih-Po Lee, Zijia Lu, Zekun Zhang, Minh Hoai, Ehsan Elhamifar, CVPR 2024. [[code]](https://github.com/robert80203/EgoPER_official)

- [Are you Struggling? Dataset and Baselines for Struggle Determination in Assembly Videos](https://arxiv.org/abs/2402.11057) - Shijia Feng, Michael Wray, Brian Sullivan, Casimir Ludwig, Iain Gilchrist, Walterio Mayol-Cuevas,  2024.

- [Bringing Online Egocentric Action Recognition into the wild](https://arxiv.org/abs/2211.03004) - Gabriele Goletto, Mirco Planamente, Barbara Caputo, Giuseppe Averta, RA-L 2023. [[code]](https://github.com/EgocentricVision/EgoWild)

- [EgoAdapt: A multi-stream evaluation study of adaptation to real-world egocentric user video](https://arxiv.org/abs/2307.05784) - Matthias De Lange, Hamid Eghbalzadeh, Reuben Tan, Michael Iuzzolino, Franziska Meier, Karl Ridgeway,  2023.

- [Training a Large Video Model on a Single Machine in a Day](https://arxiv.org/abs/2309.16669) - Yue Zhao, Philipp Krähenbühl,  2023. [[code]](https://github.com/zhaoyue-zephyrus/AVION)

- [Learning from One Continuous Video Stream](https://arxiv.org/abs/2312.00598) - João Carreira, Michael King, Viorica Pătrăucean, Dilara Gokay, Cătălin Ionescu, Yi Yang, Daniel Zoran, Joseph Heyward, Carl Doersch, Yusuf Aytar, Dima Damen, Andrew Zisserman,  2023.

- [Wearable System for Personalized and Privacy-preserving Egocentric Visual Context Detection using On-device Deep Learning](https://dl.acm.org/doi/abs/10.1145/3450614.3461684) - Mina Khan, Glenn Fernandes, Akash Vaish, Mayank Manuja, Pattie Maes, UMAP 2021.

- [NeuralDiff: Segmenting 3D objects that move in egocentric videos](https://ieeexplore.ieee.org/abstract/document/9665956?casa_token=MCT217Rr7zAAAAAA:i-kwUh6jAScWI81EYHQU2vcze3AF5OPWNoy5EdU3t2UP2JQ2s55gDT6QYDbVapahhQdHKpTU7A) - Vadim Tschernezki, Diane Larlus, Andrea Vedaldi, 3DV 2021.

- [Learning Robot Activities From First-Person Human Videos Using Convolutional Future Regression](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w5/html/Lee_Learning_Robot_Activities_CVPR_2017_paper.html) - Jangwon Lee, Michael S. Ryoo, CVPR 2017.

### Human to Robot
- [Integrating Egocentric and Robotic Vision for Object Identification Using Siamese Networks and Superquadric Estimations in Partial Occlusion Scenarios](https://www.mdpi.com/2313-7673/9/2/100) - Elisabeth Menendez, Santiago Martínez, Fernando Díaz-de-María, Carlos Balaguer, Intelligent Human-Robot Interaction 2024.

- [Rank2Reward: Learning Shaped Reward Functions from Passive Video](https://arxiv.org/pdf/2404.14735) - Daniel Yang, Davin Tjia, Jacob Berg, Dima Damen, Pulkit Agrawal, Abhishek Gupta, ICRA 2024. [[project page]](https://rank2reward.github.io/)

- [Real-time 3D Semantic Scene Perception for Egocentric Robots with Binocular Vision](https://arxiv.org/pdf/2402.11872.pdf) - K. Nguyen, T. Dang, M. Huber,  2024. [[code]](https://github.com/mkhangg/semantic_scene_perception)

- [Learning Interaction Regions and Motion Trajectories Simultaneously From Egocentric Demonstration Videos](https://ieeexplore.ieee.org/abstract/document/10202190/authors#authors) - Jianjia Xin, Lichun Wang, Kai Xu, Chao Yang, Baocai Yin,, RA-L 2023.

- [Affordances from Human Videos as a Versatile Representation for Robotics](https://openaccess.thecvf.com/content/CVPR2023/html/Bahl_Affordances_From_Human_Videos_as_a_Versatile_Representation_for_Robotics_CVPR_2023_paper.html) - Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, Deepak Pathak, CVPR 2023. [[project page]](https://vision-robotics-bridge.github.io/)

- [R3M: A Universal Visual Representation for Robot Manipulation](https://arxiv.org/abs/2203.12601) - Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn,  Abhinav Gupta,  2022.

- [ManipulaTHOR: A Framework for Visual Object Manipulation](https://openaccess.thecvf.com/content/CVPR2021/papers/Ehsani_ManipulaTHOR_A_Framework_for_Visual_Object_Manipulation_CVPR_2021_paper.pdf) - Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi, CVPR 2021. [[project page]](https://ai2thor.allenai.org/manipulathor/)

- [Learning Robot Activities From First-Person Human Videos Using Convolutional Future Regression](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w5/html/Lee_Learning_Robot_Activities_CVPR_2017_paper.html) - Jangwon Lee, Michael S. Ryoo, CVPR 2017.

- [One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning](http://www.roboticsproceedings.org/rss14/p02.pdf) - Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang, Pieter Abbeel, Sergey Levine, RSS 2014.

### Asssitive Egocentric Vision
- [TEXT2TASTE: A Versatile Egocentric Vision System for Intelligent Reading Assistance Using Large Language Model](https://arxiv.org/abs/2404.09254) - Wiktor Mucha, Florin Cuconasu, Naome A. Etori, Valia Kalokyri, Giovanni Trappolini, ICCHP 2024.

- [Preserved action recognition in children with autism spectrum disorders: Evidence from an EEG and eye-tracking study](https://onlinelibrary.wiley.com/doi/10.1111/psyp.13740) - Mohammad Saber Sotoodeh, Hamidreza Taheri-Torbati, Nouchine Hadjikhani, Amandine Lassalle, Psychophysiology 2020.

- [A Computational Model of Early Word Learning from the Infant's Point of View](https://arxiv.org/abs/2006.02802) - Satoshi Tsutsui, Arjun Chandrasekaran, Md Alimoor Reza, David Crandall, Chen Yu, CogSci 2020.

### Popular Architectures
#### 2D
- [GSM](https://openaccess.thecvf.com/content_CVPR_2020/html/Sudhakaran_Gate-Shift_Networks_for_Video_Action_Recognition_CVPR_2020_paper.html) - Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz, CVPR 2020. [[code]](https://github.com/swathikirans/GSM)

- [TSM](https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.html) - Ji Lin, Chuang Gan, Song Han, ICCV 2019.

- [TBN](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf) - Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima, ICCV 2019. [[code]](https://github.com/ekazakos/temporal-binding-network)

- [TRN](https://arxiv.org/pdf/1711.08496.pdf) - Bolei Zhou, Alex Andonian, Aude Oliva, Antonio Torralba, ECCV 2018.

- [R(2+1)](https://openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html) - Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri, CVPR 2018.

- [TSN](https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2) - Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool, ECCV 2016.

#### 3D
- [SlowFast](https://openaccess.thecvf.com/content_ICCV_2019/html/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.html) - Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He, ICCV 2019.

- [I3D](https://openaccess.thecvf.com/content_cvpr_2017/html/Carreira_Quo_Vadis_Action_CVPR_2017_paper.html) - Joao Carreira, Andrew Zisserman, CVPR 2017.

#### RNN
- [RULSTM](https://arxiv.org/pdf/1905.09035) - Antonino Furnari, Giovanni Maria Farinella, ICCV 2019. [[code]](https://github.com/fpv-iplab/rulstm)

- [LSTA](https://openaccess.thecvf.com/content_CVPR_2019/papers/Sudhakaran_LSTA_Long_Short-Term_Attention_for_Egocentric_Action_Recognition_CVPR_2019_paper.pdf) - Sudhakaran, Swathikiran and Escalera, Sergio and Lanz, Oswald, CVPR 2019. [[code]](https://github.com/swathikirans/LSTA)

#### Transformer
- [Ego-STAN](https://arxiv.org/abs/2206.04785) - Jinman Park, Kimathi Kaai, Saad Hossain, Norikatsu Sumi, Sirisha Rambhatla, Paul Fieguth, WCVPR 2022.

- [XViT](https://proceedings.neurips.cc/paper/2021/file/a34bacf839b923770b2c360eefa26748-Paper.pdf) - Adrian Bulat, Juan-Manuel Perez-Rua, Swathikiran Sudhakaran, Brais Martinez, Georgios Tzimiropoulos, NIPS 2021.

- [TimeSformer](https://arxiv.org/abs/2102.05095) - Gedas Bertasius, Heng Wang, Lorenzo Torresani, ICML 2021.

- [ViViT](https://openaccess.thecvf.com/content/ICCV2021/html/Arnab_ViViT_A_Video_Vision_Transformer_ICCV_2021_paper.html) - Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid, ICCV 2021.

### Other EGO-Context
- [EgoGen: An Egocentric Synthetic Data Generator](https://arxiv.org/abs/2401.08739) - Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan Zhang, Marc Pollefeys, Siyu Tang, CVPR 2024. [[project page]](https://ego-gen.github.io/)

- [Active Object Detection with Knowledge Aggregation and Distillation from Large Models](https://openaccess.thecvf.com//content/CVPR2024/html/Yang_Active_Object_Detection_with_Knowledge_Aggregation_and_Distillation_from_Large_CVPR_2024_paper) - Dejie Yang, Yang Liu, CVPR 2024. [[code]](https://github.com/idejie/KAD)

- [Self-Supervised Object Detection from Egocentric Videos](https://openaccess.thecvf.com/content/ICCV2023/html/Akiva_Self-Supervised_Object_Detection_from_Egocentric_Videos_ICCV_2023_paper.html) - Peri Akiva, Jing Huang, Kevin J Liang, Rama Kovvuri, Xingyu Chen, Matt Feiszli, Kristin Dana, Tal Hassner, ICCV 2023.

- [COPILOT: Human-Environment Collision Prediction and Localization from Egocentric Videos](https://openaccess.thecvf.com/content/ICCV2023/papers/Pan_COPILOT_Human-Environment_Collision_Prediction_and_Localization_from_Egocentric_Videos_ICCV_2023_paper.pdf) - Boxiao Pan, Bokui Shen, Davis Rempe, Despoina Paschalidou, Kaichun Mo, Yanchao Yang, Leonidas J. Guibas, ICCV 2023. [[project page]](https://sites.google.com/stanford.edu/copilot)

- [Revisiting 3D Object Detection From an Egocentric Perspective](https://proceedings.neurips.cc/paper/2021/hash/db182d2552835bec774847e06406bfa2-Abstract.html) - Boyang Deng, Charles R. Qi, Mahyar Najibi, Thomas Funkhouser, Yin Zhou, Dragomir Anguelov, NIPS 2021.

- [Learning by Watching](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_by_Watching_CVPR_2021_paper.pdf) - Jimuyang Zhang, Eshed Ohn-Bar, CVPR 2021.

### Datasets
- [IndustReal] - The IndustReal dataset contains 84 videos, demonstrating how 27 participants perform maintenance and assembly procedures on a construction-toy assembly set. WACV 2024. [[paper]](https://arxiv.org/abs/2310.17323) [[code]](https://github.com/TimSchoonbeek/IndustReal)

- [IKEA Ego 3D Dataset](https://sitzikbs.github.io/IKEAEgo3D.github.io/) - A novel dataset for ego-view 3D point cloud action recognition. The dataset consists of approximately 493k frames and 56 classes of intricate furniture assembly actions of four different furniture types. WACV 2024. [[paper]](https://openaccess.thecvf.com/content/WACV2024/html/Ben-Shabat_IKEA_Ego_3D_Dataset_Understanding_Furniture_Assembly_Actions_From_Ego-View_WACV_2024_paper.html)

- [EvIs-Kitchen] - The EvIs-Kitchen dataset is the first VIdeo-Sensor-Sensor (V-S-S) interaction-focused dataset for ego-HAR tasks, capturing sequences of everyday kitchen activities. This dataset uses two inertial sensors on both wrists to better capture subject-object interactions. IEEE Sensors Journal 2024. [[paper]](https://ieeexplore.ieee.org/abstract/document/10387162)

- [Ego-Exo4D](https://ego-exo4d-data.org/) - Ego-Exo4D, a vast multimodal multiview video dataset capturing skilled human activities in both egocentric and exocentric perspectives (e.g., sports, music, dance). With 800+ participants in 13 cities, it offers 1,422 hours of combined footage, featuring diverse activities in 131 natural scene contexts, ranging from 1 to 42 minutes per video. CVPR 2024. [[paper]](https://arxiv.org/abs/2311.18259)

- [EgoExoLearn] - EgoExoLearn, a large-scale dataset that emulates the human demonstration following process, in which individuals record egocentric videos as they execute tasks guided by demonstration videos. EgoExoLearn contains egocentric and demonstration video data spanning 120 hours captured in daily life scenarios and specialized laboratories. CVPR 2024. [[paper]](https://arxiv.org/abs/2403.16182) [[code]](https://github.com/OpenGVLab/EgoExoLearn)

- [OAKINK2](https://oakink.net/v2/) - A dataset of bimanual object manipulation tasks for complex daily activities.  OAKINK2 introduces three level of abstraction to organize the manipulation tasks: Affordance, Primitive Task, and Complex Task. OAKINK2 dataset provides multi-view image streams and precise pose annotations for the human body, hands and various interacting objects. This extensive collection supports applications such as interaction reconstruction and motion synthesis. CVPR 2024. [[paper]](https://arxiv.org/abs/2403.19417)

- [UnrealEgo2-UnrealEgo-RW](https://4dqv.mpi-inf.mpg.de/UnrealEgo2/) - UnrealEgo2 Dataset: An expanded dataset capturing over 15,200 motions of realistic 3D human models with a glasses-based device, offering 1.25 million stereo views and comprehensive joint annotations. UnrealEgo-RW Dataset: A real-world dataset utilizing a compact mobile device with fisheye cameras, designed for versatile egocentric image capture in various environments. CVPR 2024. [[paper]](https://openaccess.thecvf.com//content/CVPR2024/html/Akada_3D_Human_Pose_Perception_from_Egocentric_Stereo_Videos_CVPR_2024_paper) [[code]](https://unrealego.mpi-inf.mpg.de/)

- [TF2023] - A novel dataset featuring synchronized first-person and third-person views, including masks of camera wearers linked to their respective views. It consists of 208,794 training and 87,449 testing image pairs, with no actor overlap between sets. Each scene averages 4.29 actors, focusing on complex interactions like puzzle games, enhancing its value for cross-view matching in egocentric vision. CVPR 2024. [[paper]](https://openaccess.thecvf.com//content/CVPR2024/html/Zhao_Fusing_Personal_and_Environmental_Cues_for_Identification_and_Segmentation_of_CVPR_2024_paper) [[code]](https://github.com/ziweizhao1993/PEN)

- [TACO](https://taco2024.github.io/) - A large-scale dataset of real-world bimanual tool-object interactions, featuring 131 tool-action-object triplets across 2.5K motion sequences and 5.2M frames with egocentric and 3rd-person views. TACO enables benchmarks in action recognition, hand-object motion forecasting, and grasp synthesis, advancing generalization research in human-object interactions. CVPR 2024. [[paper]](https://openaccess.thecvf.com//content/CVPR2024/html/Liu_TACO_Benchmarking_Generalizable_Bimanual_Tool-ACtion-Object_Understanding_CVPR_2024_paper)

- [BioVL-QR] - A biochemical vision-and-language dataset, which consists of 24 egocentric experiment videos, corresponding protocols, and video-and-language alignments. This study focuses on Micro QR Codes to detect objects automatically. From our preliminary study, we found that detecting objects only using Micro QR Codes is still difficult because the researchers manipulate objects, causing blur and occlusion frequently.  2024. [[paper]](https://arxiv.org/abs/2404.03161v1)

- [HOI-Ref](https://sid2697.github.io/hoi-ref/) - It consists of 3.9M question-answer pairs for training and evaluating VLMs. HOI-QA includes questions relating to locating hands, objects, and critically their interactions (e.g. referring to the object being manipulated by the hand).  2024. [[paper]](https://arxiv.org/abs/2404.09933)

- [HOT3D](https://www.projectaria.com/datasets/hot3d/) - HOT3D is benchmark dataset for egocentric vision-based understanding of 3D hand-object interactions. The dataset offers over 833 minutes (more than 3.7M images) of multi-view RGB/monochrome image streams showing 19 subjects interacting with 33 diverse rigid objects, multi-modal signals such as eye gaze or scene point clouds, as well as comprehensive ground truth annotations including 3D poses of objects, hands, and cameras, and 3D models of hands and objects.  2024. [[paper]](https://arxiv.org/abs/2406.09598) [[code]](https://github.com/TimSchoonbeek/IndustReal)

- [ADL4D] - ADL4D dataset offers a novel perspective on human-object interactions, providing video sequences of everyday activities involving multiple people and objects interacting simultaneously.  2024. [[paper]](https://arxiv.org/abs/2402.17758)

- [ENIGMA-51](https://iplab.dmi.unict.it/ENIGMA-51/#paper) - ENIGMA-51 is a new egocentric dataset acquired in an industrial scenario by 19 subjects who followed instructions to complete the repair of electrical boards using industrial tools (e.g., electric screwdriver) and equipments (e.g., oscilloscope). The 51 egocentric video sequences are densely annotated with a rich set of labels that enable the systematic study of human behavior in the industrial domain. WACV 2023. [[paper]](https://arxiv.org/abs/2309.14809)

- [VidChapters-7M](https://antoyang.github.io/vidchapters.html) - VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters in total. VidChapters-7M is automatically created from videos online in a scalable manner by scraping user-annotated chapters and hence without any additional manual annotation. NeurIPS 2023. [[paper]](https://arxiv.org/abs/2309.13952)

- [POV-Surgery](https://batfacewayne.github.io/POV_Surgery_io/) - POV-Surgery, a large-scale, synthetic, egocentric dataset focusing on pose estimation for hands with different surgical gloves and three orthopedic surgical instruments, namely scalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329 frames, featuring high-resolution RGB-D video streams with activity annotations, accurate 3D and 2D annotations for hand-object pose, and 2D hand-object segmentation masks. MICCAI 2023. [[paper]](https://arxiv.org/abs/2307.10387)

- [CaptainCook4D](https://captaincook4d.github.io/captain-cook/) - CaptainCook4D, comprising 384 recordings (94.5 hours) of people performing recipes in real kitchen environments. This dataset consists of two distinct types of activity: one in which participants adhere to the provided recipe instructions and another in which they deviate and induce errors. We provide 5.3K step annotations and 10K fine-grained action annotations and benchmark the dataset for the following tasks: supervised error recognition, multistep localization, and procedure learning. ICMLW 2023. [[paper]](https://arxiv.org/abs/2312.14556)

- [ARGO1M](https://chiaraplizz.github.io/what-can-a-cook/) - Action Recognition Generalisation dataset (ARGO1M) from videos and narrations from Ego4D. ARGO1M is the first to test action generalisation across both scenario and location shifts, and is the largest domain generalisation dataset across images and video. ICCV 2023. [[paper]](https://arxiv.org/abs/2306.08713)

- [EgoObjects] - EgoObjects, a large-scale egocentric dataset for fine-grained object understanding. contains over 9K videos collected by 250 participants from 50+ countries using 4 wearable devices, and over 650K object annotations from 368 object categories. ICCV 2023. [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_EgoObjects_A_Large-Scale_Egocentric_Dataset_for_Fine-Grained_Object_Understanding_ICCV_2023_paper.html) [[code]](https://github.com/facebookresearch/EgoObjects)

- [HoloAssist](https://holoassist.github.io/) - HoloAssist: a large-scale egocentric human interaction dataset that spans 166 hours of data captured by 350 unique instructor-performer pairs, wearing mixed-reality headsets during collaborative tasks. ICCV 2023. [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.html)

- [AssemblyHands](https://assemblyhands.github.io/) - AssemblyHands, a large-scale benchmark dataset with accurate 3D hand pose annotations, to facilitate the study of egocentric activities with challenging handobject interactions. CVPR 2023. [[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.pdf)

- [EpicSoundingObject](https://github.com/WikiChao/Ego-AV-Loc) - Epic Sounding Object dataset with sounding object annotations to benchmark the localization performance in egocentric videos. CVPR 2023. [[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Egocentric_Audio-Visual_Object_Localization_CVPR_2023_paper.pdf) [[code]](https://github.com/WikiChao/Ego-AV-Loc)

- [VOST](https://www.vostdataset.org/) - Video Object Segmentation under Transformations (VOST). It consists of more than 700 high-resolution videos, captured in diverse environments, which are 20 seconds long on average and densely labeled with instance masks. CVPR 2023. [[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Tokmakov_Breaking_the_Object_in_Video_Object_Segmentation_CVPR_2023_paper.html)

- [ARCTIC](https://arctic.is.tue.mpg.de/) - A dataset with 2.1 million video frames shows two hands skillfully manipulating objects. It includes precise 3D models of the hands and objects, as well as detailed, dynamic contact information. The dataset features two-handed actions with objects like scissors and laptops, capturing the changing hand positions and object states over time. CVPR 2023. [[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.pdf)

- [Aria Digital Twin](https://www.projectaria.com/datasets/adt/) - Aria Digital Twin (ADT) - an egocentric dataset captured using Aria glasses with extensive object, environment, and human level ground truth. This ADT release contains 200 sequences of real-world activities conducted by Aria wearers. Very challenging research problems such as 3D object detection and tracking, scene reconstruction and understanding, sim-to-real learning, human pose prediction - while also inspiring new machine perception tasks for augmented reality (AR) applications.  2023. [[paper]](https://arxiv.org/pdf/2306.06362.pdf) [[code]](https://github.com/facebookresearch/projectaria_tools)

- [WEAR](https://mariusbock.github.io/wear/) - The dataset comprises data from 18 participants performing a total of 18 different workout activities with untrimmed inertial (acceleration) and camera (egocentric video) data recorded at 10 different outside locations.  2023. [[paper]](https://arxiv.org/abs/2304.05088)

- [EPIC Fields](https://epic-kitchens.github.io/epic-fields/) - EPIC Fields, an augmentation of EPIC-KITCHENS with 3D camera information. Like other datasets for neural rendering, EPIC Fields removes the complex and expensive step of reconstructing cameras using photogrammetry, and allows researchers to focus on modelling problems.  2023. [[paper]](https://arxiv.org/abs/2306.08731)

- [EGOFALLS] - The dataset comprises 10,948 video samples from 14 subjects, focusing on falls among the elderly. Extracting multimodal descriptors from egocentric camera videos.  2023. [[paper]](https://arxiv.org/abs/2309.04579)

- [Exo2EgoDVC] - EgoYC2, a novel egocentric dataset, adapts procedural captions from YouCook2 to cooking videos re-recorded with head-mounted cameras. Unique in its weakly-paired approach, it aligns caption content with exocentric videos, distinguishing itself from other datasets focused on action labels.  2023. [[paper]](https://arxiv.org/abs/2311.16444)

- [EgoWholeBody](https://people.mpi-inf.mpg.de/~jianwang/projects/egowholemocap/) - EgoWholeBody, a large synthetic dataset, comprising 840,000 high-quality egocentric images captured across a diverse range of whole-body motion sequences. Quantitative and qualitative evaluations demonstrate the effectiveness of our method in producing high-quality whole-body motion estimates from a single egocentric camera.  2023. [[paper]](https://arxiv.org/abs/2311.16495)

- [IT3DEgo] - IT3DEgo dataset: Addresses 3D instance tracking using egocentric sensors (AR/VR). Recorded in diverse indoor scenes with HoloLens2, it comprises 50 recordings (5+ minutes each). Evaluates tracking performance in 3D coordinates, leveraging camera pose and allocentric representation.  2023. [[paper]](https://arxiv.org/pdf/2312.04117.pdf) [[code]](https://github.com/IT3DEgo/IT3DEgo/)

- [Touch and Go](https://touch-and-go.github.io/) - we present a dataset, called Touch and Go, in which human data collectors walk through a variety of environments, probing objects with tactile sensors and simultaneously recording their actions on video. NeurIPS 2022. [[paper]](https://arxiv.org/pdf/2211.12498.pdf) [[code]](https://github.com/fredfyyang/Tactile-Driven-Image-Stylization/)

- [EPIC-Visor](https://epic-kitchens.github.io/VISOR/) - VISOR, a new dataset of pixel annotations and a benchmark suite for segmenting hands and active objects in egocentric video. NeurIPS 2022. [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/590a7ebe0da1f262c80d0188f5c4c222-Abstract-Datasets_and_Benchmarks.html)

- [AssistQ](https://showlab.github.io/assistq/) - A new dataset comprising 529 question-answer samples derived from 100 newly filmed first-person videos. Each question should be completed with multi-step guidances by inferring from visual details (e.g., buttons' position) and textural details (e.g., actions like press/turn). ECCV 2022. [[paper]](https://arxiv.org/abs/2203.04203)

- [EgoProceL](https://sid2697.github.io/egoprocel/) - EgoProceL dataset focuses on the key-steps required to perform a task instead of every action in the video. EgoProceL consistis of 62 hours of videos captured by 130 subjects performing 16 tasks. ECCV 2022. [[paper]](https://arxiv.org/pdf/2207.10883.pdf)

- [EgoHOS](https://github.com/owenzlz/EgoHOS) - EgoHOS, a labeled dataset consisting of 11,243 egocentric images with per-pixel segmentation labels of hands and objects being interacted with during a diverse array of daily activities. Our dataset is the first to label detailed hand-object contact boundaries. ECCV 2022. [[paper]](https://arxiv.org/abs/2208.03826) [[code]](https://github.com/owenzlz/EgoHOS)

- [UnrealEgo](https://4dqv.mpi-inf.mpg.de/UnrealEgo/) - UnrealEgo, i.e., a new large-scale naturalistic dataset for egocentric 3D human pose estimation. It is the first dataset to provide in-the-wild stereo images with the largest variety of motions among existing egocentric datasets. ECCV 2022. [[paper]](https://arxiv.org/abs/2208.01633)

- [Assembly101](https://assembly101.github.io/) - Procedural activity dataset featuring 4321 videos of people assembling and disassembling 101 “take-apart” toy vehicles. CVPR 2022. [[paper]](https://arxiv.org/pdf/2203.14712.pdf)

- [EgoPAT3D](https://ai4ce.github.io/EgoPAT3D/) - A large multimodality dataset of more than 1 million frames of RGB-D and IMU streams, with evaluation metrics based on high-quality 2D and 3D labels from semi-automatic annotation. CVPR 2022. [[paper]](https://arxiv.org/abs/2203.13116)

- [AGD20K](https://github.com/lhc1224/Cross-View-AG) - Affordance dataset constructed by collecting and labeling over 20K images from 36 affordance categories. CVPR 2022. [[paper]](https://arxiv.org/abs/2203.09905)

- [HOI4D](https://hoi4d.github.io/) - A large-scale 4D egocentric dataset with rich annotations, to catalyze the research of category-level human-object interaction. HOI4D consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by 4 participants interacting with 800 different object instances from 16 categories over 610 different indoor rooms. Frame-wise annotations for panoptic segmentation, motion segmentation, 3D hand pose, category-level object pose and hand action have also been provided, together with reconstructed object meshes and scene point clouds. CVPR 2022. [[paper]](https://arxiv.org/abs/2203.01577)

- [EgoPW](https://arxiv.org/abs/2201.07929) - A dataset captured by a head-mounted fisheye camera and an auxiliary external camera, which provides an additional observation of the human body from a third-person perspective. CVPR 2022. [[paper]](https://arxiv.org/abs/2201.07929)

- [Ego4D](https://ego4d-data.org) - 3,025 hours of daily-life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 855 unique camera wearers from 74 worldwide locations and 9 different countries. CVPR 2022. [[paper]](https://www.cs.utexas.edu/~grauman/papers/ego4d-cvpr2022.pdf)

- [N-EPIC-Kitchens](https://github.com/EgocentricVision/N-EPIC-Kitchens) - N-EPIC-Kitchens, the first event-based camera extension of the large-scale EPIC-Kitchens dataset. CVPR 2022. [[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Plizzari_E2GOMOTION_Motion_Augmented_Event_Stream_for_Egocentric_Action_Recognition_CVPR_2022_paper.html)

- [EasyCom-Clustering](https://arxiv.org/abs/2203.13166) - The first large-scale egocentric video face clustering dataset.  2022. [[paper]](https://arxiv.org/abs/2203.13166)

- [First2Third-Pose](https://github.com/nudlesoup/First2Third-Pose) - A new paired synchronized dataset of nearly 2,000 videos depicting human activities captured from both first- and third-view perspectives.  2022. [[paper]](https://arxiv.org/abs/2201.02017)

- [TREK-100](https://machinelearning.uniud.it/datasets/trek100/) - Object tracking in first person vision. WICCV 2021. [[paper]](https://openaccess.thecvf.com/content/ICCV2021W/VOT/html/Dunnhofer_Is_First_Person_Vision_Challenging_for_Object_Tracking_ICCVW_2021_paper.html)

- [BioVL] - A novel biochemical video-andlanguage (BioVL) dataset, which consists of experimental
videos, corresponding protocols, and annotations of alignment between events in the video and instructions in the protocol.  16 videos from four protocols with a total length of 1.6 hours. WICCV 2021. [[paper]](https://openaccess.thecvf.com/content/ICCV2021W/CLVL/papers/Nishimura_Egocentric_Biochemical_Video-and-Language_Dataset_ICCVW_2021_paper.pdf)

- [MECCANO](https://iplab.dmi.unict.it/MECCANO/) - 20 subject assembling a toy motorbike. WACV 2021. [[paper]](https://openaccess.thecvf.com/content/WACV2021/html/Ragusa_The_MECCANO_Dataset_Understanding_Human-Object_Interactions_From_Egocentric_Videos_in_WACV_2021_paper.html)

- [EPIC-Kitchens 2020](https://epic-kitchens.github.io/2020-100) - Subjects performing unscripted actions in their native environments. IJCV 2021. [[paper]](https://link.springer.com/article/10.1007/s11263-021-01531-2)

- [H2O](https://taeinkwon.com/projects/h2o/) - H2O (2 Hands and Objects), provides synchronized multi-view RGB-D images, interaction labels, object classes, ground-truth 3D poses for left & right hands, 6D object poses, ground-truth camera poses, object meshes and scene point clouds. ICCV 2021. [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Kwon_H2O_Two_Hands_Manipulating_Objects_for_First_Person_Interaction_Recognition_ICCV_2021_paper.pdf)

- [HOMAGE](https://homeactiongenome.org/) - Home Action Genome (HOMAGE): a multi-view action dataset with multiple modalities and view-points supplemented with hierarchical activity and atomic action labels together with dense scene composition labels. CVPR 2021. [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Rai_Home_Action_Genome_Cooperative_Compositional_Action_Understanding_CVPR_2021_paper.pdf)

- [EgoCom](https://github.com/facebookresearch/EgoCom-Dataset) - A natural conversations dataset containing multi-modal human communication data captured simultaneously from the participants' egocentric perspectives. TPAMI 2020. [[paper]](https://ieeexplore.ieee.org/document/9200754)

- [EGO-CH](https://iplab.dmi.unict.it/EGO-CH/) - 70 subjects visiting two cultural sites in Sicily, Italy. Pattern Recognition Letters 2020. [[paper]](https://www.sciencedirect.com/science/article/pii/S0167865519303794)

- [EPIC-Tent](https://data.bristol.ac.uk/data/dataset/2ite3tu1u53n42hjfh3886sa86) - 29 participants assembling a tent while wearing two head-mounted cameras. ICCV 2019. [[paper]](https://openaccess.thecvf.com/content_ICCVW_2019/html/EPIC/Jang_EPIC-Tent_An_Egocentric_Video_Dataset_for_Camping_Tent_Assembly_ICCVW_2019_paper.html)

- [EPIC-Kitchens 2018](https://epic-kitchens.github.io/2018) - 32 subjects performing unscripted actions in their native environments. ECCV 2018. [[paper]](https://openaccess.thecvf.com/content_ECCV_2018/html/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.html)

- [Charade-Ego](https://allenai.org/plato/charades/) - Paired first-third person videos. 

- [EGTEA Gaze+](http://cbs.ic.gatech.edu/fpv/) - 32 subjects, 86 cooking sessions, 28 hours. 

- [ADL](https://www.csee.umbc.edu/~hpirsiav/papers/ADLdataset/) - 20 subjects performing daily activities in their native environments. 

- [CMU kitchen](http://kitchen.cs.cmu.edu/index.php) - Multimodal, 18 subjects cooking 5 different recipes: brownies, eggs, pizza, salad, sandwich. 

- [EgoSeg](http://www.vision.huji.ac.il/egoseg/) - Long term actions (walking, running, driving, etc.). 

- [First-Person Social Interactions](http://ai.stanford.edu/~alireza/Disney/) - 8 subjects at disneyworld. 

- [UEC Dataset](http://www.cs.cmu.edu/~kkitani/datasets/) - Two choreographed datasets with different egoactions (walk, jump, climb, etc.) + 6 YouTube sports videos. 

- [JPL](http://michaelryoo.com/jpl-interaction.html) - Interaction with a robot. 

- [FPPA](http://tamaraberg.com/prediction/Prediction.html) - Five subjects performing 5 daily actions. 

- [UT Egocentric](http://vision.cs.utexas.edu/projects/egocentric/index.html) - 3-5 hours long videos capturing a person's day. 

- [VINST/ Visual Diaries](http://www.csc.kth.se/cvap/vinst/NovEgoMotion.html) - 31 videos capturing the visual experience of a subject walking from metro station to work. 

- [Bristol Egocentric Object Interaction (BEOID)](https://www.cs.bris.ac.uk/~damen/BEOID/) - 8 subjects, six locations. Interaction with objects and environment. 

- [Object Search Dataset](https://github.com/Mengmi/deepfuturegaze_gan) - 57 sequences of 55 subjects on search and retrieval tasks. 

- [UNICT-VEDI](http://iplab.dmi.unict.it/VEDI/) - Different subjects visiting a museum. 

- [UNICT-VEDI-POI](http://iplab.dmi.unict.it/VEDI_POIs/) - Different subjects visiting a museum. 

- [Simulated Egocentric Navigations](http://iplab.dmi.unict.it/SimulatedEgocentricNavigations/) - Simulated navigations of a virtual agent within a large building. 

- [EgoCart](http://iplab.dmi.unict.it/EgocentricShoppingCartLocalization/) - Egocentric images collected by a shopping cart in a retail store. 

- [Unsupervised Segmentation of Daily Living Activities](http://iplab.dmi.unict.it/dailylivingactivities) - Egocentric videos of daily activities. 

- [Visual Market Basket Analysis](http://iplab.dmi.unict.it/vmba/) - Egocentric images collected by a shopping cart in a retail store. 

- [Location Based Segmentation of Egocentric Videos](http://iplab.dmi.unict.it/PersonalLocationSegmentation/) - Egocentric videos of daily activities. 

- [Recognition of Personal Locations from Egocentric Videos](http://iplab.dmi.unict.it/PersonalLocations/) - Egocentric videos clips of daily. 

- [EgoGesture](http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html) - 2k videos from 50 subjects performing 83 gestures. 

- [EgoHands](http://vision.soic.indiana.edu/projects/egohands/) - 48 videos of interactions between two people. 

- [DoMSEV](http://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2018-dataset/) - 80 hours/different activities. 

- [DR(eye)VE](http://aimagelab.ing.unimore.it/dreyeve) - 74 videos of people driving. 

- [THU-READ](http://ivg.au.tsinghua.edu.cn/dataset/THU_READ.php) - 8 subjects performing 40 actions with a head-mounted RGBD camera. 

- [EgoDexter](https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/EgoDexter.htm) - 4 sequences with 4 actors (2 female), and varying interactions with various objects and and cluttered background.  [[paper]](https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/index.htm)

- [First-Person Hand Action (FPHA)](https://guiggh.github.io/publications/first-person-hands/) - 3D hand-object interaction. Includes 1175 videos belonging to 45 different activity categories performed by 6 actors.  [[paper]](https://arxiv.org/pdf/1704.02463.pdf)

- [UTokyo Paired Ego-Video (PEV)](https://yonetaniryo.github.io/fpv_data.html) - 1,226 pairs of first-person clips extracted from the ones recorded synchronously during dyadic conversations. 

- [UTokyo Ego-Surf](https://yonetaniryo.github.io/fpv_data.html) - Contains 8 diverse groups of first-person videos recorded synchronously during face-to-face conversations. 

- [TEgO: Teachable Egocentric Objects Dataset](https://iamlabumd.github.io/tego/) - Contains egocentric images of 19 distinct objects taken by two people for training a teachable object recognizer. 

- [Multimodal Focused Interaction Dataset](https://cvip.computing.dundee.ac.uk/datasets/focusedinteraction/) - Contains 377 minutes of continuous multimodal recording captured during 19 sessions, with 17 conversational partners in 18 different indoor/outdoor locations. 

### Not Yet Explored Task

## Challenges

- [Ego4D](https://ego4d-data.org)- Episodic Memory, Hand-Object Interactions, AV Diarization, Social, Forecasting.

- [Epic Kitchen Challenge](https://epic-kitchens.github.io/)- Action Recognition, Action Detection, Action Anticipation, Unsupervised Domain Adaptation for Action Recognition, Multi-Instance Retrieval.

- [MECCANO](https://iplab.dmi.unict.it/MECCANO/challenge.html)- Multimodal Action Recognition (RGB-Depth-Gaze).


## Devices

- [GoPro](https://gopro.com/it/it/)

- [Narrative clip](http://getnarrative.com/)

- [Autographer5](https://en.wikipedia.org/wiki/Autographer)

- [Microsoft SenseCam](https://www.microsoft.com/en-us/research/project/sensecam/)

- [SMI eye-tracker](https://imotions.com/hardware/smi-eye-tracking-glasses/)

- [ASL Mobile eye](https://est-kl.com/manufacturer/asl/mobile-eye-5.html)

- [Tobii eye-tracker](https://www.tobii.com/)

- [Pupil Invisible](https://pupil-labs.com/)

- [Microsoft Hololens 2](https://www.microsoft.com/it-it/hololens/)

- [Google Glass](https://www.google.com/glass/start/)

- [Vusix Blade](https://www.vuzix.com/products/vuzix-blade-smart-glasses-upgraded)

- [Magic Leap](https://www.magicleap.com/)

- [Nreal Light](https://www.nreal.ai/)

- [Epson Moverio](https://www.epson.it/products/see-through-mobile-viewer)

- [Realwear](https://www.realwear.com/)

- [TCL Smart Glasses Thunderbird]

- [OrCam](https://www.orcam.com/it/)

- [Xiaomi Smart Glasses]

- [Ray-Ban Stories](https://www.ray-ban.com/italy/ray-ban-stories)

- [dynaEdge](https://it.dynabook.com/generic/dynaedge/)

- [Apple Glass]

- [Alpha Glass]

- [GWD HiiDii](https://www.gwdbi.com/)

- [Spectacles](https://www.spectacles.com/it/shop/spectacles-3/)

- [ProjectAria](https://about.meta.com/it/realitylabs/projectaria/)

This is a work in progress... 